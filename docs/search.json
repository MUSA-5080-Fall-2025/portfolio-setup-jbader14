[
  {
    "objectID": "weekly-notes/week-09-notes.html",
    "href": "weekly-notes/week-09-notes.html",
    "title": "Week 9 Notes - Logistic Regression: The Case of Recidivism",
    "section": "",
    "text": "Logistic Regression: Predicting binary outcomes\n\nPredict the probability of Y, not Y directly\nNeed 0-1 bounds\n\nLogit Transformation: Behind the scenes, work with log-odds\n\nLog-Odds: Logit(p) = ln (p / 1-p)\nCreates linear relationship: Logit(p) = B0 + B1*x1 ‚Ä¶\nCoefs are log odds\nInterpret as odds ratio\n\n1: predictor increases odds of outcome\n\n\nClassify threshold to split on classification: Diff risks\nConfusion Matrix (TP, FP, FN, TN)\nMetrics to evaluate: Sensitivity (Recall, TPR), Specificity (TNR), Precision\nROC curve: Receiver Operating Characteristic\n\nEvery possible threshold\nTrade off between TPR and FPR\nOverall model discrimination ability"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-09-notes.html#key-concepts-learned",
    "title": "Week 9 Notes - Logistic Regression: The Case of Recidivism",
    "section": "",
    "text": "Logistic Regression: Predicting binary outcomes\n\nPredict the probability of Y, not Y directly\nNeed 0-1 bounds\n\nLogit Transformation: Behind the scenes, work with log-odds\n\nLog-Odds: Logit(p) = ln (p / 1-p)\nCreates linear relationship: Logit(p) = B0 + B1*x1 ‚Ä¶\nCoefs are log odds\nInterpret as odds ratio\n\n1: predictor increases odds of outcome\n\n\nClassify threshold to split on classification: Diff risks\nConfusion Matrix (TP, FP, FN, TN)\nMetrics to evaluate: Sensitivity (Recall, TPR), Specificity (TNR), Precision\nROC curve: Receiver Operating Characteristic\n\nEvery possible threshold\nTrade off between TPR and FPR\nOverall model discrimination ability"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#coding-techniques",
    "href": "weekly-notes/week-09-notes.html#coding-techniques",
    "title": "Week 9 Notes - Logistic Regression: The Case of Recidivism",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nLogit Model: Use glm() with family = ‚Äòbinomial‚Äô\n\nexp() convert to odds ratio\ncoef() interpret coefs\npredict(): make predictions\n\nPlotting:\n\nconfusionMatrix()\nggroc()"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#questions-challenges",
    "href": "weekly-notes/week-09-notes.html#questions-challenges",
    "title": "Week 9 Notes - Logistic Regression: The Case of Recidivism",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#connections-to-policy",
    "href": "weekly-notes/week-09-notes.html#connections-to-policy",
    "title": "Week 9 Notes - Logistic Regression: The Case of Recidivism",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nLogistic regression is very important to policy markers for supporting algorithmic decision making for binary outcomes."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#reflection",
    "href": "weekly-notes/week-09-notes.html#reflection",
    "title": "Week 9 Notes - Logistic Regression: The Case of Recidivism",
    "section": "Reflection",
    "text": "Reflection\n\nLearning these topics in a much more applied setting compared to my Data Science classes has really helped me further grasp the concepts."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html",
    "href": "weekly-notes/week-07-notes.html",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "",
    "text": "Hide progress bars when loading spatial data:\n\nprogress = FALSE inside get_acs\nOR: options(tigris_progress = FALSE)\n\nGood (random) errors:\n\nNo systematic pattern\nScattered across space\nPrediction equally good everywhere\nModel captures key relationships\n\nBad (clustered) errors:\n\nSpatial pattern visible\nUnder / over predict in areas\nModel misses something about location\nNeed more spatial features\n\nLook for spatial autocorrelation in the errors\nMoran‚Äôs I: [-1, 1]\n\n1: Perfect spatial correlation (clustering)\n0: Random spatial pattern\n(-1): Perfect negative correlation (dispersion)\nIf high: (diff buffer sizes, more amenities, neighborhood specific vars)\n\n\nAdd more spatial features\n\n\nTry spatial fixed effects (neighborhood or grid cell dummies)\n\n\nConsider spatial regression models (spatial lag or error models)\n\n\nNOT for this class.\n\n\n\nDefining ‚Äúneighbors‚Äù\n\nContiguity:\n\nPolygons that share a border\n\nDistance:\n\nAll within X meters\nFixed threshold\n\nk-Nearest:\n\nClosest k points\nAdaptive distance\n\n\nSpatial lag = avg value of neighbors (see slide for syntax)"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-07-notes.html#key-concepts-learned",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "",
    "text": "Hide progress bars when loading spatial data:\n\nprogress = FALSE inside get_acs\nOR: options(tigris_progress = FALSE)\n\nGood (random) errors:\n\nNo systematic pattern\nScattered across space\nPrediction equally good everywhere\nModel captures key relationships\n\nBad (clustered) errors:\n\nSpatial pattern visible\nUnder / over predict in areas\nModel misses something about location\nNeed more spatial features\n\nLook for spatial autocorrelation in the errors\nMoran‚Äôs I: [-1, 1]\n\n1: Perfect spatial correlation (clustering)\n0: Random spatial pattern\n(-1): Perfect negative correlation (dispersion)\nIf high: (diff buffer sizes, more amenities, neighborhood specific vars)\n\n\nAdd more spatial features\n\n\nTry spatial fixed effects (neighborhood or grid cell dummies)\n\n\nConsider spatial regression models (spatial lag or error models)\n\n\nNOT for this class.\n\n\n\nDefining ‚Äúneighbors‚Äù\n\nContiguity:\n\nPolygons that share a border\n\nDistance:\n\nAll within X meters\nFixed threshold\n\nk-Nearest:\n\nClosest k points\nAdaptive distance\n\n\nSpatial lag = avg value of neighbors (see slide for syntax)"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#coding-techniques",
    "href": "weekly-notes/week-07-notes.html#coding-techniques",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\neval: true -&gt; code chunk actually ran (#| ) before\necho: true -&gt; code chunk is actualy displayed\nspdep library to do spatial analysis like knn, weights, etc. (see slides)"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#questions-challenges",
    "href": "weekly-notes/week-07-notes.html#questions-challenges",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#connections-to-policy",
    "href": "weekly-notes/week-07-notes.html#connections-to-policy",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nIdentifying spatial autocorrelation is very important in understanding what errors are good / bad and how we can fix them to make better models for policy considerations."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#reflection",
    "href": "weekly-notes/week-07-notes.html#reflection",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "Reflection",
    "text": "Reflection\n\nI havent‚Äôt learned about Moran‚Äôs I before, but I think it is very useful to use to test for autocorrelaton and ultimately, model performance."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 5 - Intro to Linear Regression",
    "section": "",
    "text": "Statistical learning = a set of approaches for estimating relationship between variables\n\nObserve data like: counties, income, population, education, etc.\n\nResponse / outcome variable Y as a function of predictors X‚Äôs and noise‚Ä¶\n\nùëå=ùëì(ùëã)+ùúñ\nf = the systematic information X provides about Y\nŒµ = random error (irreducible)\n\nf represents the true relationship between predictors and outcome\n\nfixed but unknown\nwhat we want to estimate\nI think this is just the coefficient\n\nWhy estimate f?\n\n\nPrediction:\n\n\nEstimate Y for new observations\nDon‚Äôt necessarily care about the exact form of f\nPrediction intervals (confidence) matter\nDon‚Äôt need to understand why\nFocus: accuracy of predictions\n\n\nInference\n\n\nUnderstand how X affects Y (mechanisms)\nStatistical significance matters\nWhich predictors matter?\nWhat‚Äôs the nature of the relationship?\nFocus: interpreting the model / coefs\n\nintercept (usually not that useful), slope (coef), p-value (reject null / fail to reject)\n\n\n\nHow do we estimate f?\n\nParametric Methods\n\nMake an assumption about the functional form (e.g., linear)\nReduces problem to estimating a few parameters\nEasier to interpret\nThis is what we‚Äôll focus on\nMethod: OLS\n\nNon-Parametric Methods\n\nDon‚Äôt assume a specific form\nMore flexible\nRequire more data\nHarder to interpret\n\nKey Diff:\n\nParametric: We assume f is linear, then estimate Œ≤‚ÇÄ and Œ≤‚ÇÅ\nNon-Parametric: We let the data determine the shape of f.\n\n\nWhy linear regression?\n\nAdvantages:\n\nSimple and interpretable\nWell-understood properties\nWorks remarkably well for many problems\nFoundation for more complex methods\n\nLimitations:\n\nAssumes linearity\nSensitive to outliers\nMakes several assumptions\n\n\nStatistical Significance:\n\nNull hypothesis (H‚ÇÄ): Œ≤‚ÇÅ = 0 (no relationship)\nOur estimate: Œ≤‚ÇÅ = 0.02\nQuestion: Could we get 0.02 just by chance if H‚ÇÄ is true?\nt-statistic: How many standard errors away from 0?\n\nBigger |t| = more confidence the relationship is real\n\np-value: Probability of seeing our estimate if H‚ÇÄ is true\n\nSmall p ‚Üí reject H‚ÇÄ, conclude relationship exists\n\n\nModel Evaluation:\n\nHow well does it fit the data we used? (in sample fit)\n\nR-squared: _ % of variation in income is explained by population.\n\nFor prediction: Pop is moderately good predictor of income.\nFor inference: Pop matters, but other factors exist.\nR-squared alone does not tell us if the model is trustworthy (overfit)\n\n\nHow well would it predict new data? (out of sample performance)\n\nPROBLEM:\n\nUnderfitting: Model too simple (high bias)\nGood fit: Captures pattern without noise\nOverfitting: Memorizes training data (high variance)\n\nCross-Validation: Multiple train-test splits\n\nGives more stable estimate of true prediction performance\n\nLinear regression makes assumptions. If violated:\n\nCoefficients may be biased\nStandard errors wrong\nPredictions unreliable\n\nImproving the model:\n\nAdd more predictors\nLog transform predictors\nCategorical variables\n\nRegression workflow:\n\nUnderstand the framework: What‚Äôs f? What‚Äôs the goal?\nVisualize first: Does a linear model make sense?\nFit the model: Estimate coefficients\nEvaluate performance: Train/test split, cross-validation\nCheck assumptions: Residual plots, VIF, outliers\nImprove if needed: Transformations, more variables\nConsider ethics: Who could be harmed by this model?"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "title": "Week 5 - Intro to Linear Regression",
    "section": "",
    "text": "Statistical learning = a set of approaches for estimating relationship between variables\n\nObserve data like: counties, income, population, education, etc.\n\nResponse / outcome variable Y as a function of predictors X‚Äôs and noise‚Ä¶\n\nùëå=ùëì(ùëã)+ùúñ\nf = the systematic information X provides about Y\nŒµ = random error (irreducible)\n\nf represents the true relationship between predictors and outcome\n\nfixed but unknown\nwhat we want to estimate\nI think this is just the coefficient\n\nWhy estimate f?\n\n\nPrediction:\n\n\nEstimate Y for new observations\nDon‚Äôt necessarily care about the exact form of f\nPrediction intervals (confidence) matter\nDon‚Äôt need to understand why\nFocus: accuracy of predictions\n\n\nInference\n\n\nUnderstand how X affects Y (mechanisms)\nStatistical significance matters\nWhich predictors matter?\nWhat‚Äôs the nature of the relationship?\nFocus: interpreting the model / coefs\n\nintercept (usually not that useful), slope (coef), p-value (reject null / fail to reject)\n\n\n\nHow do we estimate f?\n\nParametric Methods\n\nMake an assumption about the functional form (e.g., linear)\nReduces problem to estimating a few parameters\nEasier to interpret\nThis is what we‚Äôll focus on\nMethod: OLS\n\nNon-Parametric Methods\n\nDon‚Äôt assume a specific form\nMore flexible\nRequire more data\nHarder to interpret\n\nKey Diff:\n\nParametric: We assume f is linear, then estimate Œ≤‚ÇÄ and Œ≤‚ÇÅ\nNon-Parametric: We let the data determine the shape of f.\n\n\nWhy linear regression?\n\nAdvantages:\n\nSimple and interpretable\nWell-understood properties\nWorks remarkably well for many problems\nFoundation for more complex methods\n\nLimitations:\n\nAssumes linearity\nSensitive to outliers\nMakes several assumptions\n\n\nStatistical Significance:\n\nNull hypothesis (H‚ÇÄ): Œ≤‚ÇÅ = 0 (no relationship)\nOur estimate: Œ≤‚ÇÅ = 0.02\nQuestion: Could we get 0.02 just by chance if H‚ÇÄ is true?\nt-statistic: How many standard errors away from 0?\n\nBigger |t| = more confidence the relationship is real\n\np-value: Probability of seeing our estimate if H‚ÇÄ is true\n\nSmall p ‚Üí reject H‚ÇÄ, conclude relationship exists\n\n\nModel Evaluation:\n\nHow well does it fit the data we used? (in sample fit)\n\nR-squared: _ % of variation in income is explained by population.\n\nFor prediction: Pop is moderately good predictor of income.\nFor inference: Pop matters, but other factors exist.\nR-squared alone does not tell us if the model is trustworthy (overfit)\n\n\nHow well would it predict new data? (out of sample performance)\n\nPROBLEM:\n\nUnderfitting: Model too simple (high bias)\nGood fit: Captures pattern without noise\nOverfitting: Memorizes training data (high variance)\n\nCross-Validation: Multiple train-test splits\n\nGives more stable estimate of true prediction performance\n\nLinear regression makes assumptions. If violated:\n\nCoefficients may be biased\nStandard errors wrong\nPredictions unreliable\n\nImproving the model:\n\nAdd more predictors\nLog transform predictors\nCategorical variables\n\nRegression workflow:\n\nUnderstand the framework: What‚Äôs f? What‚Äôs the goal?\nVisualize first: Does a linear model make sense?\nFit the model: Estimate coefficients\nEvaluate performance: Train/test split, cross-validation\nCheck assumptions: Residual plots, VIF, outliers\nImprove if needed: Transformations, more variables\nConsider ethics: Who could be harmed by this model?"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#coding-techniques",
    "href": "weekly-notes/week-05-notes.html#coding-techniques",
    "title": "Week 5 - Intro to Linear Regression",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nFit the model:\n\nlinear reg: model1 &lt;- lm(median_incomeE ~ total_popE, data = pa_data)\nsummary stats: summary(model1)\n\nTrain/Test Split:\n\nset.seed(123)\nn &lt;- nrow(pa_data)\n70% training, 30% testing\ntrain_indices &lt;- sample(1:n, size = 0.7 * n)\ntrain_data &lt;- pa_data[train_indices, ]\ntest_data &lt;- pa_data[-train_indices, ]\nFit on training data only\nmodel_train &lt;- lm(median_incomeE ~ total_popE, data = train_data)\nPredict on test data\ntest_predictions &lt;- predict(model_train, newdata = test_data)\n\nEvaluate Prediction:\n\nCalculate prediction error (RMSE)\nrmse_test &lt;- sqrt(mean((test_data$median_incomeE - test_predictions)^2))\nrmse_train &lt;- summary(model_train)$sigma\ncat(‚ÄúTraining RMSE:‚Äù, round(rmse_train, 0), ‚Äú‚Äù)\nTraining RMSE: 12893\ncat(‚ÄúTest RMSE:‚Äù, round(rmse_test, 0), ‚Äú‚Äù)\nTest RMSE: 9536\nInterpreting RMSE\nOn new data (test set), our predictions are off by ~$9,500 on average. Is this level of error acceptable for policy decisions?\n\nCross-Validation:\n\nlibrary(caret)\n10-fold cross-validation\ntrain_control &lt;- trainControl(method = ‚Äúcv‚Äù, number = 10)\ncv_model &lt;- train(median_incomeE ~ total_popE, data = pa_data, method = ‚Äúlm‚Äù, trControl = train_control)\ncv_model$results\nintercept RMSE Rsquared MAE RMSESD RsquaredSD MAESD\nTRUE  12577.76 0.5643826 8859.865 5609.002  0.2997098 2238.042\nKey Metrics (Averaged Across 10 Folds)\n\nRMSE: Typical prediction error (~$12,578)\nR¬≤: % of variation explained (0.564)\nMAE: Average absolute error (~$8,860) - easier to interpret\n\n\nAssumption 1: Linearity - Check: Residual Plot\n\npa_data$residuals &lt;- residuals(model1)\npa_data$fitted &lt;- fitted(model1)\nggplot(pa_data, aes(x = fitted, y = residuals)) +\ngeom_point() +\ngeom_hline(yintercept = 0, color = ‚Äúred‚Äù, linetype = ‚Äúdashed‚Äù) +\nlabs(title = ‚ÄúResidual Plot‚Äù, x = ‚ÄúFitted Values‚Äù, y = ‚ÄúResiduals‚Äù) +\ntheme_minimal()\n\nReading residual plots:\n\nGood: Random scatter, points around zero, constant spread\nBad: Curved pattern, model missing something, predictions biased\nLinearity violations hurt predictions, not just inference:\n\nIf the true relationship is curved and you fit a straight line, you‚Äôll systematically underpredict in some regions and overpredict in others\nBiased predictions in predictable ways (not random errors!)\nResidual plots should show random scatter - any pattern means your model is missing something systematic\n\n\nAssumption 2: Constant Variance - Check: Residual Plot\n\nHeteroskedasticity: Variance changes across X\nImpact; Standard errors are wrong -&gt; misleading p-values\nOften a symptom of model misspecification:\n\nModel fits well for some values (e.g., small counties) but poorly for others (large counties)\nMay indicate missing variables that matter more at certain X values\nAsk: ‚ÄúWhat‚Äôs different about observations with large residuals?‚Äù\n\nFormal test: Breusch-Pagan\n\nlibrary(lmtest)\nbptest(model1)\np &gt; 0.05: Constant variance assumption OK\np &lt; 0.05: Evidence of heteroscedasticity\n\nIf detected, solutions:\n\nTransform Y (try log(income))\nRobust standard errors\nAdd missing variables\nAccept it (point predictions still OK for prediction goals)\n\n\nAsssumption 3: Normality of Residuals - Check: Q-Q Plot\n\nLess critical for point predictions (unbiased regardless)\nImportant for confidence intervals and prediction intervals\nNeeded for valid hypothesis tests (t-tests, F-tests)\n\nAssumption 4: No Multicollinearity - VIF\n\nX‚Äôs shouldn‚Äôt be correlated w/ one another\nCoefs can become unstable, harder to interpret\nlibrary(car)\nvif(model1) # Variance Inflation Factor\nRule of thumb: VIF &gt; 10 suggests problems\nNot relevant with only 1 predictor!\n\nAssumption 5: No influential outliers:\n\nNot all outliers are problems - only those with high leverage AND large residuals\n\nHigh leverage + large residual = pulls regression line\n\nCheck w/ visual diagnostic / Identify influential points (z-score)\nWhat to do with influential outliers:\n\nInvestigate: Why is this observation unusual? (data error? truly unique?)\nReport: Always note influential observations in your analysis\nSensitivity check: Refit model without them - do conclusions change?\nDon‚Äôt automatically remove: They might represent real, important cases\nFor policy: An influential county might need special attention, not exclusion!"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#questions-challenges",
    "href": "weekly-notes/week-05-notes.html#questions-challenges",
    "title": "Week 5 - Intro to Linear Regression",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#connections-to-policy",
    "href": "weekly-notes/week-05-notes.html#connections-to-policy",
    "title": "Week 5 - Intro to Linear Regression",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nThere are a lot of human choices in modeling, which influences the conclusions we draw to make policy decisions. It is important to know how to verify assumptions, improve modeling, etc. to make for better policy considerations."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#reflection",
    "href": "weekly-notes/week-05-notes.html#reflection",
    "title": "Week 5 - Intro to Linear Regression",
    "section": "Reflection",
    "text": "Reflection\n\nI have covered regression many times, but this was a good refresher especially on all of the assumptions that we should verify and methods to do so."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3: EDA & ggplot2",
    "section": "",
    "text": "EDA:\n\nCreate summary statistics table\n\nBut can hide underlying data distribution\n\nOutliers, non-linear relationships\n\nSo data visualization is important\nCan have same mean, var, cov, regression but diff patterns in visualization\n\nBad / Misleading data visualizations\n\nMisleading scales or axes\nCherry-picked time periods\nHidden or ignored uncertainty\nMissing context about data reliability\nResult: Poor policy decisions\n\nGoal: Understand your data before making decisions or building models\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\nWhat‚Äôs unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?\n\nWorkflow w/ Data Quality Focus: Enhanced process for policy analysis\n\nLoad and inspect - dimensions, variable types, missing data\nAssess reliability - examine margins of error, calculate coefficients of variation\nVisualize distributions - histograms, boxplots for each variable\nExplore relationships - scatter plots, correlations\nIdentify patterns - grouping, clustering, geographical patterns\nQuestion anomalies - investigate outliers and unusual patterns\nDocument limitations - prepare honest communication about data quality\n\nJOINS - join predicate does not need to be same name, NEED SAME TYPE\n\nleft (most-common) / right: keeps everything from left/right\nfull: keeps everything from both columns\ninner: keeps only matches"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "title": "Week 3: EDA & ggplot2",
    "section": "",
    "text": "EDA:\n\nCreate summary statistics table\n\nBut can hide underlying data distribution\n\nOutliers, non-linear relationships\n\nSo data visualization is important\nCan have same mean, var, cov, regression but diff patterns in visualization\n\nBad / Misleading data visualizations\n\nMisleading scales or axes\nCherry-picked time periods\nHidden or ignored uncertainty\nMissing context about data reliability\nResult: Poor policy decisions\n\nGoal: Understand your data before making decisions or building models\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\nWhat‚Äôs unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?\n\nWorkflow w/ Data Quality Focus: Enhanced process for policy analysis\n\nLoad and inspect - dimensions, variable types, missing data\nAssess reliability - examine margins of error, calculate coefficients of variation\nVisualize distributions - histograms, boxplots for each variable\nExplore relationships - scatter plots, correlations\nIdentify patterns - grouping, clustering, geographical patterns\nQuestion anomalies - investigate outliers and unusual patterns\nDocument limitations - prepare honest communication about data quality\n\nJOINS - join predicate does not need to be same name, NEED SAME TYPE\n\nleft (most-common) / right: keeps everything from left/right\nfull: keeps everything from both columns\ninner: keeps only matches"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03-notes.html#coding-techniques",
    "title": "Week 3: EDA & ggplot2",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nggplot2\n\nData -&gt; Aesthetics -&gt; Geometry -&gt; Visual\n\nData: Your dataset (census data, survey responses, etc.)\nAesthetics: What variables map to visual properties (x, y, color, size)\nGeometries: How to display the data (points, bars, lines)\nAdditional layers: Scales, themes, facets, annotations\nGeneral syntax: ggplot(data = your_data) + aes(x = variable1, y = variable2) + geom_something() + additional_layers()\n\nAesthetic Mappings: maps data to visual properties\n\nx, y - position\ncolor - point/line color\nfill - area fill color\nsize - point/line size\nshape - point shape\nalpha - transparency\n\nstr_detect(), str_extract(), regex() to search within strings"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3: EDA & ggplot2",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3: EDA & ggplot2",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nLearning how to make good (and not misleading) plots is crucial in conveying information to less data-savvy people who may not understand advanced statistical terms.\nEDA is extremely important in learning more about your data before operating on it and then later drawing analysis from for policy considerations."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3: EDA & ggplot2",
    "section": "Reflection",
    "text": "Reflection\n\nI think that these topics are very important to learn to be a good analyst. Being prepared and having a good understanding of your data is a crucial skill to have and plotting allows you to easily convey your findings to your peers."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 - Course Introduction",
    "section": "",
    "text": "Upcoming assignments: lab0: 9/15 create Quatro portfolio; lab1: 9/27 EDA\nGit: Version-control system - tracks changes in files\n\nCollaboration tool for people teams to work\n\nGithub: cloud host for Git repos (projects)\n\nBackup work in cloud\nShare / collaborate on projects with other\nGithub pages hosts websites\n\nGithub Concepts:\n\nRepo: folder containing proj files\nCommit: snapshot of work at point in time\nPush: Send changes to Github cloud\nPull: Get latest changes from Github cloud\n\nWorkflow each week:\n\nEdit files in RStudio (local)\nCommit changes w/ message\nPush to Github\nPortfolio propogates changes automatically\n\nGithub Classroom:\n\nCreates individual repos for each student\nDistributes assignments automatically\nTAs can provide feedback\n\nLists syntax\nLinks / Images syntax"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 - Course Introduction",
    "section": "",
    "text": "Upcoming assignments: lab0: 9/15 create Quatro portfolio; lab1: 9/27 EDA\nGit: Version-control system - tracks changes in files\n\nCollaboration tool for people teams to work\n\nGithub: cloud host for Git repos (projects)\n\nBackup work in cloud\nShare / collaborate on projects with other\nGithub pages hosts websites\n\nGithub Concepts:\n\nRepo: folder containing proj files\nCommit: snapshot of work at point in time\nPush: Send changes to Github cloud\nPull: Get latest changes from Github cloud\n\nWorkflow each week:\n\nEdit files in RStudio (local)\nCommit changes w/ message\nPush to Github\nPortfolio propogates changes automatically\n\nGithub Classroom:\n\nCreates individual repos for each student\nDistributes assignments automatically\nTAs can provide feedback\n\nLists syntax\nLinks / Images syntax"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nR (open software - people create packages free to use) and dplyr:\n\ntidyverse: Data Science packages\n\nConsistent syntax across functions\nReadable code tells story\nEfficient workflows for common tasks\n\nTibbles: enhanced data frames\n\nTraditional df: class(data)\nTibble: car_data &lt;- as_tibble(data)\nPretty print: Shows ONLY first 10 rows, displays cleaner than traditional df\nread_csv (read.csv for traditional df)\n\nEssential dplyr functions (data cleaning):\nselect(): choose columns - select(df, col1, col2)\nfilter(): choose rows - filter(df, condition)\n\nUse &, | symbol to join conditions (and/or)\n\nmutate(): create new variables - mutate(df, new_col = old_col / 1000)\n\nCan use case_when (if statement) argument: ex) car_df = mutate(car_df, case_when(age &gt; 20 ~ ‚Äúold‚Äù, TRUE ~ ‚Äúnot old‚Äù))\n\nsummarize(): calculate stats\ngroup_by(): operate on groups (usually goes together w/ summarize())\n\ngroup_by(col1) |&gt; summarize (n = n()) |&gt; mutate(freq = n / sum(n)) -&gt; calculates proportions (frequencies) for groups\n\nMust assign back to original or new df name to save the operation (always start with dataframe operating on in syntax)\nOthers:\n\nnames(): column names\nglimpse(): rows, columnns, some entries etc.\nClick on df to view in table format\nRename(): renames column: rename(df, new = old)\nRemove a column: select(df, -col)\n\n%&gt;% or |&gt;: Pipes -&gt; combines lines of codes, pass outputs through each pipe\n\nQuarto (better version of R Markdown) - publishing system that combines:\n\nCode (R, Python, etc.), Text (explanations, analysis, etc.), Output (plots, tables results, etc.)\nBenefits:\n\nReproducible research: Ccde and explanation in one place, others can re-run analysis, professional presentation\n\nComponents:\n\nYAML header: title, author, date, format\nR code chunks: load library, read_csv, etc.\nText (formatting: bold, italic, etc.)"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nWe learned how to clean data, transforming it to drive actionable insights. Datasets can be messy, and by learning these skills, we build foundational knowledge on how we can improve the quality of data for future analyses.\nWe learned how to create a Quarto portfolio as a professional way to organize and present our work. This is highly valuable for making our findings professional, clean, and easily accessible to peers, future employers, etc."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nI found the Quarto portfolio the most fascinating part of this lecture. It displays our work in a much cleaner format than in R-Markdown.\nI will be using this portfolio to highlight the work in this course. Additionally, I can use skills in making a Quatro portfolio for personal projects as well!"
  },
  {
    "objectID": "labs/lab0/lab0.html",
    "href": "labs/lab0/lab0.html",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "",
    "text": "Welcome to your first lab! In this (not graded) assignment, you‚Äôll practice the fundamental dplyr operations I overviewed in class using car sales data. This lab will help you get comfortable with:\n\nBasic data exploration\nColumn selection and manipulation\n\nCreating new variables\nFiltering data\nGrouping and summarizing\n\nInstructions: Copy this template into your portfolio repository under a lab_0/ folder, then complete each section with your code and answers. You will write the code under the comment section in each chunk. Be sure to also copy the data folder into your lab_0 folder."
  },
  {
    "objectID": "labs/lab0/lab0.html#data-structure-exploration",
    "href": "labs/lab0/lab0.html#data-structure-exploration",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.1 Data Structure Exploration",
    "text": "1.1 Data Structure Exploration\nExplore the structure of your data and answer these questions:\n\n# Use glimpse() to see the data structure\nglimpse(car_data)\n\nRows: 50,000\nColumns: 7\n$ Manufacturer          &lt;chr&gt; \"Ford\", \"Porsche\", \"Ford\", \"Toyota\", \"VW\", \"Ford‚Ä¶\n$ Model                 &lt;chr&gt; \"Fiesta\", \"718 Cayman\", \"Mondeo\", \"RAV4\", \"Polo\"‚Ä¶\n$ `Engine size`         &lt;dbl&gt; 1.0, 4.0, 1.6, 1.8, 1.0, 1.4, 1.8, 1.4, 1.2, 2.0‚Ä¶\n$ `Fuel type`           &lt;chr&gt; \"Petrol\", \"Petrol\", \"Diesel\", \"Hybrid\", \"Petrol\"‚Ä¶\n$ `Year of manufacture` &lt;dbl&gt; 2002, 2016, 2014, 1988, 2006, 2018, 2010, 2015, ‚Ä¶\n$ Mileage               &lt;dbl&gt; 127300, 57850, 39190, 210814, 127869, 33603, 866‚Ä¶\n$ Price                 &lt;dbl&gt; 3074, 49704, 24072, 1705, 4101, 29204, 14350, 30‚Ä¶\n\n# Check the column names\ncolnames(car_data)\n\n[1] \"Manufacturer\"        \"Model\"               \"Engine size\"        \n[4] \"Fuel type\"           \"Year of manufacture\" \"Mileage\"            \n[7] \"Price\"              \n\n# Look at the first few rows\nhead(car_data)\n\n# A tibble: 6 √ó 7\n  Manufacturer Model     `Engine size` `Fuel type` `Year of manufacture` Mileage\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n1 Ford         Fiesta              1   Petrol                       2002  127300\n2 Porsche      718 Caym‚Ä¶           4   Petrol                       2016   57850\n3 Ford         Mondeo              1.6 Diesel                       2014   39190\n4 Toyota       RAV4                1.8 Hybrid                       1988  210814\n5 VW           Polo                1   Petrol                       2006  127869\n6 Ford         Focus               1.4 Petrol                       2018   33603\n# ‚Ñπ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestions to answer: - How many rows and columns does the dataset have? - What types of variables do you see (numeric, character, etc.)? - Are there any column names that might cause problems? Why?\nYour answers: - Rows: 50,000 - Columns: 7\n- Variable types: Character, double - Problematic names:‚ÄúEngine size‚Äù, ‚ÄúFuel type‚Äù, ‚ÄúYear of manufacture‚Äù are problematic since they have spaces in their names. When we call them, we must wrap these names in quotation marks as shown above."
  },
  {
    "objectID": "labs/lab0/lab0.html#tibble-vs-data-frame",
    "href": "labs/lab0/lab0.html#tibble-vs-data-frame",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.2 Tibble vs Data Frame",
    "text": "1.2 Tibble vs Data Frame\nCompare how tibbles and data frames display:\n\n# Look at the tibble version (what we have)\ncar_data\n\n# A tibble: 50,000 √ó 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay‚Ä¶           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ‚Ñπ 49,990 more rows\n# ‚Ñπ 1 more variable: Price &lt;dbl&gt;\n\n# Convert to regular data frame and display\ncar_df &lt;- as.data.frame(car_data)\nhead(car_df, 50) # Changed for better render\n\n   Manufacturer      Model Engine size Fuel type Year of manufacture Mileage\n1          Ford     Fiesta         1.0    Petrol                2002  127300\n2       Porsche 718 Cayman         4.0    Petrol                2016   57850\n3          Ford     Mondeo         1.6    Diesel                2014   39190\n4        Toyota       RAV4         1.8    Hybrid                1988  210814\n5            VW       Polo         1.0    Petrol                2006  127869\n6          Ford      Focus         1.4    Petrol                2018   33603\n7          Ford     Mondeo         1.8    Diesel                2010   86686\n8        Toyota      Prius         1.4    Hybrid                2015   30663\n9            VW       Polo         1.2    Petrol                2012   73470\n10         Ford      Focus         2.0    Diesel                1992  262514\n11           VW       Golf         2.0    Diesel                2014   83047\n12          BMW         Z4         2.0    Petrol                1990  293666\n13           VW       Golf         1.2    Diesel                2007   92697\n14       Toyota       RAV4         2.2    Petrol                2007   79393\n15       Toyota      Yaris         1.4    Petrol                1998   97286\n16           VW       Golf         1.6    Diesel                1989  222390\n17       Toyota       RAV4         2.4    Hybrid                2003  117425\n18       Toyota      Yaris         1.2    Petrol                1992  245990\n19       Toyota       RAV4         2.0    Hybrid                2018   28381\n20           VW       Polo         1.2    Petrol                1998  155038\n21           VW       Golf         1.2    Hybrid                1987  121744\n22         Ford     Mondeo         1.6    Diesel                1996   77584\n23       Toyota      Prius         1.0    Hybrid                2003  115291\n24       Toyota      Prius         1.0    Hybrid                1990  238571\n25      Porsche        911         2.6    Petrol                2009   66273\n26       Toyota      Prius         1.8    Hybrid                2017   31958\n27      Porsche        911         3.5    Petrol                2005  151556\n28       Toyota      Yaris         1.2    Petrol                2002  179097\n29           VW       Golf         2.0    Petrol                2020   18985\n30       Toyota       RAV4         1.8    Hybrid                2002   66990\n31         Ford      Focus         1.0    Hybrid                2010   85131\n32         Ford     Fiesta         1.0    Petrol                2001  144731\n33           VW       Polo         2.0    Diesel                2008   97001\n34           VW       Polo         1.6    Petrol                2016   52409\n35         Ford     Fiesta         1.4    Petrol                2010  112714\n36           VW     Passat         2.0    Diesel                1992  198540\n37           VW     Passat         1.8    Diesel                1989  213162\n38          BMW         Z4         2.2    Petrol                2005  133174\n39           VW       Polo         1.6    Petrol                2008  129588\n40          BMW         Z4         2.0    Petrol                1990  148586\n41         Ford      Focus         2.0    Petrol                1995   91173\n42          BMW         M5         4.0    Petrol                2017   22759\n43           VW       Polo         1.4    Petrol                1990  261526\n44           VW     Passat         1.4    Diesel                1995  235594\n45          BMW         Z4         2.0    Petrol                1995   42759\n46           VW     Passat         1.8    Diesel                2005  103352\n47           VW     Passat         1.6    Petrol                2003   63372\n48       Toyota      Yaris         1.0    Petrol                2009   51787\n49         Ford     Fiesta         1.4    Petrol                2014   41495\n50       Toyota      Yaris         1.4    Petrol                2010   43111\n   Price\n1   3074\n2  49704\n3  24072\n4   1705\n5   4101\n6  29204\n7  14350\n8  30297\n9   9977\n10  1049\n11 17173\n12   719\n13  7792\n14 16026\n15  4046\n16   933\n17 11667\n18   720\n19 52671\n20  2118\n21  1890\n22  5667\n23  6512\n24   961\n25 41963\n26 38961\n27 19747\n28  2548\n29 36387\n30 13553\n31 12472\n32  2503\n33  8784\n34 17257\n35  6936\n36  1964\n37  1340\n38  8511\n39  5848\n40  2637\n41  5181\n42 97758\n43   522\n44  1439\n45  7873\n46  9633\n47 10001\n48  9689\n49 14721\n50 12928\n\n\nQuestion: What differences do you notice in how they print?\nYour answer: Tibble will only display the first 10 rows with total number of rows and columns as well as column types as additional info. It also only shows a certain number of columns (in this case, only 6 of 7 columns displayed)."
  },
  {
    "objectID": "labs/lab0/lab0.html#selecting-columns",
    "href": "labs/lab0/lab0.html#selecting-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.1 Selecting Columns",
    "text": "2.1 Selecting Columns\nPractice selecting different combinations of columns:\n\n# Select just Model and Mileage columns\nmodel_mile &lt;- select(car_data, Model, Mileage)\nhead(model_mile, 5)\n\n# A tibble: 5 √ó 2\n  Model      Mileage\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Fiesta      127300\n2 718 Cayman   57850\n3 Mondeo       39190\n4 RAV4        210814\n5 Polo        127869\n\n# Select Manufacturer, Price, and Fuel type\nmanuf_price_fuel &lt;- select(car_data, Manufacturer, Price, \"Fuel type\")\nhead(manuf_price_fuel, 5)\n\n# A tibble: 5 √ó 3\n  Manufacturer Price `Fuel type`\n  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;      \n1 Ford          3074 Petrol     \n2 Porsche      49704 Petrol     \n3 Ford         24072 Diesel     \n4 Toyota        1705 Hybrid     \n5 VW            4101 Petrol     \n\n# Challenge: Select all columns EXCEPT Engine Size\nexcept_engine &lt;- select(car_data, -\"Engine size\")\nhead(except_engine, 5)\n\n# A tibble: 5 √ó 6\n  Manufacturer Model      `Fuel type` `Year of manufacture` Mileage Price\n  &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Ford         Fiesta     Petrol                       2002  127300  3074\n2 Porsche      718 Cayman Petrol                       2016   57850 49704\n3 Ford         Mondeo     Diesel                       2014   39190 24072\n4 Toyota       RAV4       Hybrid                       1988  210814  1705\n5 VW           Polo       Petrol                       2006  127869  4101"
  },
  {
    "objectID": "labs/lab0/lab0.html#renaming-columns",
    "href": "labs/lab0/lab0.html#renaming-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.2 Renaming Columns",
    "text": "2.2 Renaming Columns\nLet‚Äôs fix a problematic column name:\n\n# Rename 'Year of manufacture' to year\ncar_data &lt;- car_data |&gt;\n    rename(\n    year = `Year of manufacture`,\n    engine_size = `Engine size`,\n    fuel = `Fuel type`\n  )\n\n# Check that it worked\nnames(car_data)\n\n[1] \"Manufacturer\" \"Model\"        \"engine_size\"  \"fuel\"         \"year\"        \n[6] \"Mileage\"      \"Price\"       \n\n\nQuestion: Why did we need backticks around Year of manufacture but not around year?\nYour answer: Because ‚Äòyear‚Äô has no spaces in its variable name."
  },
  {
    "objectID": "labs/lab0/lab0.html#calculate-car-age",
    "href": "labs/lab0/lab0.html#calculate-car-age",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.1 Calculate Car Age",
    "text": "3.1 Calculate Car Age\n\n# Create an 'age' column (2025 minus year of manufacture)\n# Create a mileage_per_year column \ncar_data &lt;- car_data |&gt;\n  mutate(age = 2025 - year,\n         mileage_per_year = Mileage / age)\n\n# Look at your new columns\nselect(car_data, Model, year, age, Mileage, mileage_per_year)\n\n# A tibble: 50,000 √ó 5\n   Model       year   age Mileage mileage_per_year\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1 Fiesta      2002    23  127300            5535.\n 2 718 Cayman  2016     9   57850            6428.\n 3 Mondeo      2014    11   39190            3563.\n 4 RAV4        1988    37  210814            5698.\n 5 Polo        2006    19  127869            6730.\n 6 Focus       2018     7   33603            4800.\n 7 Mondeo      2010    15   86686            5779.\n 8 Prius       2015    10   30663            3066.\n 9 Polo        2012    13   73470            5652.\n10 Focus       1992    33  262514            7955.\n# ‚Ñπ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/lab0.html#categorize-cars",
    "href": "labs/lab0/lab0.html#categorize-cars",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.2 Categorize Cars",
    "text": "3.2 Categorize Cars\n\n# Create a price_category column where if price is &lt; 15000, its is coded as budget, between 15000 and 30000 is midrange and greater than 30000 is mid-range (use case_when)\ncar_data &lt;- car_data |&gt;\n  mutate(price_category = case_when(\n    Price &lt; 15000 ~ \"budget\", \n    between(Price, 15000, 30000) ~ \"midrange\",\n    TRUE ~ \"luxury\"))\n\n# Check your categories select the new column and show it\nselect(car_data, price_category)\n\n# A tibble: 50,000 √ó 1\n   price_category\n   &lt;chr&gt;         \n 1 budget        \n 2 luxury        \n 3 midrange      \n 4 budget        \n 5 budget        \n 6 midrange      \n 7 budget        \n 8 luxury        \n 9 budget        \n10 budget        \n# ‚Ñπ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/lab0.html#basic-filtering",
    "href": "labs/lab0/lab0.html#basic-filtering",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.1 Basic Filtering",
    "text": "4.1 Basic Filtering\n\n# Find all Toyota cars\ntoyota &lt;- car_data |&gt;\n  filter(Manufacturer == \"Toyota\")\nhead(toyota, 5)\n\n# A tibble: 5 √ó 10\n  Manufacturer Model engine_size fuel    year Mileage Price   age\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Toyota       RAV4          1.8 Hybrid  1988  210814  1705    37\n2 Toyota       Prius         1.4 Hybrid  2015   30663 30297    10\n3 Toyota       RAV4          2.2 Petrol  2007   79393 16026    18\n4 Toyota       Yaris         1.4 Petrol  1998   97286  4046    27\n5 Toyota       RAV4          2.4 Hybrid  2003  117425 11667    22\n# ‚Ñπ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with mileage less than 30,000\nmile_less_30000 &lt;- car_data |&gt;\n  filter(Mileage &lt; 30000)\nhead(mile_less_30000, 5)\n\n# A tibble: 5 √ó 10\n  Manufacturer Model engine_size fuel    year Mileage Price   age\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Toyota       RAV4          2   Hybrid  2018   28381 52671     7\n2 VW           Golf          2   Petrol  2020   18985 36387     5\n3 BMW          M5            4   Petrol  2017   22759 97758     8\n4 Toyota       RAV4          2.4 Petrol  2018   24588 49125     7\n5 VW           Golf          2   Hybrid  2018   25017 36957     7\n# ‚Ñπ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find luxury cars (from price category) with low mileage\n# ASSUMING low mileage is less than 30000 from part 2\nluxury_low_mile &lt;- mile_less_30000 |&gt;\n  filter(price_category == \"luxury\")\nhead(luxury_low_mile, 5)\n\n# A tibble: 5 √ó 10\n  Manufacturer Model engine_size fuel    year Mileage Price   age\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Toyota       RAV4          2   Hybrid  2018   28381 52671     7\n2 VW           Golf          2   Petrol  2020   18985 36387     5\n3 BMW          M5            4   Petrol  2017   22759 97758     8\n4 Toyota       RAV4          2.4 Petrol  2018   24588 49125     7\n5 VW           Golf          2   Hybrid  2018   25017 36957     7\n# ‚Ñπ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "labs/lab0/lab0.html#multiple-conditions",
    "href": "labs/lab0/lab0.html#multiple-conditions",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.2 Multiple Conditions",
    "text": "4.2 Multiple Conditions\n\n# Find cars that are EITHER Honda OR Nissan\nhonda_nissan &lt;- car_data |&gt;\n    filter(Manufacturer %in% c(\"Honda\", \"Nissan\"))\nhead(honda_nissan, 5)\n\n# A tibble: 0 √ó 10\n# ‚Ñπ 10 variables: Manufacturer &lt;chr&gt;, Model &lt;chr&gt;, engine_size &lt;dbl&gt;,\n#   fuel &lt;chr&gt;, year &lt;dbl&gt;, Mileage &lt;dbl&gt;, Price &lt;dbl&gt;, age &lt;dbl&gt;,\n#   mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# NO NISSAN OR HONDA IN DATASET\n\n# Find cars with price between $20,000 and $35,000\nprice_20k_35k &lt;- car_data |&gt;\n  filter(Price &gt; 20000 & Price &lt; 35000)\nhead(price_20k_35k, 5)\n\n# A tibble: 5 √ó 10\n  Manufacturer Model  engine_size fuel    year Mileage Price   age\n  &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Ford         Mondeo         1.6 Diesel  2014   39190 24072    11\n2 Ford         Focus          1.4 Petrol  2018   33603 29204     7\n3 Toyota       Prius          1.4 Hybrid  2015   30663 30297    10\n4 Toyota       Prius          1.4 Hybrid  2016   43893 29946     9\n5 Toyota       Prius          1.4 Hybrid  2016   43130 30085     9\n# ‚Ñπ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find diesel cars less than 10 years old\ndiesel_less_10_year &lt;- car_data |&gt;\n  filter(fuel == \"Diesel\" & age &lt; 10)\nhead(diesel_less_10_year, 5)\n\n# A tibble: 5 √ó 10\n  Manufacturer Model  engine_size fuel    year Mileage Price   age\n  &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Ford         Fiesta         1   Diesel  2017   38370 16257     8\n2 VW           Passat         1.6 Diesel  2018   22122 36634     7\n3 VW           Passat         1.4 Diesel  2020   21413 39310     5\n4 BMW          X3             2   Diesel  2018   27389 44018     7\n5 Ford         Mondeo         2   Diesel  2016   51724 28482     9\n# ‚Ñπ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\nglimpse(diesel_less_10_year)\n\nRows: 2,040\nColumns: 10\n$ Manufacturer     &lt;chr&gt; \"Ford\", \"VW\", \"VW\", \"BMW\", \"Ford\", \"Porsche\", \"VW\", \"‚Ä¶\n$ Model            &lt;chr&gt; \"Fiesta\", \"Passat\", \"Passat\", \"X3\", \"Mondeo\", \"Cayenn‚Ä¶\n$ engine_size      &lt;dbl&gt; 1.0, 1.6, 1.4, 2.0, 2.0, 2.6, 1.2, 1.8, 1.4, 1.4, 1.4‚Ä¶\n$ fuel             &lt;chr&gt; \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Di‚Ä¶\n$ year             &lt;dbl&gt; 2017, 2018, 2020, 2018, 2016, 2019, 2018, 2016, 2020,‚Ä¶\n$ Mileage          &lt;dbl&gt; 38370, 22122, 21413, 27389, 51724, 20147, 37411, 2943‚Ä¶\n$ Price            &lt;dbl&gt; 16257, 36634, 39310, 44018, 28482, 76182, 19649, 3088‚Ä¶\n$ age              &lt;dbl&gt; 8, 7, 5, 7, 9, 6, 7, 9, 5, 7, 7, 9, 8, 3, 7, 3, 9, 7,‚Ä¶\n$ mileage_per_year &lt;dbl&gt; 4796.2500, 3160.2857, 4282.6000, 3912.7143, 5747.1111‚Ä¶\n$ price_category   &lt;chr&gt; \"midrange\", \"luxury\", \"luxury\", \"luxury\", \"midrange\",‚Ä¶\n\n\nQuestion: How many diesel cars are less than 10 years old?\nYour answer: 2040"
  },
  {
    "objectID": "labs/lab0/lab0.html#basic-summaries",
    "href": "labs/lab0/lab0.html#basic-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.1 Basic Summaries",
    "text": "5.1 Basic Summaries\n\n# Calculate average price by manufacturer\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 5 √ó 2\n  Manufacturer avg_price\n  &lt;chr&gt;            &lt;dbl&gt;\n1 BMW             24429.\n2 Ford            10672.\n3 Porsche         29104.\n4 Toyota          14340.\n5 VW              10363.\n\n# Calculate average mileage by fuel type\navg_mile_by_fuel &lt;- car_data |&gt;\n  group_by(fuel) |&gt;\n  summarize(avg_mile = mean(Mileage, na.rm = T))\n\navg_mile_by_fuel\n\n# A tibble: 3 √ó 2\n  fuel   avg_mile\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Diesel  112667.\n2 Hybrid  111622.\n3 Petrol  112795.\n\n# Count cars by manufacturer\ncount_cars_by_brand &lt;- car_data |&gt;\n  group_by(Manufacturer) |&gt;\n  summarize(count_car = n())\n\ncount_cars_by_brand\n\n# A tibble: 5 √ó 2\n  Manufacturer count_car\n  &lt;chr&gt;            &lt;int&gt;\n1 BMW               4965\n2 Ford             14959\n3 Porsche           2609\n4 Toyota           12554\n5 VW               14913"
  },
  {
    "objectID": "labs/lab0/lab0.html#categorical-summaries",
    "href": "labs/lab0/lab0.html#categorical-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.2 Categorical Summaries",
    "text": "5.2 Categorical Summaries\n\n# Frequency table for price categories\ncar_data |&gt;\n  count(price_category) |&gt;\n  mutate(percent = 100 * n / sum(n)) |&gt;\n  select(-n)\n\n# A tibble: 3 √ó 2\n  price_category percent\n  &lt;chr&gt;            &lt;dbl&gt;\n1 budget            68.1\n2 luxury            12.4\n3 midrange          19.6\n\ncar_data\n\n# A tibble: 50,000 √ó 10\n   Manufacturer Model      engine_size fuel    year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol  2002  127300  3074    23\n 2 Porsche      718 Cayman         4   Petrol  2016   57850 49704     9\n 3 Ford         Mondeo             1.6 Diesel  2014   39190 24072    11\n 4 Toyota       RAV4               1.8 Hybrid  1988  210814  1705    37\n 5 VW           Polo               1   Petrol  2006  127869  4101    19\n 6 Ford         Focus              1.4 Petrol  2018   33603 29204     7\n 7 Ford         Mondeo             1.8 Diesel  2010   86686 14350    15\n 8 Toyota       Prius              1.4 Hybrid  2015   30663 30297    10\n 9 VW           Polo               1.2 Petrol  2012   73470  9977    13\n10 Ford         Focus              2   Diesel  1992  262514  1049    33\n# ‚Ñπ 49,990 more rows\n# ‚Ñπ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\n\nName: Henry (Jack) Bader\nWhy I am taking this course: I want to deepen my understanding of geospatial data science techniques in the public sector that provides a different perspective than typical engineering data science courses.\n\n\n\n\n\nEmail: jbader14@seas.upenn.edu\nGitHub: jbader14"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Name: Henry (Jack) Bader\nWhy I am taking this course: I want to deepen my understanding of geospatial data science techniques in the public sector that provides a different perspective than typical engineering data science courses."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: jbader14@seas.upenn.edu\nGitHub: jbader14"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html",
    "href": "assignments/assignment4/assignment4.html",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "",
    "text": "Code\n# Load required packages\nlibrary(readr)\nlibrary(tidyverse)      # Data manipulation\nlibrary(sf)             # Spatial operations\nlibrary(here)           # Relative file paths\nlibrary(viridis)        # Color scales\nlibrary(terra)          # Raster operations (replaces 'raster')\nlibrary(spdep)          # Spatial dependence\nlibrary(FNN)            # Fast nearest neighbors\nlibrary(MASS)           # Negative binomial regression\nlibrary(patchwork)      # Plot composition (replaces grid/gridExtra)\nlibrary(knitr)          # Tables\nlibrary(kableExtra)     # Table formatting\nlibrary(classInt)       # Classification intervals\nlibrary(here)\n\n# Spatstat split into sub-packages\nlibrary(spatstat.geom)    # Spatial geometries\nlibrary(spatstat.explore) # Spatial exploration/KDE\n\n# Set options\noptions(scipen = 999)  # No scientific notation\nset.seed(5080)         # Reproducibility\n\n\n\n\nCode\n# Create consistent theme for visualizations\ntheme_plotting &lt;- function(base_size = 11) {\n  theme_minimal(base_size = base_size) +\n    theme(\n      plot.title = element_text(face = \"bold\", size = base_size + 1),\n      plot.subtitle = element_text(color = \"gray30\", size = base_size - 1),\n      legend.position = \"right\",\n      panel.grid.minor = element_blank(),\n      axis.text = element_blank(),\n      axis.title = element_blank()\n    )\n}"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#data-loading",
    "href": "assignments/assignment4/assignment4.html#data-loading",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "1.1 Data Loading",
    "text": "1.1 Data Loading\n\n\nCode\n# Load police districts (used for spatial cross-validation)\npoliceDistricts &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/24zt-jpfn?method=export&format=GeoJSON\", quiet = TRUE) |&gt;\n  st_transform('ESRI:102271') |&gt;\n  dplyr::select(District = dist_num)\n\n# Load police beats (smaller administrative units)\npoliceBeats &lt;- \n  st_read(\"https://data.cityofchicago.org/api/geospatial/n9it-hstw?method=export&format=GeoJSON\", quiet = TRUE) |&gt;\n  st_transform('ESRI:102271') |&gt;\n  dplyr::select(Beat = beat_num)\n\n\n\n\nCode\n# Load Chicago boundary\nchicagoBoundary &lt;- \n  st_read(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson\", quiet = TRUE) |&gt;\n  st_transform('ESRI:102271')\n\n\n\n\nCode\n# Load burglaries data\nburglaries &lt;- st_read(\"/Users/jack/Documents/GitHub/portfolio-setup-jbader14/assignments/assignment4/data/burglaries.shp\", quiet = TRUE) |&gt;\n  st_transform('ESRI:102271')\n\n\n\n\n\n\n\n\nNoteBurglary Data\n\n\n\nData Source:\nThe Chicago Police Department (CPD) provides the data on reported burglaries, where each row represents a distinct burglary incident in the city of Chicago from 2017.\nData Quality Concerns:\n\nPatrol Biases: Neighborhoods in Chicago that experience a higher volume of police patrolling may experience more reported burglaries as a result.\nUnder-reported / Missing Data: Some burglaries may not have been reported due to external circumstances, such as in neighborhoods with a higher distrust of law enforcement or ignored complaints.\nConsistent Reporting: The CPD may not report all burglary incidents into their database or possibly code them as different crimes either by error or intentionally.\n\nConsiderations:\nIt is important to consider the potential biases that arise in police reported data. These are crucial examples of potential ‚Äúdirty data‚Äù instances that must be recognized before continuing with our model building workflow. Crime data that is biased and non-neutral cannot be remedied by traditional technical data cleaning to seperate true crime from policing patterns, so we must proceed with caution.\nOur largest concern is the data may adversely affect over-policed communities in Chicago, potentially skewing our data to show these neighborhoods as more prominent ‚Äúburglary hotspots‚Äù. It may not be that these neighborhoods are truly more susceptible to burglaries but that they are more policed and reported. This is a classic case of a feedback loop where over-policed neighborhoods are identified as high-risk for burglaries by our algorithm. As a result, more police units are sent there leading to even more burglary reports, repeating the cycle.\n\n\n\n\nCode\n# Selected 311 data: All Streetlights Out\nstreetlights &lt;- read_csv(\"/Users/jack/Documents/GitHub/portfolio-setup-jbader14/assignments/assignment4/data/streetlights.csv\")"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#data-cleaning",
    "href": "assignments/assignment4/assignment4.html#data-cleaning",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "1.2 Data Cleaning",
    "text": "1.2 Data Cleaning\n\n\nCode\nstreetlights &lt;- streetlights |&gt;\n  # Filter out streetlights with no location data\n  filter(!is.na(Location)) |&gt;\n  # Create new variable creation_date in datetime type\n  mutate(\n    creation_date = mdy(`Creation Date`)\n  ) |&gt;\n  # Filter for only 2017 complaints\n  filter(\n    creation_date &gt;= as.Date(\"2017-01-01\"),\n    creation_date &lt;  as.Date(\"2018-01-01\")\n  )\n\n# Convert to sf\nstreetlights &lt;- st_as_sf(streetlights, coords = c(\"Longitude\", \"Latitude\"), crs = 4326) |&gt;\n  st_transform('ESRI:102271')\n\n\n\n\n\n\n\n\nNote311 Street Lights (All Out) Data\n\n\n\nData Source:\nThe city of Chicago provides data on their 311 service request line for various offenses. The street lights (all out) dataset represents incidents where there is an outage of 3 or more lights in a city circuit, where each circuit typically has 8-16 streetlights.\nModeling Choice of Street Lights (All Out):\nWe decided to use street lights (all out) as our 311 offense variable over other types such as graffiti or pothole incidents because we believe it is a better choice of a predictor for modeling burglary risk in Chicago. Assuming that most burglaries tend to occur at night, city blocks without working streetlights are seen as much more at risk for burglaries since it is harder to surveil the surrounding environment due to the lack of visibility. Also, streetlight outages can occur in distinct clustering patterns based on the dataset description that make it a suitable choice for modeling burglary hotspots.\nData Cleaning:\n\nFor spatial analysis, we filtered out any incidents that failed to report a location.\nTo remain consistent with the burglaries dataset, we filtered the street lights (all out) dataset to only include reports in the same year (2017).\n\nData Quality Concerns:\nOne potential concern is the presence of duplicate service requests. The general procedure is for the city to send an electrician to repair the broken lights in a circuit after a request is made. However, the city emphasizes that sometimes a second request is put out for the same outage if it was not repaired properly, leading to duplicate requests in our dataset. This issue can be tied to poor city infrastructure that can impact the data quality we are using in our analysis. We will make note of it as this could overestimate pockets with high frequencies of broken streetlights due to poor data reporting and repair management. Additionally as before, reporting of 311 incidents may not be consistent across neighborhoods in Chicago for external reasons such as a lack of time to report, lack of care, etc.\nConsiderations:\nIt is important to recognize these data quality concerns as we can use this variable as a proxy for dark city blocks with poor visibility but also recognizing that other variables such as duplicates and reporting inconsistencies may skew the accuracy of our analysis and models."
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#spatial-visualizations",
    "href": "assignments/assignment4/assignment4.html#spatial-visualizations",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "1.3 Spatial Visualizations",
    "text": "1.3 Spatial Visualizations\n\n\nCode\n# Simple point map\np1 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_sf(data = burglaries, color = \"#d62828\", size = 0.1, alpha = 0.4) +\n  labs(\n    title = \"Burglary Locations\",\n    subtitle = paste0(\"Chicago 2017, n = \", nrow(burglaries))\n  ) +\n  theme_plotting()\n\n# Density surface using modern syntax\np2 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_density_2d_filled(\n    data = data.frame(st_coordinates(burglaries)),\n    aes(X, Y),\n    alpha = 0.7,\n    bins = 8\n  ) +\n  scale_fill_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    guide = \"none\"  # Modern ggplot2 syntax (not guide = FALSE)\n  ) +\n  labs(\n    title = \"Density Surface\",\n    subtitle = \"Kernel density estimation\"\n  ) +\n  theme_plotting()\n\n# Combine plots using patchwork (modern approach)\np1 + p2 + \n  plot_annotation(\n    title = \"Spatial Distribution of Burglaries in Chicago\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Simple point map\np1 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_sf(data = streetlights, color = \"#d62828\", size = 0.1, alpha = 0.4) +\n  labs(\n    title = \"Streetlight Outages Locations\",\n    subtitle = paste0(\"Chicago 2019, n = \", nrow(streetlights))\n  ) +\n  theme_plotting()\n\n# Density surface using modern syntax\np2 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_density_2d_filled(\n    data = data.frame(st_coordinates(streetlights)),\n    aes(X, Y),\n    alpha = 0.7,\n    bins = 8\n  ) +\n  scale_fill_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    guide = \"none\"\n  ) +\n  labs(\n    title = \"Density Surface\",\n    subtitle = \"Kernel Density Estimation (KDE)\"\n  ) +\n  theme_plotting()\n\n# Combine plots using patchwork (modern approach)\np1 + p2 + \n  plot_annotation(\n    title = \"Spatial Distribution of Street Lights (All Out) in Chicago\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteSpatial Visualizations Interpretation\n\n\n\nWhat we did:\nWe created side by side point-geometry and KDE density maps for Chicago burglaries and street lights (all out):\n\nThe point estimates show the exact locations to demonstrate the distribution of burglaries and broken street lights throughout Chicago.\nThe Kernel Density Estimation (KDE) maps show a better high-level understanding of spatial patterns from a smoothed heatmap of burglaries and broken street lights.\n\nWhy this step matters:\nIt is important to visualize how our target and predictor variables are spatially distributed across Chicago before we move onto further analysis and eventual modeling phases. This step helped support our decision to use street lights (all out) to better understand burglaries since their heatmaps show similar hotspot regions.\nWhy this step is important:\nFor both variables, we see similar clusters of hotspots across the city of Chicago from the KDE plots. They both exhibit a large Northern hotspot and two Southern hotspots across the city that align close to one another. This evidence suggests that there could be a presence of positive correlation between neighborhoods that experience lots of streetlight blackouts and higher burglary incidents. We want correlation between our target and regressor variable when building a model to accurately predict burglary counts in Chicago."
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#create-a-500m-vs.-500m-fishnet-grid",
    "href": "assignments/assignment4/assignment4.html#create-a-500m-vs.-500m-fishnet-grid",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "2.1 Create a 500m vs.¬†500m Fishnet Grid",
    "text": "2.1 Create a 500m vs.¬†500m Fishnet Grid\n\n\nCode\n# Create 500m x 500m grid\nfishnet &lt;- st_make_grid(\n  chicagoBoundary,\n  cellsize = 500,  # 500 m\n  square = TRUE\n) |&gt;\n  st_sf() |&gt;\n  mutate(uniqueID = row_number())\n\n# Keep only cells that intersect Chicago\nfishnet &lt;- fishnet[chicagoBoundary, ]"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#aggregate-your-violations-to-grid-cells",
    "href": "assignments/assignment4/assignment4.html#aggregate-your-violations-to-grid-cells",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "2.2 Aggregate Your Violations to Grid Cells",
    "text": "2.2 Aggregate Your Violations to Grid Cells\n\n\nCode\n# Spatial join: which cell contains each burglary?\nburglaries_fishnet &lt;- st_join(burglaries, fishnet, join = st_within) |&gt;\n  st_drop_geometry() |&gt;\n  group_by(uniqueID) |&gt;\n  summarize(countBurglaries = n())\n\n# Join back to fishnet (cells with 0 burglaries will be NA)\nfishnet &lt;- fishnet |&gt;\n  left_join(burglaries_fishnet, by = \"uniqueID\") |&gt;\n  mutate(countBurglaries = replace_na(countBurglaries, 0))\n\n# Summary statistics\ncat(\"\\nBurglary count distribution:\\n\")\n\n\n\nBurglary count distribution:\n\n\nCode\nsummary(fishnet$countBurglaries)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   3.042   5.000  40.000 \n\n\nCode\ncat(\"\\nCells with zero burglaries:\", \n    sum(fishnet$countBurglaries == 0), \n    \"/\", nrow(fishnet),\n    \"(\", round(100 * sum(fishnet$countBurglaries == 0) / nrow(fishnet), 1), \"%)\\n\")\n\n\n\nCells with zero burglaries: 781 / 2458 ( 31.8 %)\n\n\n\n\nCode\n# Spatial join to determine how many streetlights complaints in each cell\nstreetlights_fishnet &lt;- st_join(streetlights, fishnet, join = st_within) |&gt;\n  st_drop_geometry() |&gt;\n  group_by(uniqueID) |&gt;\n  summarize(countStreetlights = n())\n\n# Join back to fishnet\nfishnet &lt;- fishnet |&gt;\n  left_join(streetlights_fishnet, by = \"uniqueID\") |&gt;\n  mutate(countStreetlights = replace_na(countStreetlights, 0))\n\n# Print Summary Stats\ncat(\"\\nStreetlight count distribution:\\n\")\n\n\n\nStreetlight count distribution:\n\n\nCode\nsummary(fishnet$countStreetlights)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   1.000   5.000   5.931   9.000  42.000 \n\n\nCode\ncat(\"\\nCells with zero streetlight incidents:\", \n    sum(fishnet$countStreetlights == 0), \n    \"/\", nrow(fishnet),\n    \"(\", round(100 * sum(fishnet$countStreetlights == 0) / nrow(fishnet), 1), \"%)\\n\")\n\n\n\nCells with zero streetlight incidents: 437 / 2458 ( 17.8 %)"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#visualize-the-count-distribution",
    "href": "assignments/assignment4/assignment4.html#visualize-the-count-distribution",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "2.3 Visualize the Count Distribution",
    "text": "2.3 Visualize the Count Distribution\n\n\nCode\n# get medians\nburg_med  &lt;- median(fishnet$countBurglaries, na.rm = TRUE)\nstreet_med &lt;- median(fishnet$countStreetlights, na.rm = TRUE)\n\n# burglary histogram\np_burg_hist &lt;- ggplot(fishnet, aes(x = countBurglaries)) +\n  geom_histogram(bins = 40, fill = \"maroon\", color = \"black\") +\n  geom_vline(xintercept = burg_med, linetype = 5) +\n  annotate(\n    \"text\",\n    x = burg_med,\n    y = Inf,\n    label = \"Median\",\n    vjust = 1.5,\n    hjust = -0.1,\n    size = 3\n  ) +\n  labs(\n    title = \"Burglaries\",\n    subtitle = \"For 500m fishnet cells\"\n  ) +\n  theme_plotting()\n\n# streetlight histogram \np_street_hist &lt;- ggplot(fishnet, aes(x = countStreetlights)) +\n  geom_histogram(bins = 40, fill = \"skyblue\", color = \"black\") +\n  geom_vline(xintercept = street_med, linetype = 5) +\n  annotate(\n    \"text\",\n    x = street_med,\n    y = Inf,\n    label = \"Median\",\n    vjust = 1.5,\n    hjust = -0.1,\n    size = 3\n  ) +\n  labs(\n    title = \"Streetlights\",\n    subtitle = \"For 500m fishnet cells\"\n  ) +\n  theme_plotting()\n\np_burg_hist + p_street_hist + \n    plot_annotation(\n    title = \"Aggregated Count Distributions in Chicago (2017)\"\n  )\n\n\n\n\n\n\n\n\n\n\n\nCode\np_burg &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countBurglaries), color = NA) +\n  geom_sf(data = chicagoBoundary, fill = NA, color = \"white\", linewidth = 1) +\n  scale_fill_viridis_c(\n    name = \"Burglaries\",\n    option = \"plasma\",\n    trans = \"sqrt\",\n    breaks = c(0, 1, 5, 10, 20, 40)\n  ) +\n  labs(\n    title = \"Burglary Counts by Grid Cell\",\n    subtitle = \"500m x 500m cells, Chicago 2017\"\n  ) +\n  theme_plotting()\n\np_street &lt;- ggplot() +\n  geom_sf(data = fishnet, aes(fill = countStreetlights), color = NA) +\n  geom_sf(data = chicagoBoundary, fill = NA, color = \"white\", linewidth = 1) +\n  scale_fill_viridis_c(\n    name = \"Streetlights\",\n    option = \"plasma\",\n    trans = \"sqrt\",\n    breaks = c(0, 1, 5, 10, 20, 40)\n  ) +\n  labs(\n    title = \"Streetlight Counts by Grid Cell\",\n    subtitle = \"500m x 500m cells, Chicago 2017\"\n  ) +\n  theme_plotting()\n\np_burg + p_street +\n  plot_annotation(\n    title = \"Counts Aggregated to 500m Fishnet Cells\"\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteFishnet Grid Creation (500m Aggregation)\n\n\n\nWhat we did:\n\nFirst, we created a fishnet grid of 500m by 500m to overlay over Chicago. Each cell in the grid is of equal size in the fishnet rather than if we overlayed by zip code or district.\nSecondly, we conducted a spatial join with burglaries and streetlight requests and computed the total count of each variable within each cell in the fishnet grid.\nLastly, we plotted spatial maps showing the aggregate counts of burglaries and broken streetlights side by side for comparison. We also included histogram plots to visualize the count distributions for each variable for grid cells as well.\n\nWhat we found:\nBoth counts of burglaries and broken streetlights are sparse. The cause of this is due to our cell size only being 500m by 500m. Some cells may cover non-residential areas where burglaries don‚Äôt occur or areas that do not contain any streetlights such as parks:\n\n781 of 2,458 cells (32%) had zero burglaries.\n437 of 2,458 cells (18%) had zero streetlight requests.\n\nWe also found that the aggregated counts for both burglaries and streetlight requests were right-skewed. This indicates that there are cells that experience abnormally high numbers of burglaries or streetlight outages, compared to the rest of the grid. This right-skew pattern is typical in count data as smaller counts are observed most frequently. The variance seems quite large for both metrics, which is an early indication that a negative binomial model may be preferred over a poisson regression.\nThere seems to be a wider spread of grid cells experiencing high streetlight outages than burglaries. This makes sense since there are more reports of broken streetlights than burglaries from the data we are using. Generally, there still seems to be some pattern between the two that we look to further explore.\nWhy this step is important:\nThe modeling decision to use a fishnet grid is necessary for modeling burglaries and streetlights as areas rather than points for a spatial regression. The histogram plots confirmed that both burglaries and broken streetlights fall under count data; in other words, there are no negative or non-integer values for these two variables.\nFor the fishnet grid, having cells of equal area and identical shape makes our aggregated counts of burglaries and streetlights across cells more directly comparable. If we had used zip codes or districts, there would be an inherent size bias where larger areas experience more burglaries and streetlights. A disadvantage of this approach is that these grids are not representative of real-world social or policy boundaries. We lose out on neighborhood specific qualities that may be beneficial in modeling burglary activity. Also, it may be harder to identify specific districts of need that require target policies to address burglary concerns."
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#calculate-k-nearest-neighbor-features",
    "href": "assignments/assignment4/assignment4.html#calculate-k-nearest-neighbor-features",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "3.1 Calculate k-nearest neighbor features",
    "text": "3.1 Calculate k-nearest neighbor features\n\n\nCode\n# Calculate mean distance to 3 nearest reports of broken streetlights\n\n# Get coordinates\nstreetlights_coords &lt;- st_coordinates(streetlights)\nfishnet_coords &lt;- st_coordinates(st_centroid(fishnet))\n\n# Calculate k nearest neighbors and distances\nknn_results &lt;- get.knnx(streetlights_coords, fishnet_coords, k = 3)\n\n# Add to fishnet\nfishnet &lt;- fishnet |&gt;\n  mutate(\n    streetlights_knn = rowMeans(knn_results$nn.dist)\n  )\n\ncat(\"\\nK-NN (K = 3) for closest broken streetlights distribution:\\n\")\n\n\n\nK-NN (K = 3) for closest broken streetlights distribution:\n\n\nCode\nsummary(fishnet$streetlights_knn)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  11.12  116.95  173.40  244.12  274.62 1651.04"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#perform-local-morans-i-analysis",
    "href": "assignments/assignment4/assignment4.html#perform-local-morans-i-analysis",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "3.2 Perform Local Moran‚Äôs I Analysis",
    "text": "3.2 Perform Local Moran‚Äôs I Analysis\n\n\nCode\n# Function to calculate Local Moran's I\ncalculate_local_morans &lt;- function(data, variable, k = 5) {\n  \n  # Create spatial weights\n  coords &lt;- st_coordinates(st_centroid(data))\n  neighbors &lt;- knn2nb(knearneigh(coords, k = k))\n  weights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n  \n  # Calculate Local Moran's I\n  local_moran &lt;- localmoran(data[[variable]], weights)\n  \n  # Classify clusters\n  mean_val &lt;- mean(data[[variable]], na.rm = TRUE)\n  \n  data |&gt;\n    mutate(\n      local_i = local_moran[, 1],\n      p_value = local_moran[, 5],\n      is_significant = p_value &lt; 0.05,\n      \n      moran_class = case_when(\n        !is_significant ~ \"Not Significant\",\n        local_i &gt; 0 & .data[[variable]] &gt; mean_val ~ \"High-High\",\n        local_i &gt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-Low\",\n        local_i &lt; 0 & .data[[variable]] &gt; mean_val ~ \"High-Low\",\n        local_i &lt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-High\",\n        TRUE ~ \"Not Significant\"\n      )\n    )\n}\n\n\n\n\nCode\n# Apply to graffiti\nfishnet &lt;- calculate_local_morans(fishnet, \"countStreetlights\", k = 5)"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#identify-hot-spots-cold-spots",
    "href": "assignments/assignment4/assignment4.html#identify-hot-spots-cold-spots",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "3.3 Identify Hot Spots & Cold Spots",
    "text": "3.3 Identify Hot Spots & Cold Spots\n\n\nCode\n# Visualize hot spots\nggplot() +\n  geom_sf(\n    data = fishnet, \n    aes(fill = moran_class), \n    color = NA\n  ) +\n  scale_fill_manual(\n    values = c(\n      \"High-High\" = \"#d7191c\",\n      \"High-Low\" = \"#fdae61\",\n      \"Low-High\" = \"#abd9e9\",\n      \"Low-Low\" = \"#2c7bb6\",\n      \"Not Significant\" = \"gray90\"\n    ),\n    name = \"Cluster Type\"\n  ) +\n  labs(\n    title = \"Local Moran's I: Streetlight Outage Clusters\",\n    subtitle = \"High-High = Hot spots of disorder\"\n  ) +\n  theme_plotting()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Get centroids of \"High-High\" cells (hot spots)\nhotspots &lt;- fishnet %&gt;%\n  filter(moran_class == \"High-High\") %&gt;%\n  st_centroid()\n\n# Calculate distance from each cell to nearest hot spot\nif (nrow(hotspots) &gt; 0) {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(\n      dist_to_hotspot = as.numeric(\n        st_distance(st_centroid(fishnet), hotspots %&gt;% st_union())\n      )\n    )\n  cat(\"  - Number of hot spot cells:\", nrow(hotspots), \"\\n\")\n} else {\n  fishnet &lt;- fishnet %&gt;%\n    mutate(dist_to_hotspot = 0)\n  cat(\"‚ö† No significant hot spots found\\n\")\n}\n\n\n  - Number of hot spot cells: 186 \n\n\n\n\n\n\n\n\nNoteSpatial Features\n\n\n\nWhat we did:\nIn this section, we engineered spatial features that help us get a better understanding of the distribution of burglary clusters and its relationship to streetlight outages in Chicago.\n\nFirst, we created the variable streetlights_knn, which is the average distance to the nearest three streetlight outage reports for each cell centroid in our fishnet grid.\nSecondly, we used Local Moran‚Äôs I to gain a better understanding of the distribution of streetlight outage clusters throughout Chicago, identifying potential hotspots, coldspots, and outliers.\nLastly, we constructed a variable dist_to_hotspot as the distance from a cell to the nearest streetlight outage hotspot.\n\nWhat we found:\n\nThe Local Moran‚Äôs I map shows streetlight outage hotspots (high-high, red), coldspots (low-low, blue), and outliers (low-high, light blue and high-low, orange). The colored sections are statistically significant whereas non-colored sections are due to random chance as they did not pass the permutation test for significance.\n\nWe see a large cluster of low-low coldspots (blue) in south Chicago where these cells do not experience many streetlight outages, and their surrounding cells also do not observe many outages. In other words, the blue cells and their neighbor cells experience below-average outages compared to all cells in the grid.\nThere are a considerable amount of smaller high-high hotspots (red) scattered throughout the Central Chicago area. As the name suggests, the red cells experience above average streetlight outages and their neighbors do as well.\nThere are few low-high or high-low outliers throughout Chicago. Intuitively, this makes sense due to spatial dependence of outages. Streetlight outages are either likely to cluster in the same areas or not occur much at all in the same areas.\n\nOur engineered spatial features of streetlights_knn and dist_to_hotspot will provide our model with more detail than just using the aggregate number of streetlight outages in each cell. Exploring the spatial relationships and dependencies between burglaries and streetlight outages can provide our model with a better understanding in making accurate predictions.\n\nWhy this step is important:\nFor our modeling workflow, we need to take into account spatial features as indicators of disorder that can help us in predicting burglaries in Chicago. The fundamental idea is that neighborhoods share similar crime levels in historical data. Creating spatial features can help us model patterns in burglary activity. Incorporating spatial features can better account for spatial dependence, generate more accurate predictions, and capture local spillover effects:\n\nstreetlights_knn: This variable supports our intuition that more burglaries may occur in areas that are closer to reported multiple streetlight blackouts where visibility is dampened. Our goal is that this feature is a strong predictor of burglaries in our modeling phase.\nLocal Moran‚Äôs I: This method allows us to visualize significant streetlight outage clusters across Chicago. These hotspots are used as a proxy for disorder that can drive up burglary risk in that area, reflective of the Broken Windows theory.\ndist_to_hotspot: We take our findings from the Local Moran‚Äôs I test to engineer this feature to the nearest streetlight outage hotspot. This is another metric to use in conjunction with our knn feature to bolster the spatial predictive power of streetlight outages in our model."
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#fit-poisson-regression",
    "href": "assignments/assignment4/assignment4.html#fit-poisson-regression",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "4.1 Fit Poisson Regression",
    "text": "4.1 Fit Poisson Regression\n\n\nCode\n# Join district information to fishnet\nfishnet &lt;- st_join(\n  fishnet,\n  policeDistricts,\n  join = st_within,\n  left = TRUE\n) %&gt;%\n  filter(!is.na(District))  # Remove cells outside districts\n\n\n\n\nCode\n# Create clean modeling dataset\nfishnet_model &lt;- fishnet %&gt;%\n  st_drop_geometry() %&gt;%\n  dplyr::select(\n    uniqueID,\n    District,\n    countBurglaries,\n    countStreetlights,\n    streetlights_knn,\n    dist_to_hotspot\n  ) %&gt;%\n  na.omit()  # Remove any remaining NAs\n\n\n\n\nCode\n# Fit Poisson regression\nmodel_poisson &lt;- glm(\n  countBurglaries ~ countStreetlights + streetlights_knn + \n    dist_to_hotspot,\n  data = fishnet_model,\n  family = \"poisson\"\n)\n\n# Summary\nsummary(model_poisson)\n\n\n\nCall:\nglm(formula = countBurglaries ~ countStreetlights + streetlights_knn + \n    dist_to_hotspot, family = \"poisson\", data = fishnet_model)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.7277  -1.6005  -0.6064   0.7202  11.1338  \n\nCoefficients:\n                      Estimate   Std. Error z value             Pr(&gt;|z|)    \n(Intercept)        1.907111377  0.048699661  39.161 &lt; 0.0000000000000002 ***\ncountStreetlights  0.008627022  0.002779913   3.103              0.00191 ** \nstreetlights_knn  -0.002985175  0.000176953 -16.870 &lt; 0.0000000000000002 ***\ndist_to_hotspot   -0.000111247  0.000009055 -12.285 &lt; 0.0000000000000002 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 6710.3  on 1707  degrees of freedom\nResidual deviance: 5491.0  on 1704  degrees of freedom\nAIC: 9559.4\n\nNumber of Fisher Scoring iterations: 6"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#fit-negative-binomial-regression",
    "href": "assignments/assignment4/assignment4.html#fit-negative-binomial-regression",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "4.2 Fit Negative Binomial Regression",
    "text": "4.2 Fit Negative Binomial Regression\n\n\nCode\n# Calculate dispersion parameter\ndispersion &lt;- sum(residuals(model_poisson, type = \"pearson\")^2) / \n              model_poisson$df.residual\n\ncat(\"Dispersion parameter:\", round(dispersion, 2), \"\\n\")\n\n\nDispersion parameter: 3.59 \n\n\nCode\ncat(\"Rule of thumb: &gt;1.5 suggests overdispersion\\n\")\n\n\nRule of thumb: &gt;1.5 suggests overdispersion\n\n\nCode\nif (dispersion &gt; 1.5) {\n  cat(\"‚ö† Overdispersion detected! Consider Negative Binomial model.\\n\")\n} else {\n  cat(\"‚úì Dispersion looks okay for Poisson model.\\n\")\n}\n\n\n‚ö† Overdispersion detected! Consider Negative Binomial model.\n\n\n\n\nCode\n# Fit Negative Binomial model\nmodel_nb &lt;- glm.nb(\n  countBurglaries ~ countStreetlights + streetlights_knn + \n    dist_to_hotspot,\n  data = fishnet_model\n)\n\n# Summary\nsummary(model_nb)\n\n\n\nCall:\nglm.nb(formula = countBurglaries ~ countStreetlights + streetlights_knn + \n    dist_to_hotspot, data = fishnet_model, init.theta = 1.381058742, \n    link = log)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-2.2460  -1.0439  -0.3428   0.3643   4.4867  \n\nCoefficients:\n                     Estimate  Std. Error z value             Pr(&gt;|z|)    \n(Intercept)        2.07602154  0.09440722  21.990 &lt; 0.0000000000000002 ***\ncountStreetlights  0.00542866  0.00584517   0.929                0.353    \nstreetlights_knn  -0.00412560  0.00030961 -13.325 &lt; 0.0000000000000002 ***\ndist_to_hotspot   -0.00008647  0.00001553  -5.569         0.0000000257 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for Negative Binomial(1.3811) family taken to be 1)\n\n    Null deviance: 2338.8  on 1707  degrees of freedom\nResidual deviance: 1876.4  on 1704  degrees of freedom\nAIC: 7750.8\n\nNumber of Fisher Scoring iterations: 1\n\n              Theta:  1.3811 \n          Std. Err.:  0.0743 \n\n 2 x log-likelihood:  -7740.7580"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#model-comparison-aic",
    "href": "assignments/assignment4/assignment4.html#model-comparison-aic",
    "title": "Assignment 4: Spatial Predictive Analysis",
    "section": "4.3 Model Comparison (AIC)",
    "text": "4.3 Model Comparison (AIC)\n\n\nCode\n# Compare AIC (lower is better)\ncat(\"\\nModel Comparison:\\n\")\n\n\n\nModel Comparison:\n\n\nCode\ncat(\"Poisson AIC:\", round(AIC(model_poisson), 1), \"\\n\")\n\n\nPoisson AIC: 9559.4 \n\n\nCode\ncat(\"Negative Binomial AIC:\", round(AIC(model_nb), 1), \"\\n\")\n\n\nNegative Binomial AIC: 7750.8 \n\n\n\n\n\n\n\n\nNoteCount Regression Models: Poisson vs.¬†Negative Binomial\n\n\n\nWhat we did:\n\nFirst, we conducted a spatial join between our fishnet dataset with police boundaries to use in part 5 for spatial cross-validation. We also created a seperate dataset for modeling that selected relevant columns while dropping shape geometry and remaining nulls.\nWe chose two regression models for count data, Poisson and Negative Binomial, to model burglaries in our fishnet grid. We checked for overdispersion as a clear indicator that the Negative Binomial model is more suitable than the Poisson.\nOur predictor variables for both regression models were:\n\nThe number of reported streetlight outages (countStreetlights)\nThe average distance to the three nearest streetlight outages (streetlights_knn)\nThe distance to the nearest streetlight hotspot (dist_to_hotspot)\n\nLastly, we used Akaike Information Criteria (AIC) as a final evaluation metric to compare our models.\n\nWhat we found:\n\nFor both models, we found that the spatial regressors streetlight_knn and dist_to_hotspot were statistically significant at any reasonable confidence level with a negative effect on burglary counts. In other words, burglaries tended to be lower in locations that were further away from nearby streetlight outages or outage hotspots.\nConversely, streetlight counts were not statistically significant for the Negative Binomial model with a positive effect on predicted burglary counts. This supports our intuition that proximity to streetlight outages (spatial features) are much better predictors to use in our models than simple counts. If this variable were significant, it would suggest that more streetlight outages in a cell leads to a higher predicted burglary count.\nWe computed the overdispersion parameter to be 3.59, greater than our 1.5 threshold, suggesting a presence of overdispersion in our model. This violates the key assumption that variance = mean in a Poisson model, suggesting that the Negative Binomial model is the better choice. This is expected with crime data, which tends to have high variation.\nWe compared AIC scores across both models and found that the Negative Binomial model produces a better fit with a lower score of 7750.8 (Poisson with 9559.4). This confirms our initial findings from the overdispersion check that the Negative Binomial is the preferred model. This makes sense since the Negative Binomial model incorporates a dispersion parameter to capture excess variance in the modeling.\n\nWhy this step is important:\nFrom our count modeling phase, we gain a better understanding of the importance of spatial features in building a predictive model. We are also able to test our hypothesis of overdispersion to build our intuition that Negative Binomial models are better suited in most cases for crime-based count data. It also builds good evaluation practice in our modeling workflow using metrics such as AIC to compare goodness of fit to confirm our findings from the overdispersion test."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html",
    "href": "assignments/assignment1/assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the NJ Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an ‚ÄúAssignments‚Äù menu."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#scenario",
    "href": "assignments/assignment1/assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the NJ Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#learning-objectives",
    "href": "assignments/assignment1/assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#submission-instructions",
    "href": "assignments/assignment1/assignment1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an ‚ÄúAssignments‚Äù menu."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#data-retrieval",
    "href": "assignments/assignment1/assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\nnj_inc_pop &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n        median_income = \"B19013_001\",\n        total_pop = \"B01003_001\"\n  ),\n  state = my_state,\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\nnj_inc_pop &lt;- nj_inc_pop |&gt;\n  # Remove state name and \"County\"\n  mutate(\n    county_name = str_remove(NAME, \", New Jersey\"),\n    county_name = str_remove(county_name, \" County\")\n  ) |&gt;\n  # Drop NAME column\n  select(-NAME)\n\n# Display the first few rows\nhead(nj_inc_pop, 5)\n\n# A tibble: 5 √ó 6\n  GEOID median_incomeE median_incomeM total_popE total_popM county_name\n  &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      \n1 34001          73113           1917     274339         NA Atlantic   \n2 34003         118714           1607     953243         NA Bergen     \n3 34005         102615           1436     461853         NA Burlington \n4 34007          82005           1414     522581         NA Camden     \n5 34009          83870           3707      95456         NA Cape May"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#data-quality-assessment",
    "href": "assignments/assignment1/assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\nnj_reli &lt;- nj_inc_pop |&gt;\n  mutate(\n    # Compute MOE percentage\n    moe_percent = round((median_incomeM / median_incomeE) * 100, 2),\n    # Create reliability categories\n    reliability = case_when(\n      moe_percent &lt; 5 ~ \"High Confidence\",\n      moe_percent &gt;= 5 & moe_percent &lt;= 10 ~ \"Moderate\",\n      moe_percent &gt; 10 ~ \"Low Confidence\"\n    ),\n    # Flag when MOE is greater than 10%\n    moe_flag = moe_percent &gt; 10\n  )\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\nnj_reli_summary &lt;- nj_reli |&gt;\n  # Count the number of counties per reliability category\n  count(reliability, name = \"n\") |&gt;\n  # Add percentages - count(county) / count(all)\n  mutate(\n  reliability_percent = round(100 * n / sum(n), 1),\n  reliability_percent = paste0(reliability_percent, \"%\")\n  ) \n\n# Display summary table\nkable(nj_reli_summary,\n    caption = \"County Reliability Summary\",\n    col.names = c(\"Reliability\", \"Count\", \"Percent\"))\n\n\nCounty Reliability Summary\n\n\nReliability\nCount\nPercent\n\n\n\n\nHigh Confidence\n21\n100%"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#high-uncertainty-counties",
    "href": "assignments/assignment1/assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\nnj_top_moe &lt;- nj_reli |&gt;\n  # Sort by DESC MOE percent\n  arrange(desc(moe_percent)) |&gt;\n  # Select top 5 counties\n  slice_head(n = 5) |&gt;\n  # Select columns\n  select(county_name, median_incomeE, median_incomeM, moe_percent, reliability)\n\n# Format as table with kable() - include appropriate column names and caption\nkable(nj_top_moe,\n      col.names = c(\"County\", \"Median Income\", \"MOE\", \"MOE %\", \"Reliabilty\"),\n      caption = \"NJ Counties with the Highest Income Data Uncertainty\",\n      format.args = list(big.mark = \",\"))\n\n\nNJ Counties with the Highest Income Data Uncertainty\n\n\nCounty\nMedian Income\nMOE\nMOE %\nReliabilty\n\n\n\n\nCape May\n83,870\n3,707\n4.42\nHigh Confidence\n\n\nSalem\n73,378\n3,047\n4.15\nHigh Confidence\n\n\nCumberland\n62,310\n2,205\n3.54\nHigh Confidence\n\n\nAtlantic\n73,113\n1,917\n2.62\nHigh Confidence\n\n\nGloucester\n99,668\n2,605\n2.61\nHigh Confidence\n\n\n\n\n\nData Quality Commentary:\nIn New Jersey, all counties have a median income MOE % below our threshold of 5%, indicating that the reporting of income date is highly reliable based on our standards. Even though these fall within high confidence, we should still proceed with caution when using this data for algorithmic decision-making. A pattern amongst the top highest MOE % revealed that all five counties make up the most southern counties in New Jersey.\nFor Cape May and Atlantic counties, a potential cause for higher MOE % stems from their economies being highly reliant on tourism for many residents. Especially, when you consider their location at the Jersey Shore, the flow of tourists is highly dependent on the season (notably, Summer), which can lead to further uncertainty in income reporting.\nCumberland, Salem, and Atlantic counties represent the three lowest counties in median income from this data. It may be the case that these counties may exhibit larger margins of error due to more residents working lower paying, hourly jobs that skew the reporting of median income, leading to higher MOE %."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#focus-area-selection",
    "href": "assignments/assignment1/assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\nNote from Jack: All counties in NJ have high confidence levels. I checked with Prof Delmelle, and she said that it was fine to continue with my analysis for NJ. I chose Cape May (MOE = 4.42%, Beach), Essex (2.00%. North Jersey), and Burlington (MOE = 1.40%, South Jersey / Philly) counties for this section to get a diverse selection of MOE percentages and geographical locations.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\nselected_countries &lt;- nj_reli |&gt;\n  filter(county_name %in% c(\"Cape May\", \"Essex\", \"Burlington\")) |&gt;\n  # Select county name, median income, MOE percentage, reliability category\n  select(county_name, median_incomeE, moe_percent, reliability)\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nkable(selected_countries,\n      caption = \"Selected Counties\",\n      col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\"))\n\n\nSelected Counties\n\n\nCounty\nMedian Income\nMOE %\nReliability\n\n\n\n\nBurlington\n102615\n1.40\nHigh Confidence\n\n\nCape May\n83870\n4.42\nHigh Confidence\n\n\nEssex\n73785\n2.00\nHigh Confidence\n\n\n\n\n\nComment on the output: Burlington county has the highest median income with the lowest MOE %, indicating that it is the safest out of the three for algorithmic decision-making. As stated previously, Cape May has a high MOE %, relative to the rest of NJ, due to its tourist driven economy and volatility in income during a calendar year. Essex county has a moderately low MOE % and median income, revealing that lower income communities may not always have higher MOE %. Although, Essex could be an outlier."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#tract-level-demographics",
    "href": "assignments/assignment1/assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You‚Äôll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\nwhite &lt;- \"B03002_003\"\nblack &lt;- \"B03002_004\"\nhispanic &lt;- \"B03002_012\"\ntotal_pop &lt;- \"B03002_001\"\n\n# Set county codes (GEOID): Burlington, Cape May, Essex\n# Remove \"34\" from start of GEOID code\ngeoid_codes &lt;- c(\"005\", \"009\", \"013\")\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\nnj_tract &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n        white = white,\n        black = black,\n        hispanic = hispanic,\n        total_pop = total_pop\n  ),\n  state = my_state, \n  county = geoid_codes,\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n) \n\n# Calculate percentage of each group using mutate()\nnj_tract &lt;- nj_tract |&gt;\n  # Create percentages for white, Black, and Hispanic populations\n  mutate(\n    white_percent = round(100 * whiteE / total_popE, 2),\n    black_percent = round(100 * blackE / total_popE, 2),\n    hispanic_percent = round(100 * hispanicE / total_popE, 2)) |&gt;\n    # Add readable tract and county name columns using str_extract() or similar                \n    mutate(\n      # Tract (RegEx)\n      tract = str_remove(str_extract(NAME, \"Census Tract\\\\s*[0-9]+(?:\\\\.[0-9]+)?\"),\"^Census Tract\\\\s*\"),\n\n      # County (RegEx)\n      county = str_extract(NAME, \";\\\\s*[^;]+\\\\s*;\") |&gt;\n               str_remove(\"^;\\\\s*\") |&gt;\n               str_remove(\"\\\\s*;$\") |&gt;\n               str_trim(),\n      \n      county = str_remove(county, \"\\\\s+County\\\\b\") |&gt;\n               str_trim()\n  ) \n\n# head(nj_tract)"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#demographic-analysis",
    "href": "assignments/assignment1/assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\nhigh_hispanic &lt;- nj_tract |&gt;\n  arrange(desc(hispanic_percent)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(county, tract, hispanic_percent)\n\nkable(high_hispanic,\n    caption = \"Highest Percentage of Hispanic Residents\",\n    col.names = c(\"County\", \"Tract\", \"Hispanic %\"))\n\n\nHighest Percentage of Hispanic Residents\n\n\nCounty\nTract\nHispanic %\n\n\n\n\nEssex\n97\n89.51\n\n\n\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\navg_demo &lt;- nj_tract |&gt;\n  group_by(county) |&gt;\n  summarize(\n    num_tracts = n(),\n    avg_white = round(mean(white_percent, na.rm = TRUE), 2),\n    avg_black = round(mean(black_percent, na.rm = TRUE), 2),\n    avg_hispanic = round(mean(hispanic_percent, na.rm = TRUE), 2)\n  )\n\n# Create a nicely formatted table of your results using kable()\nkable(avg_demo,\n      col.names = c(\"County\", \"Number of Tracts\", \"Average White %\", \"Average Black %\", \"Average Hispanic %\"),\n      caption = \"Average Demographics for Burlington, Cape May, and Essex Counties (NJ)\",\n      format.args = list(big.mark = \",\"))\n\n\nAverage Demographics for Burlington, Cape May, and Essex Counties (NJ)\n\n\n\n\n\n\n\n\n\nCounty\nNumber of Tracts\nAverage White %\nAverage Black %\nAverage Hispanic %\n\n\n\n\nBurlington\n117\n62.77\n17.41\n9.79\n\n\nCape May\n33\n86.01\n2.83\n7.66\n\n\nEssex\n211\n24.98\n41.78\n23.42"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\nnj_tract &lt;- nj_tract |&gt;\n  mutate(\n    # Calculate MOE percentages for white, Black, and Hispanic variables\n    # Hint: use the same formula as before (margin/estimate * 100)\n    # Need if/else some tracts show 0 population for a demographic\n    white_moe_percent = if_else(whiteE &gt; 0, round(100 * whiteM / whiteE, 2), NA_real_),\n    black_moe_percent = if_else(blackE &gt; 0, round((blackM / blackE) * 100, 2), NA_real_),\n    hispanic_moe_percent = if_else(hispanicE &gt; 0, round((whiteM / whiteE) * 100, 2), NA_real_)\n  )\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\nflag_tracts &lt;- nj_tract |&gt;\n   mutate(\n     flag_moe = ifelse(\n      coalesce(white_moe_percent &gt; 10, FALSE) |\n      coalesce(black_moe_percent &gt; 10, FALSE) |\n      coalesce(hispanic_moe_percent &gt; 10, FALSE),\n      \"High MOE\", \"Low MOE\"\n     )\n   )\n\n# Create summary statistics showing how many tracts have data quality issues\nflag_tracts_summary &lt;- flag_tracts |&gt;\n  summarize(\n    total_tracts = n(),\n    flagged_tracts = sum(flag_moe == \"High MOE\", na.rm = TRUE),\n    flagged_percents = round(100 * flagged_tracts / total_tracts, 2)\n  )\n\nkable(flag_tracts_summary, \n      col.names = c(\"Total Tracts\", \"Flagged Tracts\", \"Flagged %\"),\n      caption = \"Summary of Tracts in Burlington, Cape May, Essex counties with High MOE (Any Demo &gt; 15%)\",\n      format.args = list(big.mark = \",\"))\n\n\nSummary of Tracts in Burlington, Cape May, Essex counties with High MOE (Any Demo &gt; 15%)\n\n\nTotal Tracts\nFlagged Tracts\nFlagged %\n\n\n\n\n361\n355\n98.34"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#pattern-analysis",
    "href": "assignments/assignment1/assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Calculate population percentages for each tract\nflag_tracts &lt;- flag_tracts %&gt;%\n  mutate(\n    white_pop_percent = if_else(total_popE &gt; 0, round(100 * whiteE / total_popE, 2), NA_real_),\n    black_pop_percent = if_else(total_popE &gt; 0, round(100 * blackE / total_popE, 2), NA_real_),\n    hispanic_pop_percent = if_else(total_popE &gt; 0, round(100 * hispanicE / total_popE, 2), NA_real_)\n  )\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\nflag_tracts_group &lt;- flag_tracts |&gt;\n  # Group by MOE threshold\n  group_by(flag_moe) |&gt;\n  summarize(\n    # Total number of tracts\n    num_tracts = n(),\n    \n    # Avg population sizes for each demographic group\n    avg_white_pop = round(mean(whiteE, na.rm = TRUE), 2),\n    avg_black_pop = round(mean(blackE, na.rm = TRUE), 2),\n    avg_hispanic_pop = round(mean(hispanicE, na.rm = TRUE), 2),\n    \n    # Avg Population percentage for each demographic group\n    avg_white_pop_percent = round(mean(white_pop_percent, na.rm = TRUE), 2),\n    avg_black_pop_percent = round(mean(black_pop_percent, na.rm = TRUE), 2),\n    avg_hispanic_pop_percent = round(mean(hispanic_pop_percent, na.rm = TRUE), 2),\n  )\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\nkable(\n  flag_tracts_group,\n  col.names = c(\n    \"MOE Flag Indicator\",\n    \"Number of Tracts\",\n    \"Avg White Population\", \"Avg Black Population\", \"Avg Hispanic Population\",\n    \"Avg White Pop %\", \"Avg Black Pop %\", \"Avg Hispanic Pop %\"\n  ),\n  caption = \"Tract Demographic Statistics Based on MOE Flag Indicator\",\n  format.args = list(big.mark = \",\")\n)\n\n\nTract Demographic Statistics Based on MOE Flag Indicator\n\n\n\n\n\n\n\n\n\n\n\n\nMOE Flag Indicator\nNumber of Tracts\nAvg White Population\nAvg Black Population\nAvg Hispanic Population\nAvg White Pop %\nAvg Black Pop %\nAvg Hispanic Pop %\n\n\n\n\nHigh MOE\n355\n1,713.62\n1,096.94\n713.41\n42.21\n30.71\n17.72\n\n\nLow MOE\n6\n2,383.67\n0.00\n130.17\n88.89\n0.00\n4.86\n\n\n\n\n\nPattern Analysis:\nOne pattern is that an overwhelming majority of tracts are flagged with high MOE (for at least one demographic) of around 98.34%. There are a few reasons why we observe this. For one, tracts are much smaller than counties and when we have a smaller sample size, this can lead to more variation, hence less reliability (higher margins of error throughout the data). Secondly, since we set our MOE indicator to be classified as ‚ÄúHigh MOE‚Äù if one or more demographic had a higher MOE than 10%. As a result, this design decision leads to more tracts being categorized as ‚ÄúHigh MOE‚Äù than if we made a stricter boundary of all three demographics must have a higher MOE than 10%. Lastly, the tract data was much sparser than the county level data with many demographic estimates and margins being zero for certain tracks, which could possibly be a reporting error.\nFrom the clear imbalance of our high and low MOE classes for the tracts in our selected counties, we should approach drawing conclusions about demographics and reliability with caution. Nevertheless, it seems that our six ‚ÄúLow MOE‚Äù tracts are much less diverse and predominantly White (88.89% compared to 42.21%). It is possible that these communities may have better access to reporting their statistics or are potentially more well informed about the Census and obligation to report. But, it is important to question the oddity that there average Black population is 0% across these tracts. This leads me to believe that this is due to the sparsity issue in the data itself. If the data showed that there are no Black residents (and very little Hispanic residents), then these tracts theoretically would only have to worry about passing the MOE threshold for the White demographic. It is important to consider all possibilities when trying to analyze and uncover patterns within data."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements:\n\nOverall Pattern Identification: What are the systematic patterns across all your analyses?\nEquity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings?\nRoot Cause Analysis: What underlying factors drive both data quality issues and bias risk?\nStrategic Recommendations: What should the Department implement to address these systematic issues?\n\nExecutive Summary:\nOverall Pattern Identification: Three significant systematic patterns arose across our analyses. The first pattern was that there was a drastic decline in data reliability when moving from county to tract-level data. In our county analysis, all counties were classified as ‚Äúhigh confidence‚Äù in MOE % for median income data. However, in our tract data for selected counties, less than 2% of tracts were classified as ‚Äúhigh confidence‚Äù for demographic data. The second pattern came from our tract-level analysis where we saw that more diverse tracts were much less reliable in terms of collective demographic MOE %. The small percentage of ‚Äúhigh confidence‚Äù tracts were predominantly white (88.89% compared to 42.21%). The third trend was revealed in geographic location in county analysis. Specifically, the highest MOE % of income data mainly resided in South Jersey, revealing clusters of uncertainty among these counties.\nEquity Assessment: Diverse communities with higher population percentages of people of color (Black, Hispanic) are most at risk of algorithmic biases. In our tract-level analysis, we saw more confident aggregate results among communities that were predominantly White rather than diverse. In addition, a majority of the missing data in these tracts came from the Black and Hispanic population estimates and margins. These discrepancies could cause further inequity in these diverse communities when using this tract-level data without caution in our algorithmic decision-making.\nRoot Cause Analysis:\n\nCounties with more variable income flow such as Cape May or Atlantic are likely to have less reliable survey responses, possibly skewing our data depending on when they are surveyed. For example, imagine an ice cream salesman at the Beach is asked about his income in July vs.¬†December.\nThe effect of a smaller sample size in our tract data compared to county data is the main cause of less reliability in our MOE estimates. Smaller sample sizes leads to fewer survey responses, which leads to greater variability causing more uncertainty.\nThe effect of sparse or missing data at the tract-level is another potential cause for biased estimates in the reliability of our tract analysis. Since most of the ‚Äúlow MOE‚Äù tracts had missing demographic data for either the Black or Hispanic demographics, this made it easier for these tracts to pass our threshold test.\n\nStrategic Recommendations:\n\nReliability Thresholds: We should establish more realistic reliability thresholds at both the county and tract level. It is evident that our current decision boundary is not insightful with all counties having high reliability and 98% of tracts having low reliability. We should tighten the threshold for county data in New Jersey. For tract data, it would be valuable to split up reliability by race rather than testing if at least one race fails the threshold. It is also important to highlight which counties have missing data and exclude them from this analysis or state their omission.\nImproved Efforts in Surveying Diverse Tracts & Country Clusters: We saw that diverse tracts had less reliability than predominantly White tracts. Policymakers should make a stronger effort at reaching these communities to ensure that we are obtaining accurate data to prevent drawing biased conclusions about these demographic groups without proper representation. Additionally, we saw that there were clusters of counties that showed higher uncertainty than the rest of the state. It would be beneficial to address these clusters by promoting survey participation to aid policymakers.\nHigher Frequency Monitoring: For immediate policy considerations, it would be helpful to have more frequent data collection such as every three months or even every month if possible. This would be very beneficial to counties like Cape May or Atlantic but others as well. It would identify problems in communities much faster and allow for more actionable, targeted intervention in policy making."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#specific-recommendations",
    "href": "assignments/assignment1/assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\nfinal_df &lt;- nj_reli |&gt;\n  # Add a new column with algorithm recommendations using case_when():\n  # - High Confidence: \"Safe for algorithmic decisions\"\n  # - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n  # - Low Confidence: \"Requires manual review or additional data\"\n  mutate(\n    reli_cat = case_when(\n      reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  ) |&gt;\n  # Sort by MOE percent (DESC)\n  arrange(desc(moe_percent)) |&gt;\n  # Select relevant columns to include\n  select(county_name, median_incomeE, moe_percent, reliability, reli_cat) |&gt;\n  # Add $ and % symbols to median_incomeE and moe_percent\n  mutate(\n    median_incomeE = dollar(median_incomeE),\n    moe_percent = paste0(number(moe_percent, accuracy = 0.01), \"%\")\n  )\n\n# Format as a professional table with kable()\nkable(\n  final_df,\n  col.names = c(\"County\", \"Median Income\", \"MOE Percent\", \"Reliability\", \"Recommendation\"),\n  caption = \"Median Income Decision Framework for NJ Counties\",\n  format.args = list(big.mark = \",\")\n)\n\n\nMedian Income Decision Framework for NJ Counties\n\n\n\n\n\n\n\n\n\nCounty\nMedian Income\nMOE Percent\nReliability\nRecommendation\n\n\n\n\nCape May\n$83,870\n4.42%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSalem\n$73,378\n4.15%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCumberland\n$62,310\n3.54%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nAtlantic\n$73,113\n2.62%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nGloucester\n$99,668\n2.61%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWarren\n$92,620\n2.60%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSomerset\n$131,948\n2.49%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSussex\n$111,094\n2.46%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHunterdon\n$133,534\n2.42%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMercer\n$92,697\n2.33%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nUnion\n$95,000\n2.33%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMorris\n$130,808\n2.08%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHudson\n$86,854\n2.05%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nEssex\n$73,785\n2.00%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPassaic\n$84,465\n1.85%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOcean\n$82,379\n1.77%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCamden\n$82,005\n1.72%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMonmouth\n$118,527\n1.61%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMiddlesex\n$105,206\n1.46%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBurlington\n$102,615\n1.40%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBergen\n$118,714\n1.35%\nHigh Confidence\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: All counties in NJ are considered suitable for algorithmic implementation. They all have MOE less than our 5% threshold, making them high confidence for algorithmic decision making. However, if we wanted to examine tracts in individual counties, based on our tract-level analysis of three NJ counties, our data is much less reliable to use. Therefore, we should proceed with more caution for any kind of tract-level algorithmic implementation.\nCounties requiring additional oversight: No counties were classified as moderate confidence in NJ. However, I would say that counties near the bound of the 5% threshold like Cape May and Salem could be more closely monitored given their relatively high MOE above 4%. Additionally, as expressed earlier, counties such as Cape May and Atlantic that are highly reliant on tourism in generating income for its residents could use some supplementary data collection for analysis. Since income flow is extremely time-dependent, we should look to monitor the changes in income related to each season (or month) to get a better understanding of the income data. This would lead to more consistent comparisons across years through potential time-series modelling methods like a seasonal ARIMA model or something similar.\nCounties needing alternative approaches: No counties were classified as low confidence in NJ, so no specific alternatives are needed. However, it may be worthwhile to monitor the percent change in some of the higher MOE counties if they were to exhibit drastic increases in MOE % in future years. As mentioned previously, tract data within each county are much less reliable and could definitely benefit from precautionary measures such as manual data validation or increased data collection through additional surveys to give analysts more confidence in deriving algorithms to address issues at this level."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#questions-for-further-investigation",
    "href": "assignments/assignment1/assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nHow does MOE % for each county trend over time? Do counties with lower MOE % or higher MOE % fluctuate over time? In the specific case of tourism-driven economies like Cape May or Atlantic, does data exist on their economic output across each month / season?\nHow does geographical location factor into MOE % within counties? We saw how there can be small clusters of counties that share higher MOE % relative to the rest of NJ? Is this a common occurrence that we see in most other states as well? What factors can cause these clusters other than income and demographic trends?\nFor tract-level data, why did we see a lot more missing data for people of color than the White demographic? What possible solutions are there to mitigating this issue for future surveys?"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#submission-checklist",
    "href": "assignments/assignment1/assignment1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll ‚Äú[Fill this in]‚Äù prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "assignments/assignment2/assignment2.html",
    "href": "assignments/assignment2/assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives:\n\nApply spatial operations to answer policy-relevant research questions\nIntegrate census demographic data with spatial analysis\nCreate publication-quality visualizations and maps\nWork with spatial data from multiple sources\nCommunicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment2/assignment2.html#assignment-overview",
    "href": "assignments/assignment2/assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives:\n\nApply spatial operations to answer policy-relevant research questions\nIntegrate census demographic data with spatial analysis\nCreate publication-quality visualizations and maps\nWork with spatial data from multiple sources\nCommunicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment2/assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "assignments/assignment2/assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data:\n\nPennsylvania county boundaries\nPennsylvania hospitals (from lecture data)\nPennsylvania census tracts\n\n\n# Load required packages\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(scales)\nlibrary(RColorBrewer)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(tigris)\nlibrary(patchwork)\nlibrary(here)\n\n# Hides progress bars from output\noptions(tigris_use_cache = TRUE, tigris_progress = FALSE) \n\n\n# Load spatial data\npa_county_bounds &lt;- st_read(\"data/Pennsylvania_County_Boundaries.shp\")\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `/Users/jack/Documents/GitHub/portfolio-setup-jbader14/assignments/assignment2/data/Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\nhospitals &lt;- st_read(\"data/hospitals.geojson\")\n\nReading layer `hospitals' from data source \n  `/Users/jack/Documents/GitHub/portfolio-setup-jbader14/assignments/assignment2/data/hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\n# Check that all data loaded correctly - hidden from submission\n#head(pa_county_bounds)\n# head(hospitals)\n\n\n# How many hospitals in your dataset?\ncount_hospitals &lt;- hospitals |&gt;\n  distinct(FACILITY_N) |&gt;\n  nrow()\n\n# How many census tracts?\npa_tract_bounds &lt;- tracts(state = \"PA\", cb = TRUE)\n\ncount_tract &lt;- pa_tract_bounds |&gt;\n  distinct(NAMELSAD) |&gt;\n  nrow()\n\n# What coordinate reference system is each dataset in?\n# HIDDEN from output\n# st_crs(pa_county_bounds)\n# st_crs(hospitals)\n\nAnalysis:\n\nThere are 222 hospitals and 2547 census tracts in PA from our datasets. Both datasets are using the WGS 84 coordinate reference systems.\n\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables:\n\nTotal population\nMedian household income\nPopulation 65 years and over (you may need to sum multiple age categories)\n\n\n# Get demographic data from ACS\n\n# Set census api key\nkey &lt;- Sys.getenv(\"CENSUS_API_KEY\")\ncensus_api_key(key)\n\n# Define age groups 65+\nage_groups_65_plus &lt;- c(\n  male_65_66 = \"B01001_020\",\n  male_67_69 = \"B01001_021\",\n  male_70_74 = \"B01001_022\",\n  male_75_79 = \"B01001_023\",\n  male_80_84 = \"B01001_024\",\n  male_85_plus = \"B01001_025\",\n  fem_65_66 = \"B01001_044\",\n  fem_67_69 = \"B01001_045\",\n  fem_70_74 = \"B01001_046\",\n  fem_75_79 = \"B01001_047\",\n  fem_80_84 = \"B01001_048\",\n  fem_85_plus = \"B01001_049\"\n)\n\n# 65+ categories to sum over\ncols_65_plus &lt;- c(\n  \"male_65_66E\",\"male_67_69E\",\"male_70_74E\",\"male_75_79E\",\"male_80_84E\",\"male_85_plusE\",\n  \"fem_65_66E\",\"fem_67_69E\",\"fem_70_74E\",\"fem_75_79E\",\"fem_80_84E\",\"fem_85_plusE\"\n)\n\n# Build variable set\nvariables &lt;- c(\n  total_pop     = \"B01003_001\",\n  age_groups_65_plus,\n  median_income = \"B19013_001\"\n)\n\npa_tract_demo &lt;- get_acs(\n  geography = \"tract\",\n  variables = variables,\n  state = \"PA\", \n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n  ) |&gt;\n  # Sum columns for age groups &gt; 65\n  mutate(\n    pop_65_plusE = rowSums(across(all_of(cols_65_plus)), na.rm = TRUE)\n  ) |&gt;\n  # Select relevant columns\n  select(GEOID, total_popE, pop_65_plusE, median_incomeE)\n\n# Join to tract boundaries\npa_tract_full &lt;- pa_tract_bounds |&gt;\n  left_join(pa_tract_demo, by = \"GEOID\")\n\n\n# How many tracts have missing income data? \nmissing_income &lt;- pa_tract_demo |&gt;\n  filter(is.na(median_incomeE)) |&gt;\n  nrow()\n\n# What is the median income across all PA census tracts?\nmedian_income &lt;- pa_tract_demo |&gt;\n  summarize(median_income = median(median_incomeE, na.rm = TRUE))\n\nAnalysis:\n\nThe ACS data in this analysis is from 2022 for PA where 63 tracts have missing income data and the median income across all tracts is $70,188.\n\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria:\n\nLow median household income (choose an appropriate threshold)\nSignificant elderly population (choose an appropriate threshold)\n\n\n# Filter for vulnerable tracts based on your criteria\n\n# Create elderly population percentage\npa_tract_full &lt;- pa_tract_full |&gt;\n  mutate(\n    percent_65_plus = round(100 * pop_65_plusE / total_popE, 2)\n  )\n\n# Define thresholds for each criteria\n# Low median household income: &lt;= 25th percentile\n# High elderly population percentage: &gt;= 75th percentile\nincome_thr   &lt;- quantile(pa_tract_full$median_incomeE, probs = 0.10, na.rm = TRUE)\nelderly_thr  &lt;- quantile(pa_tract_full$percent_65_plus, probs = 0.90, na.rm = TRUE)\n\n# Create new df with flagged thresholds\npa_tract_flagged &lt;- pa_tract_full |&gt;\n  mutate(\n    # Construct boolean cols for income and elderly vulnerable tracts\n    # Handle NA entries\n    income_vuln = (!is.na(median_incomeE)) & (median_incomeE &lt;= income_thr),\n    elderly_vuln = (!is.na(percent_65_plus)) & (percent_65_plus &gt;= elderly_thr),\n    # Tract is vulnerable if below income thresh OR above elderly thresh\n    vulnerable = income_vuln | elderly_vuln\n  )\n\n\n# How many tracts meet your vulnerability criteria?\ncount_vuln &lt;- sum(pa_tract_flagged$vulnerable, na.rm = TRUE)\n\n# What percentage of PA census tracts are considered vulnerable by your definition?\npercent_vuln &lt;- round(100 * count_vuln / 2547, 2)\n\nAnalysis:\n\nThe income threshold is set to identify vulnerable tracts for all tracts that fall at or below the 10th percentile of median household income. Instead of setting an arbitrary threshold value like $50,000, this method will flag the lowest 10% of median incomes as vulnerable.\nSimilarly, the elderly population threshold is set to identify vulnerable tracts for all tracts that fall at or above the 90th percentile of elderly populations. The tracts the highest 10% of elderly population percentages are flagged as vulnerable. We chose to use elderly population percentages over raw estimates to normalize for total tract populations.\n659 tracts are classified as vulnerable, meaning they are either vulnerable by a low median income or high elderly population, which accounts for 25.87% of tracts. After testing different thresholds such as 25th income / 75th elderly population percentiles and using ‚Äúand‚Äù, meaning both conditions must be true. I opted to make both thresholds stricter but use ‚Äúor‚Äù, only one condition must be true. Ultimately, this decision led to more insightful results when analyzing underserved tracts with respect to vulnerable ones, which was conducted later in my analysis.\n\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\n\n\n\n# Calculate distance from each tract centroid to nearest hospital\n\n# Get centroids of each tract (POINT)\ncentroids &lt;- st_centroid(pa_tract_flagged) \n\n# Transform to US Albers Equal Area CRS\ncentroids &lt;- st_transform(centroids, 5070)\nhospitals &lt;- st_transform(hospitals, 5070)\n\n# Create distance matrix between centroids & hospitals\ndistance_matrix &lt;- st_distance(centroids, hospitals)\n\n# Find closest hospital to each centroid\nmin_dist &lt;- apply(distance_matrix, 1, min)\n\n# Create new df with feature for distance to nearest hospital\npa_tract_flagged &lt;- pa_tract_flagged |&gt;\n  mutate(\n    # Convert nearest diff to miles\n    dist_to_near_hosp = round(as.numeric(min_dist) * 0.000621371192, 2)\n  )\n\npa_tract_vuln &lt;- pa_tract_flagged |&gt;\n  # Filter for only vulnerable tracts\n  filter(vulnerable)\n\nRequirements: - Use an appropriate projected coordinate system for Pennsylvania - Calculate distances in miles\n\n# What is the average distance to the nearest hospital for vulnerable tracts?\navg_dist_to_hosp &lt;- round(mean(pa_tract_vuln$dist_to_near_hosp, na.rm = TRUE), 2)\n\n# What is the maximum distance?\nmax_dist_to_hosp &lt;- max(pa_tract_vuln$dist_to_near_hosp, na.rm = TRUE)\n\n# How many vulnerable tracts are more than 15 miles from the nearest hospital?\ncount_tracts_far_from_hosp &lt;- sum(pa_tract_vuln$dist_to_near_hosp &gt; 15, na.rm = TRUE)\n\nAnalysis:\n\nWe are using the US Albers Equal Area projection because it preserves areas and distances without much distortion like the Web Mercator projection. It also has been proven to be effective to use with demographic and statistical analysis in the United States, making it a strong choice for this work.\nAnong vulnerable tracts, the average distance to the nearest hospital for vulnerable tracts is 3.6 miles, and the maximum distance is 28.98 miles.\nThere are 19 vulnerable tracts whose nearest hospital is greater than 15 miles away. These tracts are categorized as underserved in step 5.\n\n\n\n\nStep 5: Identify Underserved Areas\nDefine ‚Äúunderserved‚Äù as vulnerable tracts that are more than 15 miles from the nearest hospital.\n\n# Create underserved variable\npa_tract_flagged &lt;- pa_tract_flagged |&gt;\n  mutate(\n    underserved = case_when((dist_to_near_hosp &gt; 15 & vulnerable == TRUE) ~ TRUE)\n  ) \n\n\n# Count number of underserved tracts\ncount_underserved &lt;- round(sum(pa_tract_flagged$underserved == TRUE, na.rm = TRUE), 2)\n\n# What percentage of vulnerable tracts are underserved?\npercent_underserved &lt;- round(100 * count_tracts_far_from_hosp / count_vuln, 2)\n\nAnalysis:\n\nAs discussed previously, there are 19 underserved tracts. 2.88% of vulnerable tracts are classified as underserved. This metric seems somewhat surprisingly low to me. It would make sense that vulnerable tracts with lower median incomes would have worse access to hospitals than this. However, we must remember that some vulnerable tracts are classified only by high elderly population percentages. Intuitively, these tracts should have more hospitals nearby since elderly populations would most likely need access to more hospital resources and services than younger populations. Since these two effects inversely relate to one another, it makes some sense why only a small percentage of vulnerable tracts are classified as underserved; however, the metric still seems low to me. In a future analysis, it could be worthwhile to experiment with a lower threshold for distance to nearest hospital to get more concrete results of underserved tracts in PA.\n\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\n\n# Transform CRS\npa_tract_flagged &lt;- st_transform(pa_tract_flagged, 5070)\npa_county_bounds &lt;- st_transform(pa_county_bounds, 5070)\n\ncounty_level_stats &lt;- pa_tract_flagged |&gt;\n  # Spatial join tracts to counties\n  st_join(pa_county_bounds) |&gt;\n  # Drop geom\n  st_drop_geometry() |&gt;\n  # Group by county\n  group_by(COUNTY_NAM) |&gt;\n  # Aggregate statistics by county\n  summarize(\n    # Number of vulnerable tracts\n    n_vuln_tracts = sum(vulnerable, na.rm = TRUE),\n    # Underserved total pop\n    total_under_pop = sum(total_popE[underserved], na.rm = TRUE),\n    # Percentage of vulnerable tracts that are underserved\n    n_under_tracts = sum(underserved, na.rm = TRUE),\n    pct_under_of_vuln = round(dplyr::if_else(\n      n_vuln_tracts &gt; 0,\n      100 * n_under_tracts / n_vuln_tracts,\n      0), \n      2),\n    # Avg hospital distance for vulnerable tracts\n    avg_dist_vuln_hosp = round(mean(dist_to_near_hosp[vulnerable], na.rm = TRUE), 2),\n    # Total vulnerable population\n    total_vuln_pop = sum(total_popE[vulnerable], na.rm = TRUE)\n  ) |&gt;\n  ungroup()\n\n# county_level_stats - hidden from output\n\n\n# Which 5 counties have the highest percentage of underserved vulnerable tracts?\ntop5_counties_under &lt;- county_level_stats |&gt;\n  arrange(desc(pct_under_of_vuln)) |&gt;\n  slice_head(n = 5) |&gt;\n  select(COUNTY_NAM, n_vuln_tracts, n_under_tracts, pct_under_of_vuln)\n\n# top5_counties_under - hidden from output\n\n# Which counties have the most vulnerable people living far from hospitals?\n# Reminder: Underserved populations = vulnerable populations living far from hospitals\ntop5_under_pops &lt;- county_level_stats |&gt;\n  arrange(desc(total_under_pop)) |&gt;\n  slice_head(n = 10) |&gt;\n  select(COUNTY_NAM, total_under_pop)\n\n# top5_under_pops - hidden from output\n\nRequired county-level statistics:\n\nNumber of vulnerable tracts\nNumber of underserved tracts\nPercentage of vulnerable tracts that are underserved\nAverage distance to nearest hospital for vulnerable tracts\nTotal vulnerable population\n\nAnalysis:\n\nThe five counties with the highest percentage of underserved tracts among vulnerable tracts are Bradford, Cameron, Potter, Sullivan, and Tioga.\nThe counties with the most vulnerable people living far from hospitals are Pike, Bradford, Clearfield, Elk, and Chester.\nThere is a common theme that most underserved tracts with worse access to nearby hospitals come from more rural counties in North Central PA. This intuitively makes sense for a few reasons. Firstly, there are less hospitals in those counties. Secondly, the counties in this region of PA are geographically larger as well as the census tracts within them. The combination of less hospitals and larger census tract areas means there is less access to nearby hospitals (within the 15 mile threshold), leading to higher underserved populations. It is hard to distinguish patterns from tables alone; therefore, we will supplement this analysis with plots in future sections.\n\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\n\n# Create and format priority counties table\npriority_counties &lt;- county_level_stats |&gt;\n  arrange(desc(total_under_pop)) |&gt;\n  slice_head(n = 10) |&gt;\n  transmute(\n    County = COUNTY_NAM,\n    `Underserved Population` = comma(total_under_pop),\n    `Vulnerable tracts` = n_vuln_tracts,\n    `Underserved tracts` = n_under_tracts,\n    `% of Underserved Tracts` = percent(pct_under_of_vuln / 100, accuracy = 0.1),\n    `Avg distance to Hospital for Vulnerable Tracts` = number(avg_dist_vuln_hosp, accuracy = 0.01)\n  )\n  \n  \nkable(priority_counties, \n      caption = \"Top 10 Priority Counties for Healthcare Investment in PA\",\n      format.args = list(big.mark = \",\"))\n\n\nTop 10 Priority Counties for Healthcare Investment in PA\n\n\n\n\n\n\n\n\n\n\nCounty\nUnderserved Population\nVulnerable tracts\nUnderserved tracts\n% of Underserved Tracts\nAvg distance to Hospital for Vulnerable Tracts\n\n\n\n\nPIKE\n7,666\n12\n5\n41.7%\n15.90\n\n\nBRADFORD\n7,166\n2\n2\n100.0%\n18.35\n\n\nCLEARFIELD\n5,891\n4\n2\n50.0%\n12.75\n\n\nELK\n5,891\n3\n2\n66.7%\n12.89\n\n\nCHESTER\n5,626\n13\n1\n7.7%\n5.79\n\n\nCLINTON\n5,578\n4\n3\n75.0%\n15.51\n\n\nPOTTER\n5,578\n3\n3\n100.0%\n17.40\n\n\nLUZERNE\n4,905\n31\n3\n9.7%\n5.15\n\n\nCAMERON\n4,536\n2\n2\n100.0%\n18.93\n\n\nSULLIVAN\n3,860\n3\n3\n100.0%\n20.97\n\n\n\n\n\nRequirements:\n\nUse knitr::kable() or similar for formatting\nInclude descriptive column names\nFormat numbers appropriately (commas for population, percentages, etc.)\nAdd an informative caption\nSort by priority (you decide the metric)"
  },
  {
    "objectID": "assignments/assignment2/assignment2.html#part-2-comprehensive-visualization",
    "href": "assignments/assignment2/assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\n\n# Create county-level access map\n\ncounty_level_stats_sf &lt;- pa_county_bounds |&gt;\n  select(COUNTY_NAM, geometry) |&gt;\n  left_join(county_level_stats, by = \"COUNTY_NAM\") |&gt;\n  st_transform(5070)\n\nggplot() +\n  geom_sf(data = county_level_stats_sf, aes(fill = pct_under_of_vuln), color = \"slategray\", size = 0.75) +\n  geom_sf(data = hospitals, color = \"black\", size = 1.5) +\n  scale_fill_distiller(\n    palette = \"Reds\",\n    direction = 1,\n    na.value = \"gray\",\n    name = \"% of underserved tracts\",\n    labels = scales::label_percent(accuracy = 1, scale = 1)\n  ) +\n  labs(\n    title = \"Healthcare Access Challenges in PA Counties\",\n    subtitle = \"Underserved Tracts: Vulnerable tracts that are &gt;15 miles from the nearest hospital.\",\n    caption = \"Vulnerable Tracts: Tracts below 10th percentile in income or above 90th percentile in elderly population.\"\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10),\n    legend.text = element_text(size = 8),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text()\n  )\n\n\n\n\n\n\n\n\nRequirements:\n\nFill counties by percentage of vulnerable tracts that are underserved\nInclude hospital locations as points\nUse an appropriate color scheme\nInclude clear title, subtitle, and caption\nUse theme_void() or similar clean theme\nAdd a legend with formatted labels\n\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\n\n# Create detailed tract-level map\n\ncrs &lt;- 5070\npa_tract_flagged  &lt;- st_transform(pa_tract_flagged, crs)\npa_county_bounds &lt;- st_transform(pa_county_bounds, crs)\nhospitals &lt;- st_transform(hospitals, crs)\n\nunderserved_tracts &lt;- pa_tract_flagged |&gt;\n  filter(underserved)\n\nggplot() +\n  # Tracts\n  geom_sf(data = pa_tract_flagged, fill = \"gray\", color = \"white\", size = 0.07) +\n  \n  # Underserved tracts\n  geom_sf(data = underserved_tracts, aes(fill = total_popE),size = 0.2) +\n\n  # County boundaries \n  geom_sf(data = pa_county_bounds, fill = NA, color = \"slategrey\", size = 0.7) +\n\n  # Hospitals \n  geom_sf(data = hospitals, color = \"black\", size = 2) +\n\n  # Color scale by underserved population sizes\n  scale_fill_distiller(\n    palette = \"Reds\", \n    direction = 1,\n    na.value = \"transparent\",\n    name = \"Underserved population\",\n    labels = scales::label_comma()\n  ) +\n  # Title, subtitle, caption\n  labs(\n    title = \"Underserved Vulnerable Census Tracts in Pennsylvania\",\n    subtitle = \"Underserved Tracts: Vulnerable tracts that are &gt;15 miles from the nearest hospital.\",\n    caption = \"Vulnerable Tracts: Tracts below 10th percentile in income or above 90th percentile in elderly population.\"\n  ) +\n  # Theme, positioning, sizing\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10),\n    legend.text = element_text(size = 8),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text()\n  )\n\n\n\n\n\n\n\n\nRequirements:\n\nShow underserved vulnerable tracts in a contrasting color\nInclude county boundaries for context\nShow hospital locations\nUse appropriate visual hierarchy (what should stand out?)\nInclude informative title and subtitle\n\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\n\n# Get median distance to hospital among vuln tracts\nmedian_dist  &lt;- median(pa_tract_vuln$dist_to_near_hosp, na.rm = TRUE)\n\n# Create distribution visualization\nggplot(pa_tract_vuln, aes(x = dist_to_near_hosp)) +\n  # Histogram\n  geom_histogram(binwidth = 1, boundary = 0, closed = \"left\", fill = \"skyblue\", color = \"black\", linewidth = 0.3) +\n  # Underserved threshold (15 miles)\n  geom_vline(xintercept = 15, color = \"firebrick\", linewidth = 0.7) +\n  annotate(\"text\",\n           x = 15, \n           y = 100, \n           label = \"Underserved Threshold\", \n           hjust = - 0.1, \n           color = \"firebrick\", \n           size = 3) +\n  # Median distance\n  geom_vline(xintercept = median_dist, color = \"black\", linewidth = 0.7) +\n  annotate(\"text\",\n           x = median_dist, \n           y = 100, \n           label = \"Median Distance\", \n           hjust = - 0.1, \n           color = \"black\", \n           size = 3) +\n  # Title, axes labels, caption\n  labs(\n    title = \"Distance to Nearest Hospital for Vulnerable Census Tracts in PA\",\n    x = \"Distance to Nearest Hospital (miles)\",\n    y = \"Number of Tracts\",\n    caption = \"The data is right-skewed with a majority of vulnerable tracts below our underserved threshold. 50% of tracts are within 2 miles of the nearest hospital.\") +\n  # Theme\n  theme_minimal() +\n  # Resize the caption text to fit\n  theme(plot.caption = element_text(size = 7.5))\n\n\n\n\n\n\n\n\nSuggested chart types:\n\nHistogram or density plot of distances\nBox plot comparing distances across regions\nBar chart of underserved tracts by county\nScatter plot of distance vs.¬†vulnerable population size\n\nRequirements:\n\nClear axes labels with units\nAppropriate title\nProfessional formatting\nBrief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "assignments/assignment2/assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "assignments/assignment2/assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\nNote these are just loose suggestions to spark ideas - follow or make your own as the data permits and as your ideas evolve. This analysis should include bringing in your own dataset, ensuring the projection/CRS of your layers align and are appropriate for the analysis (not lat/long or geodetic coordinate systems). The analysis portion should include some combination of spatial and attribute operations to answer a relatively straightforward question\n\n\nEducation & Youth Services\nOption A: Educational Desert Analysis - Data: Schools, Libraries, Recreation Centers, Census tracts (child population) - Question: ‚ÄúWhich neighborhoods lack adequate educational infrastructure for children?‚Äù - Operations: Buffer schools/libraries (0.5 mile walking distance), identify coverage gaps, overlay with child population density - Policy relevance: School district planning, library placement, after-school program siting\nOption B: School Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: ‚ÄúAre school zones safe for walking/biking, or are they crime hotspots?‚Äù - Operations: Buffer schools (1000ft safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\n\n\n\nEnvironmental Justice\nOption C: Green Space Equity - Data: Parks, Street Trees, Census tracts (race/income demographics) - Question: ‚ÄúDo low-income and minority neighborhoods have equitable access to green space?‚Äù - Operations: Buffer parks (10-minute walk = 0.5 mile), calculate tree canopy or park acreage per capita, compare by demographics - Policy relevance: Climate resilience, environmental justice, urban forestry investment ‚Äî\n\n\nPublic Safety & Justice\nOption D: Crime & Community Resources - Data: Crime Incidents, Recreation Centers, Libraries, Street Lights - Question: ‚ÄúAre high-crime areas underserved by community resources?‚Äù - Operations: Aggregate crime counts to census tracts or neighborhoods, count community resources per area, spatial correlation analysis - Policy relevance: Community investment, violence prevention strategies ‚Äî\n\n\nInfrastructure & Services\nOption E: Polling Place Accessibility - Data: Polling Places, SEPTA stops, Census tracts (elderly population, disability rates) - Question: ‚ÄúAre polling places accessible for elderly and disabled voters?‚Äù - Operations: Buffer polling places and transit stops, identify vulnerable populations, find areas lacking access - Policy relevance: Voting rights, election infrastructure, ADA compliance\n\n\n\nHealth & Wellness\nOption F: Recreation & Population Health - Data: Recreation Centers, Playgrounds, Parks, Census tracts (demographics) - Question: ‚ÄúIs lack of recreation access associated with vulnerable populations?‚Äù - Operations: Calculate recreation facilities per capita by neighborhood, buffer facilities for walking access, overlay with demographic indicators - Policy relevance: Public health investment, recreation programming, obesity prevention\n\n\n\nEmergency Services\nOption G: EMS Response Coverage - Data: Fire Stations, EMS stations, Population density, High-rise buildings - Question: ‚ÄúAre population-dense areas adequately covered by emergency services?‚Äù - Operations: Create service area buffers (5-minute drive = ~2 miles), assess population coverage, identify gaps in high-density areas - Policy relevance: Emergency preparedness, station siting decisions\n\n\n\nArts & Culture\nOption H: Cultural Asset Distribution - Data: Public Art, Museums, Historic sites/markers, Neighborhoods - Question: ‚ÄúDo all neighborhoods have equitable access to cultural amenities?‚Äù - Operations: Count cultural assets per neighborhood, normalize by population, compare distribution across demographic groups - Policy relevance: Cultural equity, tourism, quality of life, neighborhood identity\n\n\n\n\nData Sources\nOpenDataPhilly: https://opendataphilly.org/datasets/ - Most datasets available as GeoJSON, Shapefile, or CSV with coordinates - Always check the Metadata for a data dictionary of the fields.\nAdditional Sources: - Pennsylvania Open Data: https://data.pa.gov/ - Census Bureau (via tidycensus): Demographics, economic indicators, commute patterns - TIGER/Line (via tigris): Geographic boundaries\n\n\nRecommended Starting Points\nIf you‚Äôre feeling confident: Choose an advanced challenge with multiple data layers. If you are a beginner, choose something more manageable that helps you understand the basics\nIf you have a different idea: Propose your own question! Just make sure: - You can access the spatial data - You can perform at least 2 spatial operations\n\n\nYour Analysis\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load crime, ppr sites, and street light datasets\ncrime &lt;- st_read(\"data/crime_incidents.shp\")\n\nReading layer `crime_incidents' from data source \n  `/Users/jack/Documents/GitHub/portfolio-setup-jbader14/assignments/assignment2/data/crime_incidents.shp' \n  using driver `ESRI Shapefile'\nreplacing null geometries with empty geometries\nSimple feature collection with 117984 features and 13 fields (with 4770 geometries empty)\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.27421 ymin: 5.684342e-14 xmax: 5.684342e-14 ymax: 40.13683\nGeodetic CRS:  WGS 84\n\nppr_sites &lt;- st_read(\"data/PPR_Program_Sites.geojson\")\n\nReading layer `PPR_Program_Sites' from data source \n  `/Users/jack/Documents/GitHub/portfolio-setup-jbader14/assignments/assignment2/data/PPR_Program_Sites.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 171 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.2563 ymin: 39.90444 xmax: -74.96944 ymax: 40.12284\nGeodetic CRS:  WGS 84\n\nlights &lt;- st_read(\"data/Street_Poles.geojson\")\n\nReading layer `Street_Poles' from data source \n  `/Users/jack/Documents/GitHub/portfolio-setup-jbader14/assignments/assignment2/data/Street_Poles.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 202942 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.28108 ymin: 39.87517 xmax: -74.95891 ymax: 40.13786\nGeodetic CRS:  WGS 84\n\n# Transform CRS to EPSG: 2272 (South PA)\ncrime &lt;- st_transform(crime, 2272)\nppr_sites &lt;- st_transform(ppr_sites, 2272)\nlights &lt;- st_transform(lights, 2272)\n\n# Get Philadelphia tract data\nphl_tracts &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(total_pop = \"B01003_001\"),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2023,\n  survey = \"acs5\",\n  geometry = TRUE,\n  output = \"wide\",\n  cache_table = TRUE\n)\n\n# Transform CRS to EPSG: 2272\nphl_tracts &lt;- st_transform(phl_tracts, 2272)\n\n\n# Crime Summary Stats\n\ncrime &lt;- crime |&gt;\n  # Create a column for time of day\n  mutate(\n    time_of_day = case_when(\n      hour &gt;= 0  & hour &lt;= 5 ~ \"night\",\n      hour &gt;= 6  & hour &lt;=11 ~ \"morning\",\n      hour &gt;=12  & hour &lt;=17 ~ \"afternoon\",\n      TRUE                   ~ \"evening\"\n    )\n  )\n\n# Summary Stats: Type of Crime\ncrime_type &lt;- crime |&gt;\n  # Drop geometry\n  st_drop_geometry() |&gt;\n  # Group by type\n  group_by(text_gener) |&gt;\n  # Count each type of crime\n  summarize(count = n()) |&gt;\n  # Arrange by desc count\n  arrange(desc(count))\n\n# Summary Stats: Time of Day\ncrime_tod &lt;- crime |&gt;\n  # Drop geometry\n  st_drop_geometry() |&gt;\n  # Group by time of day\n  group_by(time_of_day) |&gt;\n  # Count crimes for each time of day\n  summarize(count = n()) |&gt;\n  # Arrange by desc count\n  arrange(desc(count))\n\n# Create tables for each summary stat\n\ncrime_type_table &lt;- crime_type |&gt;\n  # Create percentage variable\n  mutate(pct = count / sum(count)) |&gt;\n  mutate(\n    # Add commas for count, % symbols for percentage (rounded)\n    count = comma(count),\n    pct   = percent(pct, accuracy = 0.01)\n  ) |&gt;\n  # Select top 10 most common crimes by type\n  slice_head(n = 10)\n\ncrime_tod_table &lt;- crime_tod |&gt;\n  mutate(pct = count / sum(count)) |&gt;\n  mutate(\n    count = comma(count),\n    pct   = percent(pct, accuracy = 0.01)\n  )\n\n# Display summary statistics using kable()\nkable(\n  crime_type_table,\n  col.names = c(\"Crime type\", \"Count\", \"Percent\"),\n  caption = \"Crime incidents by Type\"\n)\n\n\nCrime incidents by Type\n\n\nCrime type\nCount\nPercent\n\n\n\n\nThefts\n28,206\n23.91%\n\n\nOther Assaults\n20,323\n17.23%\n\n\nAll Other Offenses\n12,796\n10.85%\n\n\nMotor Vehicle Theft\n11,542\n9.78%\n\n\nVandalism/Criminal Mischief\n9,841\n8.34%\n\n\nTheft from Vehicle\n7,521\n6.37%\n\n\nFraud\n7,161\n6.07%\n\n\nAggravated Assault No Firearm\n4,170\n3.53%\n\n\nBurglary Residential\n2,429\n2.06%\n\n\nNarcotic / Drug Law Violations\n1,875\n1.59%\n\n\n\n\nkable(\n  crime_tod_table,\n  col.names = c(\"Time of day\", \"Count\", \"Percent\"),\n  caption = \"Crime incidents by Time of Day\"\n)\n\n\nCrime incidents by Time of Day\n\n\nTime of day\nCount\nPercent\n\n\n\n\nafternoon\n42,762\n36.24%\n\n\nmorning\n33,324\n28.24%\n\n\nevening\n29,275\n24.81%\n\n\nnight\n12,623\n10.70%\n\n\n\n\n\nAnalysis:\n\nThe datasets used were crime incidents, Park and Rec Program (PPR) sites, and street lights.These datasets were selected to conduct crime analysis at the tract level in Philadelphia. Our goal was to see if tracts with greater street light densities and access to nearby PPR sites experienced lower crime levels.\nThe data source for all of our datasets came from from openphilly.org. Crime incidents are from 2025, and PPR sites are from 2022. There is no date associated with the street lights dataset.\nDataset Features:\n\nCrime incidents: 14 features\nPPR programs: 11 features\nStreet lights: 17 features\n\nThe CRS for the data was in WGS 84. I changed it to a projected CRS such as StatePlane PA South that is more locally accurate for area and distance calculations with low distortion levels.\nWe calculated count and percentage statistics from the crimes dataset, grouping by type of crime and time of day.\n\n\n\nPose a research question\n\nIn Philadelphia, do census tracts with worse access to recreation centers and fewer street lights experience higher evening / night time crime rates?\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+):\n\nBuffers\nSpatial joins\nSpatial filtering with predicates\nDistance calculations\nIntersections or unions\nPoint-in-polygon aggregation\n\n\n# Step 1a: Filter crime for only evening + night crimes for street lights analysis\ncrime_en &lt;- crime |&gt;\n  filter(time_of_day %in% c(\"evening\", \"night\"))\n\n# Step 1b: Filter crime for only morning + afternoon crimes for PPR sites analysis\ncrime_am &lt;- crime |&gt;\n  filter(time_of_day %in% c(\"morning\", \"afternoon\"))\n\n# Step 2: Spatial Join - crime, lights with census tracts\ncrime_tracts_en &lt;- st_join(crime_en, phl_tracts, join = st_within)\ncrime_tracts_am &lt;- st_join(crime_am, phl_tracts, join = st_within)\nlight_tracts &lt;- st_join(lights, phl_tracts, join = st_within)\n\n# Step 3: Calculate total number of crimes and street lights per tract\ncrime_count_en &lt;- crime_tracts_en |&gt;\n  st_drop_geometry() |&gt;\n  filter(!is.na(GEOID)) |&gt;\n  group_by(GEOID) |&gt;\n  summarise(crime_count_en = n(), .groups = \"drop\")\n\ncrime_count_am &lt;- crime_tracts_am |&gt;\n  st_drop_geometry() |&gt;\n  filter(!is.na(GEOID)) |&gt;\n  group_by(GEOID) |&gt;\n  summarise(crime_count_am = n(), .groups = \"drop\")\n\nlights_count &lt;- light_tracts |&gt;\n  st_drop_geometry() |&gt;\n  filter(!is.na(GEOID)) |&gt;\n  group_by(GEOID) |&gt;\n  summarise(lights_count = n(), .groups = \"drop\")\n\n# Step 4: Calculate distance to nearest ppr site for each tract centroid\n\n# Calculate tract centroids\ntr_centroids &lt;- st_centroid(phl_tracts)\n\n# Create distance matrix between centroids & ppr sites\ndist_to_ppr &lt;- st_distance(tr_centroids, ppr_sites)\n\n# Find closest ppr site to each centroid\nmin_dist_to_ppr &lt;- apply(dist_to_ppr, 1, min)\n\nphl_tracts_dist_ppr &lt;- phl_tracts |&gt;\n  st_drop_geometry() |&gt;\n  # Create new min dist feature\n  mutate(dist_to_ppr = round(as.numeric(min_dist_to_ppr) / 5280, 2))\n\n# Step 5: Join dataframes together to get tract-level statistics\ncrime_ppr_light_stats &lt;- phl_tracts_dist_ppr |&gt;\n  # Drop geometry\n  st_drop_geometry() |&gt;\n  # Join in crime counts and light counts\n  left_join(crime_count_am, by = \"GEOID\") |&gt;\n  left_join(crime_count_en, by = \"GEOID\") |&gt;\n  left_join(lights_count,  by = \"GEOID\")\n\n# Step 6: Add geometry back to stats df for mapping\ncrime_ppr_light_stats_sf &lt;- phl_tracts |&gt;\n  select(GEOID, geometry) |&gt;\n  left_join(crime_ppr_light_stats, by = \"GEOID\")\n\n\n# Step 7: Plot a choropleth across PA census tracts w/ point geom for PPR sites\nggplot(crime_ppr_light_stats_sf) +\n  geom_sf(aes(fill = crime_count_am), color = \"slategray\") +\n  geom_sf(data = ppr_sites, color = \"black\", size = 1.5) +\n  scale_fill_distiller(\n    palette = \"Reds\", direction = 1, na.value = \"gray\",\n    name = \"Morning/Afternoon crimes\"\n  ) +\n  labs(\n    title = \"Morning/Afternoon Crimes in Philadelphia Census Tracts\",\n    subtitle = \"Points represent park and recreation sites.\"\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n# Step 7b: Create similar choropleth but find crime heavy tracts\n# Where crime count is at or above 75th percentile\n\n# Crime-heavy threshold at the 75th percentile\ncrime_thr &lt;- quantile(crime_ppr_light_stats_sf$crime_count_am, probs = 0.75, na.rm = TRUE)\n\n# Create var that labels tracts as crime-heavy vs other\ncrime_flagged &lt;- crime_ppr_light_stats_sf |&gt;\n  mutate(crime_bucket = if_else(!is.na(crime_count_am) & crime_count_am &gt;= crime_thr,\"Crime-heavy\", \"Other tracts\"))\n\n# Plot Choropleth\nggplot() +\n  geom_sf(data = crime_flagged, aes(fill = crime_bucket), color = \"white\", size = 0.1) +\n  geom_sf(data = ppr_sites, color = \"black\", size = 1.5) +\n  scale_fill_manual(\n    values = c(\"Other tracts\" = \"gray\", \"Crime-heavy\" = \"firebrick\"),\n    guide = guide_legend(title = NULL, override.aes = list(color = NA))\n  ) +\n  labs(\n    title = \"Crime-Heavy Census Tracts in Philadelphia\",\n    subtitle = paste0(\"Crime-heavy tracts are at or above the 75th percentile of crime counts.\"),\n    caption = \"This data is for morning to afternoon crimes.\"\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n# Step 8: Output table for crime counts, light counts, and distance to nearest ppr site\ntable_am &lt;- crime_ppr_light_stats |&gt;\n  # Select relevant cols\n  select(GEOID, crime_count_am, dist_to_ppr) |&gt;\n  # Order by descending AM crime count\n  arrange(desc(crime_count_am)) |&gt;\n  # Format for kable()\n  mutate(\n    crime_count_am = comma(crime_count_am),\n    dist_to_ppr    = number(dist_to_ppr, accuracy = 0.01)\n  ) |&gt;\n  slice_head(n = 10)\n\nkable(\n  table_am,\n  col.names = c(\"GEOID\", \"Crimes (Morning/Afternoon)\", \"Nearest PPR (miles)\"),\n  caption = \"Morning/Afternoon Crimes in Philadelphia Tracts with Distance to Nearest PPR Site\"\n)\n\n\nMorning/Afternoon Crimes in Philadelphia Tracts with Distance to Nearest PPR Site\n\n\nGEOID\nCrimes (Morning/Afternoon)\nNearest PPR (miles)\n\n\n\n\n42101017800\n1,058\n0.27\n\n\n42101000500\n995\n0.58\n\n\n42101017701\n729\n0.16\n\n\n42101017702\n703\n0.21\n\n\n42101011100\n637\n0.35\n\n\n42101029800\n561\n0.26\n\n\n42101036902\n538\n0.44\n\n\n42101000701\n523\n0.61\n\n\n42101037900\n520\n0.24\n\n\n42101014700\n505\n0.24\n\n\n\n\n\n\n# Step 8b: Evening/Night crimes with streetlight counts\ntable_pm &lt;- crime_ppr_light_stats |&gt;\n  select(GEOID, crime_count_en, lights_count) |&gt;\n  arrange(desc(crime_count_en)) |&gt;\n  mutate(\n    crime_count_en = comma(crime_count_en),\n    lights_count   = comma(lights_count)\n  ) |&gt;\n  slice_head(n = 10)\n\nkable(\n  table_pm,\n  col.names = c(\"Tract GEOID\", \"Crimes (Evening/Night)\", \"Streetlights\"),\n  caption = \"Evening/Night Crimes in Philadelphia Tracts with Streetlight Counts\"\n)\n\n\nEvening/Night Crimes in Philadelphia Tracts with Streetlight Counts\n\n\nTract GEOID\nCrimes (Evening/Night)\nStreetlights\n\n\n\n\n42101017800\n642\n860\n\n\n42101017701\n434\n443\n\n\n42101037900\n418\n1,107\n\n\n42101000500\n415\n838\n\n\n42101017702\n369\n614\n\n\n42101014700\n325\n326\n\n\n42101034900\n284\n672\n\n\n42101011100\n284\n800\n\n\n42101006600\n277\n546\n\n\n42101015200\n277\n700\n\n\n\n\n\n```\nAnalysis requirements:\n\nClear code comments explaining each step\nAppropriate CRS transformations\nSummary statistics or counts\nAt least one map showing your findings\nBrief interpretation of results (3-5 sentences)\n\nInterpretation: Looking at the choropleths and tables, it does not seem there is a strong relationship between number of crimes and nearby PPR sites / number of streetlights. Even by seperating the data to analyze morning/afternoon crimes with PPR sites and evening/night crimes with PPR sites, there are not significant discoveries or trends to be made from plots and tables alone. From the choropleth of crime-heavy tracts, it does look like these tracts do cluster near eachother, signifying they belong to unsafe neighborhoods. Additionally, from the tables, tracts that experience high crime levels in the morning/afternoon tend to have higher crime levels in the evening/night at a glance. For further research, I think it would be beneficial to do zip code level analysis to get a better sense of patterns in crime throughout Philadelphia in larger geographic regions than at the tract level. I also think it could be valuable to normalize crime counts into rates using population data and street light counts into densities with area data. However, it may be the case that these variables are not strong indicators or predictors of crime level throughout the city. For that, we could do correlation, regression analyses among others to delve deeper into the factors that cause high crime, which can benefit policymakers in their decision-making."
  },
  {
    "objectID": "assignments/assignment2/assignment2.html#incorporation-of-feedback",
    "href": "assignments/assignment2/assignment2.html#incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Incorporation of Feedback:",
    "text": "Incorporation of Feedback:\nI made sure to delete brackets and questions. I hid outputs not required by the assignment‚Äôs instructions. I also hid the census API key in a .env file."
  },
  {
    "objectID": "assignments/assignment2/assignment2.html#submission-requirements",
    "href": "assignments/assignment2/assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it‚Äôs a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\nAssignment 1: Census Data Quality for Policy Decisions\n\n\n\n\n\n\nAssignment 2: Spatial Analysis and Visualization\n\n\n\n\n\n\nAssignment 4: Spatial Predictive Analysis\n\n\n\n\n\n\nAssignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "labs/index.html",
    "href": "labs/index.html",
    "title": "Labs",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\nLab 0: Getting Started with dplyr\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "weekly-notes/index.html",
    "href": "weekly-notes/index.html",
    "title": "Weekly Notes",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nWeek 9 Notes - Logistic Regression: The Case of Recidivism\n\n\nNov 10, 2025\n\n\n\n\n\n\nWeek 8 Notes - Predictive Policying, Dirty Data, and Count Regression\n\n\nNov 3, 2025\n\n\n\n\n\n\nWeek 7 Notes - Model Diagnostics & Spatial Autocorrelation\n\n\nOct 27, 2025\n\n\n\n\n\n\nWeek 6 - Spatial ML & Advanced LR\n\n\nOct 13, 2025\n\n\n\n\n\n\nWeek 5 - Intro to Linear Regression\n\n\nOct 6, 2025\n\n\n\n\n\n\nWeek 4: GIS & Spatial Analysis in R\n\n\nSep 29, 2025\n\n\n\n\n\n\nWeek 3: EDA & ggplot2\n\n\nSep 22, 2025\n\n\n\n\n\n\nWeek 2 - Algorithmic Decision Making & Census Data\n\n\nSep 15, 2025\n\n\n\n\n\n\nWeek 1 - Course Introduction\n\n\nSep 8, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "",
    "text": "Clone Repo Workflow:\n\nNavigate to repository\nClick &lt;&gt; code: green button\nOpen with Github Desktop\nCopy over data, template scripts, etc. over from cloned class repo to personal repo\nOTHER METHOD: Use terminal\n\ncd ~/file location\ngit clone https://github.com/MUSA-5080-Fall-2025/MUSA-5080-Fall-2025.git\n\n\nALGORITHM: Set of rules/instructions for solving a problem or completing a task.\n\nIn Government: Systems used to assist / replace human decision-makers.\n\nBased on predictions from models that process historical data:\nInputs: features/predictors/independent variables/x\nOutputs: labels/outcome/dependent variable/y\nHelps eliminate human bias / subjectivity\nExamples: criminal justice (bail, sentencing), housing and finance (whether or not to rent), healthcare (prioritization, resource allocation)\n\nHealthcare Algorithmic Bias: Algorithm used to identify high-risk patients for additional care systematically discriminated against Black patients.\n\nAlgorithm used healthcare costs as a proxy for need / risk.\nHistorical inequity: Black patients typically incur lower costs due to systematic inequities in access.\nResult: Black patients under-prioritized despite equivalent levels of illness.\nScale: Used by hospitals and insurers for over 200 million people annually.\n\nCOMPAS Recidivism Prediction (Criminal Justice):\n\nAlgorithm twice as likely to flag Black defendants as high risk.\nHistorical arrest data reflects biased policing patterns.\n\nDutch Welfare Fraud Detection:\n\n‚ÄúBlack Box‚Äù secret system\nDisproportionally targeted vulnerable populations\n\n\n\n\nTerms:\n\nData Science: Computer science/engineering focus on algorithms and methods.\nData Analytics: Application of data science methods to other disciplines.\nMachine Learning: Algorithms for classification and prediction that learn from data.\nAI: Algorithms that adjust and improve across iterations (neural nets, etc.).\n\nPublic Sector context:\n\nLong history of govt data collection:\n\nCivic registration systems\nCensus data\nAdministrative records\nOperations research (post-WWII)\n\nWhat‚Äôs new?\n\nMore data (official and ‚Äúaccidental‚Äù such as social media data)\nFocus on prediction rather than explanation\nHarder to interpret and explain\n\nWhy govt use algorithms:\n\nGovts have limited budgets and need to serve everyone.\nAlgorithmic decision making is appealing because it promises:\nEfficiency: faster\nConsistency: established methods\nObjectivity: less human bias\nCost savings: less staff\n\n\nData Analytics is subjective:\n\nEvery step involves human choices (embedded values and biases):\n\nData cleaning\nData coding / classification\nData collection - use of imperfect proxies\nHow you interpret results\nWhat variables you use in the model\n\n\nScenario Example: Housing assistance allocation\n\nProxy: median household income\nBlind spots:\n\nLower income communities: median is a central measure.\nHigh rent/demand communities (NYC): median is a central measure.\n\nHarm + Fixes:\n\nUpper bound for hh income (well under city median income) to provide more housing assistance.\nNeighborhood housing price scale (divide income by housing price) to see how well off a hh is compared to the relative neighborhood.\nGuardrail:\n\nRent control\nHousing stipends / upper bounded interest rates\n\n\n\nCENSUS:\n\nFoundation for:\n\nUnderstanding community demographics\nAllocating government resources\nTracking neighborhood change\nDesigning fair algorithms (like those we just discussed)\n\nDecennial Census (2020)\n\nEveryone counted every 10 years (full population)\n9 basic questions: age, race, sex, housing\nConstitutional requirement\nDetermines political representation\n\n\nAmerican Community Survey (ACS):\n\nWhat it is:\n\n3% of households surveyed annually\nDetailed questions: income, education, employment, housing costs\nReplaced old ‚Äúlong form‚Äù in 2005\n\n1-Year estimates (areas &gt; 65K people)\n\nMost current data, smallest sample\nTake with grain of salt (only aggregate levels, not neighborhood)\n\n5-Year estimates (all areas including census tracts)\n\nMost reliable data, largest sample\nKey point: All ACS data comes with margins of error (since samples) - uncertainty\n\n\nCensus Geography Hierarchy:\n\nCounty level: state and regional planning\nCensus tract level: neighborhood analysis (1500 - 8000 people)\nBlock group level: very local analysis (huge MOEs) (600 - 3000)\n\nBlocks: decennial only, 85 people\n\n\n2020 Census Innovation: Differential Privacy\n\nChallenge: Modern computing can re-identify individuals from census data.\nSolution: Add mathematical noise to protect privacy while preserving overall patterns.\nControversy: Some places now show impossible results in populations (ex. living underwater).\nWhy this matters: Even objective data involves subjective choices about privacy vs.¬†accuracy (can cause errors).\n\nMargin of Error (MOE)\n\nLarge MOE relative to estimate = less reliable\nIn analysis:\n\nAlways report MOE alongside estimates.\nBe cautious comparing estimates with overlapping error margins.\nConsider using 5-year estimates for greater reliability\n\nDate intervals can be affected by new advancements during that time.\n\n\n\nTwo Types of Census Data:\n\nSummary Tables (what we‚Äôll use mostly)\n\nPre-calculated statistics by geography\nMedian income, percent college-educated, etc.\nGood for: Mapping, comparing places\n\nPUMS: Individual Records\n\nAnonymous individual/household responses\nGood for: Custom analysis, regression models\nMore complex but more flexible\n\n\nData Sources you‚Äôll use:\n\nTIGER/Line Files\n\nGeographic boundaries (shapefiles)\nCensus tracts, counties, states\nNow released as shapefiles (easier to use!)\n\nHistorical Data Sources:\n\nNHGIS (nhgis.org): Historical census data\nNeighborhood Change Database\nLongitudinal Tract Database: Track changes over time‚Äô"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "",
    "text": "Clone Repo Workflow:\n\nNavigate to repository\nClick &lt;&gt; code: green button\nOpen with Github Desktop\nCopy over data, template scripts, etc. over from cloned class repo to personal repo\nOTHER METHOD: Use terminal\n\ncd ~/file location\ngit clone https://github.com/MUSA-5080-Fall-2025/MUSA-5080-Fall-2025.git\n\n\nALGORITHM: Set of rules/instructions for solving a problem or completing a task.\n\nIn Government: Systems used to assist / replace human decision-makers.\n\nBased on predictions from models that process historical data:\nInputs: features/predictors/independent variables/x\nOutputs: labels/outcome/dependent variable/y\nHelps eliminate human bias / subjectivity\nExamples: criminal justice (bail, sentencing), housing and finance (whether or not to rent), healthcare (prioritization, resource allocation)\n\nHealthcare Algorithmic Bias: Algorithm used to identify high-risk patients for additional care systematically discriminated against Black patients.\n\nAlgorithm used healthcare costs as a proxy for need / risk.\nHistorical inequity: Black patients typically incur lower costs due to systematic inequities in access.\nResult: Black patients under-prioritized despite equivalent levels of illness.\nScale: Used by hospitals and insurers for over 200 million people annually.\n\nCOMPAS Recidivism Prediction (Criminal Justice):\n\nAlgorithm twice as likely to flag Black defendants as high risk.\nHistorical arrest data reflects biased policing patterns.\n\nDutch Welfare Fraud Detection:\n\n‚ÄúBlack Box‚Äù secret system\nDisproportionally targeted vulnerable populations\n\n\n\n\nTerms:\n\nData Science: Computer science/engineering focus on algorithms and methods.\nData Analytics: Application of data science methods to other disciplines.\nMachine Learning: Algorithms for classification and prediction that learn from data.\nAI: Algorithms that adjust and improve across iterations (neural nets, etc.).\n\nPublic Sector context:\n\nLong history of govt data collection:\n\nCivic registration systems\nCensus data\nAdministrative records\nOperations research (post-WWII)\n\nWhat‚Äôs new?\n\nMore data (official and ‚Äúaccidental‚Äù such as social media data)\nFocus on prediction rather than explanation\nHarder to interpret and explain\n\nWhy govt use algorithms:\n\nGovts have limited budgets and need to serve everyone.\nAlgorithmic decision making is appealing because it promises:\nEfficiency: faster\nConsistency: established methods\nObjectivity: less human bias\nCost savings: less staff\n\n\nData Analytics is subjective:\n\nEvery step involves human choices (embedded values and biases):\n\nData cleaning\nData coding / classification\nData collection - use of imperfect proxies\nHow you interpret results\nWhat variables you use in the model\n\n\nScenario Example: Housing assistance allocation\n\nProxy: median household income\nBlind spots:\n\nLower income communities: median is a central measure.\nHigh rent/demand communities (NYC): median is a central measure.\n\nHarm + Fixes:\n\nUpper bound for hh income (well under city median income) to provide more housing assistance.\nNeighborhood housing price scale (divide income by housing price) to see how well off a hh is compared to the relative neighborhood.\nGuardrail:\n\nRent control\nHousing stipends / upper bounded interest rates\n\n\n\nCENSUS:\n\nFoundation for:\n\nUnderstanding community demographics\nAllocating government resources\nTracking neighborhood change\nDesigning fair algorithms (like those we just discussed)\n\nDecennial Census (2020)\n\nEveryone counted every 10 years (full population)\n9 basic questions: age, race, sex, housing\nConstitutional requirement\nDetermines political representation\n\n\nAmerican Community Survey (ACS):\n\nWhat it is:\n\n3% of households surveyed annually\nDetailed questions: income, education, employment, housing costs\nReplaced old ‚Äúlong form‚Äù in 2005\n\n1-Year estimates (areas &gt; 65K people)\n\nMost current data, smallest sample\nTake with grain of salt (only aggregate levels, not neighborhood)\n\n5-Year estimates (all areas including census tracts)\n\nMost reliable data, largest sample\nKey point: All ACS data comes with margins of error (since samples) - uncertainty\n\n\nCensus Geography Hierarchy:\n\nCounty level: state and regional planning\nCensus tract level: neighborhood analysis (1500 - 8000 people)\nBlock group level: very local analysis (huge MOEs) (600 - 3000)\n\nBlocks: decennial only, 85 people\n\n\n2020 Census Innovation: Differential Privacy\n\nChallenge: Modern computing can re-identify individuals from census data.\nSolution: Add mathematical noise to protect privacy while preserving overall patterns.\nControversy: Some places now show impossible results in populations (ex. living underwater).\nWhy this matters: Even objective data involves subjective choices about privacy vs.¬†accuracy (can cause errors).\n\nMargin of Error (MOE)\n\nLarge MOE relative to estimate = less reliable\nIn analysis:\n\nAlways report MOE alongside estimates.\nBe cautious comparing estimates with overlapping error margins.\nConsider using 5-year estimates for greater reliability\n\nDate intervals can be affected by new advancements during that time.\n\n\n\nTwo Types of Census Data:\n\nSummary Tables (what we‚Äôll use mostly)\n\nPre-calculated statistics by geography\nMedian income, percent college-educated, etc.\nGood for: Mapping, comparing places\n\nPUMS: Individual Records\n\nAnonymous individual/household responses\nGood for: Custom analysis, regression models\nMore complex but more flexible\n\n\nData Sources you‚Äôll use:\n\nTIGER/Line Files\n\nGeographic boundaries (shapefiles)\nCensus tracts, counties, states\nNow released as shapefiles (easier to use!)\n\nHistorical Data Sources:\n\nNHGIS (nhgis.org): Historical census data\nNeighborhood Change Database\nLongitudinal Tract Database: Track changes over time‚Äô"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nnew dplyr functions:\n\nglimpse(): brief overview of df (num rows/cols, col name/type, some row examples)\ncolnames(): column names of df\nbetween(x, lower bound, upper bound): use w/ case_when + mutate\nfilter(): Joint string conditions\n\nfilter(Manufacturer %in% c(‚ÄúHonda‚Äù, ‚ÄúNissan‚Äù))\n\nPiping: |&gt; chain together dplyr commands on a single dataframe\n\nAccessing Census Data in R:\n\nModern approach: use R packages to access data directly.\n\nAlways get latest data\nReproducible workflows\nAutomatic geographic boundaries\nBuilt-in error handling\n\nUse tidycensus package\nTable organization:\n\nB19013: Median Household Income\nB25003: Housing Tenure (Own/Rent)\nB15003: Educational Attainment\nB08301: Commuting to Work\n\nVariable Examples: E (Estimate), M (Margin of Error)\n\nB19013_001E = Median household income (estimate)\nB19013_001M = Median household income (margin of error)\n\n\nSAMPLE CODE:\n\ncensus_api_key(): access data\nget_acs(geography, variables, year, state, survey, output): get the data into file\nstr_remove(var, str_to_remove): remove substrings of original string in column"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nThe examples provided context to how we use data science skills and techniques to apply in a real-world setting.\nThe conversations around algorithmic biases and biased data is important to consider when making policy considerations. It is extremely important to understand our data and recognize flaws in algorithms that can perpetuate bias, which usually harm marginalized groups of people.\nThe Census data provides valuable information (ACS) for us to analyze with data-driven methods in formulating policies that solve problems in bettering communities."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "Reflection",
    "text": "Reflection\n\nI had not had a formal introduction to Census data before. Sometimes, working with assigned datasets in class overlook a key aspect being data collection, which is a key decision in framing an analytics problem with human choice.\nI think that it is very valuable to go over biases in algorithms and data. One saying that I was taught is ‚ÄúData is never neutral‚Äù, highlighting that there are many subjective human decisions that go into creating data-driven solutions. It is crucial that we recognize these risks in our problem-solving process to mitigate risk of biases in our policy decisions to protect at-risk groups of people."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4: GIS & Spatial Analysis in R",
    "section": "",
    "text": "Add the spatial dimension to understand:\n\nGeographic clustering of problems\nSpatial relationships between communities\nAccess to services and resources\n\nGeographic bias in algorithms:\n\nTraining data may under-represent certain areas\nSpatial autocorrelation violates independence assumptions\nService delivery algorithms may reinforce geographic inequities\nExamples:\n\nRideshare algorithms avoiding certain neighborhoods\nCrime prediction concentrating enforcement in specific areas\nSocial service algorithms missing rural communities\n\n\nVector Data Model (DISCRETE)\n\nTypes of geometric representation:\n\nPoints: Locations (schools, hospitals)\nLines: Linear features (roads, rivers, train routes)\nPolygons: Areas (census tracts, neighborhoods, service areas)\n\nEach feature has:\n\nGeometry: shape and location\nAttributes: data about that feature (population, income, etc.)\n\n\nCommon coordinate ref systems:\n\nWGS84 (EPSG:4326)\n\nGPS standard, global coverage\nGeographic system (lat/lon)\nGood for web mapping, data sharing\n\nWeb Mercator (EPSG:3857)\n\nWeb mapping standard\nProjected system\nHeavily distorts areas near poles\n\nState Plane / UTM zones\n\nLocal accuracy\nDifferent zones for different regions\nOptimized for specific geographic areas\n\nAlbers Equal Area\n\nPreserves area\nGood for demographic/statistical analysis\n\n\nPolicy Analysis Workflow:\n\nLoad data: Get spatial boundaries and attribute data\nCheck projections: Transform to appropriate CRS\nJoin datasets: Combine spatial and non-spatial data\nSpatial operations: Buffers, intersections, distance calculations\nAggregation: Summarize across spatial units\nVisualization: Maps and charts\nInterpretation: Policy recommendations"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "title": "Week 4: GIS & Spatial Analysis in R",
    "section": "",
    "text": "Add the spatial dimension to understand:\n\nGeographic clustering of problems\nSpatial relationships between communities\nAccess to services and resources\n\nGeographic bias in algorithms:\n\nTraining data may under-represent certain areas\nSpatial autocorrelation violates independence assumptions\nService delivery algorithms may reinforce geographic inequities\nExamples:\n\nRideshare algorithms avoiding certain neighborhoods\nCrime prediction concentrating enforcement in specific areas\nSocial service algorithms missing rural communities\n\n\nVector Data Model (DISCRETE)\n\nTypes of geometric representation:\n\nPoints: Locations (schools, hospitals)\nLines: Linear features (roads, rivers, train routes)\nPolygons: Areas (census tracts, neighborhoods, service areas)\n\nEach feature has:\n\nGeometry: shape and location\nAttributes: data about that feature (population, income, etc.)\n\n\nCommon coordinate ref systems:\n\nWGS84 (EPSG:4326)\n\nGPS standard, global coverage\nGeographic system (lat/lon)\nGood for web mapping, data sharing\n\nWeb Mercator (EPSG:3857)\n\nWeb mapping standard\nProjected system\nHeavily distorts areas near poles\n\nState Plane / UTM zones\n\nLocal accuracy\nDifferent zones for different regions\nOptimized for specific geographic areas\n\nAlbers Equal Area\n\nPreserves area\nGood for demographic/statistical analysis\n\n\nPolicy Analysis Workflow:\n\nLoad data: Get spatial boundaries and attribute data\nCheck projections: Transform to appropriate CRS\nJoin datasets: Combine spatial and non-spatial data\nSpatial operations: Buffers, intersections, distance calculations\nAggregation: Summarize across spatial units\nVisualization: Maps and charts\nInterpretation: Policy recommendations"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coding-techniques",
    "href": "weekly-notes/week-04-notes.html#coding-techniques",
    "title": "Week 4: GIS & Spatial Analysis in R",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nsf package in R (Shape File):\n\nWhy sf: Spatial data is just data.frame + geometry column\n\nModern replacement for older spatial packages\nIntegrates with tidyverse workflows\nFollows international standards\nFast and reliable\nRead in shape file and turn into VECTOR data model\n\nExtensions: .shp, .shx, .dbf\n\nCommon spatial data formats:\n\nShapefiles (.shp + supporting files)\nGeoJSON (.geojson)\nKML/KMZ (Google Earth)\nDatabase connections (PostGIS)\n\nSpatial subsetting: Extract key features based on spatial relationships\n\nst_filter(), st_intersects(), st_touches(), st_within()\nst_filter(): Spatial selection based on spatial relationship.\nst_touches(): Share boundary, no interior overlap\n\n‚ÄúWhich counties border Allegheny?‚Äù\n\nst_within(): Completely inside\n\n‚ÄúWhich tracts are IN Allegheny?‚Äù\n\nst_intersects(): Any overlap at all\n\n‚ÄúWhich tracts overlap a metro area?‚Äù\n\nDefault: If no .predicate specified, uses st_intersects\n.predicate tells st_filter() what spatial relationship to look for:\n\nr predicate-structure # Basic structure: st_filter(data_to_filter, reference_geometry, .predicate = relationship)\n\nst_contains(): Completely contains\n\n‚ÄúDistricts containing hospitals\n\nst_overlaps(): Partial overlap\n\n‚ÄúOverlapping service areas‚Äù\n\nst_disjoint(): No spatial relationship\n\n‚ÄúCounties separate from urban areas‚Äù\n\nMost common: st_intersects() (any overlap) and st_touches() (neighbors)\n\nSpatial join: Combine datasets based on spatial relationships.\nDistance calculations:\n\nst_centroid(): converts polygons to points\nst_distance():\n\nArea calculations: Examples in code - Units depend on coordinate reference system.\nDot (.): The dot (.) is a placeholder that represents the data being passed through the pipe (%&gt;%).\n\npa_counties &lt;- pa_counties %&gt;% mutate( area_sqkm = as.numeric(st_area(.)) / 1000000 )\nThe . refers to pa_counties - the data frame being passed through the pipe. So this is equivalent to:\npa_counties &lt;- pa_counties %&gt;% mutate( area_sqkm = as.numeric(st_area(pa_counties)) / 1000000 )\n\nBuffer operations: Create zones around features\n\n`{r buffers} # 10km buffer around all hospitals hospital_buffers &lt;- hospitals %&gt;% st_buffer(dist = 10000) # 10,000 meters\n\nCoordinate Reference Systems:\n\nProblems:\n\nCan‚Äôt preserve area, distance, and angles simultaneously\nDifferent projections optimize different properties\nWrong projection ‚Üí wrong analysis results!\n\nIn-Class:\n\nStep 1: Approximate Earth‚Äôs shape w/ ellipsoid\nStep 2: Tie ellipsoid to real Earth (Datum)\nStep 3: Put down lat/long grid\n\nGeographic (geodetic) coord systems:\n\nNorth American Datum 1927 (NAD27)\nNAD 83, GRS 80, WGS 84\nAll lat/long on 3D ellipse.\nUnits: decimal degrees\nGood for: Global datasets, web mapping\nBad for: Area/distance calculations\n\nProjected Coordinate Systems (PCS):\n\nOn computer screen\nX/Y coordinates on a flat plane\nUnits: meters, feet, etc.\nGood for: Local analysis, accurate measurements\nBad for: Large areas, global datasets\n\nTransform when:\n\nCalculating areas or distances\nCreating buffers\nDoing geometric operations\nWorking with local/regional data\n\nCODE:\n\nTo simply check current CRS st_crs(pa_counties)\nTo set CRS (ONLY if missing) pa_counties &lt;- st_set_crs(pa_counties, 4326)\nTransform to different CRS\nPennsylvania South State Plane (good for PA analysis)\npa_counties_projected &lt;- pa_counties %&gt;% st_transform(crs = 3365)\nTransform to Albers Equal Area (good for area calculations) pa_counties_albers &lt;- pa_counties %&gt;% st_transform(crs = 5070) `"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#questions-challenges",
    "href": "weekly-notes/week-04-notes.html#questions-challenges",
    "title": "Week 4: GIS & Spatial Analysis in R",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#connections-to-policy",
    "href": "weekly-notes/week-04-notes.html#connections-to-policy",
    "title": "Week 4: GIS & Spatial Analysis in R",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nLearning techniques such as the sf package is extremely valuable in spatial visualizations in R. These help analysts draw conclusions to make policy considerations and also help display trends / patterns to non-data colleagues."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#reflection",
    "href": "weekly-notes/week-04-notes.html#reflection",
    "title": "Week 4: GIS & Spatial Analysis in R",
    "section": "Reflection",
    "text": "Reflection\n\nI have worked with the sf package before but not in this much depth. It was very useful going over the useful commands, parameters, relationships, etc. to deepen my understanding of this topic."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html",
    "href": "weekly-notes/week-06-notes.html",
    "title": "Week 6 - Spatial ML & Advanced LR",
    "section": "",
    "text": "Build a baseline model, interpret coefs / performance / limitations\nBuilding dummy variables: binary indicators\n\ntype of categorical variable\nNeightborhoods:\n\nRittenhouse is reference (omit)\nPos coef: More expensive than Rittenhouse, Neg: Less expensive (all else equal)\n\n\nInteraction Effects: When effect of one variable depends on the level of another variable.\n\nex) Housing: Sq Foot matter more in wealthy neighborhood\n\nPolynomial Terms: nonlinear relationships\n\nU Shaped Age Effect: Really new / old houses high val - Middle aged houses are low in val.\n\nFirst Law of Geography: ‚ÄúEverything is related to everything else, but near things are more related than distant things‚Äù\nFixed Effects: Categorical variables that capture all unmeasured characteristics of a group\n\nIn Hedonic Models:\n\nEach neighborhood gets its own dummy variable\nCaptures everything unique about that neighborhood we didn‚Äôt explicitly measure\n\nBenefits:\n\nBetter predictions\nChange coefs\nBut Black Box (Don‚Äôt know the why)\n\n\nLOOCV - Leave one observation out at a time (special case of k-fold)"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-06-notes.html#key-concepts-learned",
    "title": "Week 6 - Spatial ML & Advanced LR",
    "section": "",
    "text": "Build a baseline model, interpret coefs / performance / limitations\nBuilding dummy variables: binary indicators\n\ntype of categorical variable\nNeightborhoods:\n\nRittenhouse is reference (omit)\nPos coef: More expensive than Rittenhouse, Neg: Less expensive (all else equal)\n\n\nInteraction Effects: When effect of one variable depends on the level of another variable.\n\nex) Housing: Sq Foot matter more in wealthy neighborhood\n\nPolynomial Terms: nonlinear relationships\n\nU Shaped Age Effect: Really new / old houses high val - Middle aged houses are low in val.\n\nFirst Law of Geography: ‚ÄúEverything is related to everything else, but near things are more related than distant things‚Äù\nFixed Effects: Categorical variables that capture all unmeasured characteristics of a group\n\nIn Hedonic Models:\n\nEach neighborhood gets its own dummy variable\nCaptures everything unique about that neighborhood we didn‚Äôt explicitly measure\n\nBenefits:\n\nBetter predictions\nChange coefs\nBut Black Box (Don‚Äôt know the why)\n\n\nLOOCV - Leave one observation out at a time (special case of k-fold)"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#coding-techniques",
    "href": "weekly-notes/week-06-notes.html#coding-techniques",
    "title": "Week 6 - Spatial ML & Advanced LR",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nst_as_sf(coords =, crs = ): Make data spatial\nDummy Variable:\n\nEnsure name is a factor\nboston.sf &lt;- boston.sf %&gt;%\nmutate(name = as.factor(name))\nCheck which is reference (first alphabetically)\nlevels(boston.sf$name)[1]\nFit model with neighborhood fixed effects\nmodel_neighborhoods &lt;- lm(SalePrice ~ LivingArea + name, data = boston.sf)\n\nInteraction Effect:\n\nmodel_interact &lt;- lm(SalePrice ~ LivingArea * wealthy_neighborhood, data = boston.sf)\n\nApproaches to Spatial Aggregation (See Lecture Slides):\n\nBuffer Aggregation: Count or sum events within a defined distance\nk-Nearest Neighbors (kNN): Average distance to k closest events\nDistance to Specific Points: Straight-line distance to important locations"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#questions-challenges",
    "href": "weekly-notes/week-06-notes.html#questions-challenges",
    "title": "Week 6 - Spatial ML & Advanced LR",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#connections-to-policy",
    "href": "weekly-notes/week-06-notes.html#connections-to-policy",
    "title": "Week 6 - Spatial ML & Advanced LR",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nI think these more advanced techniques for modeling regressions is very important to build better models to use for decision making in policy considerations"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#reflection",
    "href": "weekly-notes/week-06-notes.html#reflection",
    "title": "Week 6 - Spatial ML & Advanced LR",
    "section": "Reflection",
    "text": "Reflection\n\nI am excited to employ some of these advanced strategies on the midterm with my table! I think doing this lecture in the context of housing prices makes it easier to wrap my head around the concepts for the midterm."
  },
  {
    "objectID": "weekly-notes/week-08-notes.html",
    "href": "weekly-notes/week-08-notes.html",
    "title": "Week 8 Notes - Predictive Policying, Dirty Data, and Count Regression",
    "section": "",
    "text": "Broken Window‚Äôs Theory: Signs of disorder (abandoned cars) predict property crime (burglaries)\nGlobal v. Local Spatial Autocorrelation: Moran‚Äôs I (diff formulas for each)\n\nUse Moran‚Äôs I scatterplot to visualize hotspots/coldspots and outliers\n\nStatistical Significance Testing:\n\nCalculate observed I_i for location ùëñ\nRandomly shuffle values across locations (999 times)\nRecalculate I_i for each permutation\nCompare observed vs.¬†distribution of permuted values\nIf observed is extreme ‚Üí statistically significant (p &lt; 0.05)\n\nWhy linear regression does not work for ‚Äúcount‚Äù target data:\n\nCan predict negative values (impossible for counts)\nAssumes constant variance (counts often have variance ‚â† mean)\nAssumes continuous outcome (counts are discrete)\nAssumes normal errors (count data is skewed)\n\nModel 1: Poisson\n\nRaise e to the power of sum of regressors\nLog-scaled\n\nOver-Dispersion Problem: Variance &gt;&gt;&gt; Mean\n\nUnobserved heterogeneity: Some areas have unmeasured crime attractors\nContagion effects: One crime leads to others (not independent)\nMeasurement error: Counting issues, data quality\nModel misspecification: Missing important variables\nDispersion = Residual Deviance / Degrees of Freedom\n\nModel 2: Negative Bibomial (for over-dispersion)\n\nRelaxes variance = mean assumption\n\nFishnet Grid:\n\nStandard approach in predictive policing\nEasier spatial operations\nConsistent unit of analysis\n\nLOGO-CV: Leave-one-out CV for spatial data\nKernel Density Estimation (KDE): Simpleset spatial prediction method\n\nPlace smooth ‚Äúbump‚Äù over each crime location\nSum all bumps to create risk surface\nNo predictors, no model, just past locations"
  },
  {
    "objectID": "weekly-notes/week-08-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-08-notes.html#key-concepts-learned",
    "title": "Week 8 Notes - Predictive Policying, Dirty Data, and Count Regression",
    "section": "",
    "text": "Broken Window‚Äôs Theory: Signs of disorder (abandoned cars) predict property crime (burglaries)\nGlobal v. Local Spatial Autocorrelation: Moran‚Äôs I (diff formulas for each)\n\nUse Moran‚Äôs I scatterplot to visualize hotspots/coldspots and outliers\n\nStatistical Significance Testing:\n\nCalculate observed I_i for location ùëñ\nRandomly shuffle values across locations (999 times)\nRecalculate I_i for each permutation\nCompare observed vs.¬†distribution of permuted values\nIf observed is extreme ‚Üí statistically significant (p &lt; 0.05)\n\nWhy linear regression does not work for ‚Äúcount‚Äù target data:\n\nCan predict negative values (impossible for counts)\nAssumes constant variance (counts often have variance ‚â† mean)\nAssumes continuous outcome (counts are discrete)\nAssumes normal errors (count data is skewed)\n\nModel 1: Poisson\n\nRaise e to the power of sum of regressors\nLog-scaled\n\nOver-Dispersion Problem: Variance &gt;&gt;&gt; Mean\n\nUnobserved heterogeneity: Some areas have unmeasured crime attractors\nContagion effects: One crime leads to others (not independent)\nMeasurement error: Counting issues, data quality\nModel misspecification: Missing important variables\nDispersion = Residual Deviance / Degrees of Freedom\n\nModel 2: Negative Bibomial (for over-dispersion)\n\nRelaxes variance = mean assumption\n\nFishnet Grid:\n\nStandard approach in predictive policing\nEasier spatial operations\nConsistent unit of analysis\n\nLOGO-CV: Leave-one-out CV for spatial data\nKernel Density Estimation (KDE): Simpleset spatial prediction method\n\nPlace smooth ‚Äúbump‚Äù over each crime location\nSum all bumps to create risk surface\nNo predictors, no model, just past locations"
  },
  {
    "objectID": "weekly-notes/week-08-notes.html#coding-techniques",
    "href": "weekly-notes/week-08-notes.html#coding-techniques",
    "title": "Week 8 Notes - Predictive Policying, Dirty Data, and Count Regression",
    "section": "Coding Techniques",
    "text": "Coding Techniques"
  },
  {
    "objectID": "weekly-notes/week-08-notes.html#questions-challenges",
    "href": "weekly-notes/week-08-notes.html#questions-challenges",
    "title": "Week 8 Notes - Predictive Policying, Dirty Data, and Count Regression",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-08-notes.html#connections-to-policy",
    "href": "weekly-notes/week-08-notes.html#connections-to-policy",
    "title": "Week 8 Notes - Predictive Policying, Dirty Data, and Count Regression",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nUnderstanding dirty data is very important for preventing algorithmic biases in our modeling process."
  },
  {
    "objectID": "weekly-notes/week-08-notes.html#reflection",
    "href": "weekly-notes/week-08-notes.html#reflection",
    "title": "Week 8 Notes - Predictive Policying, Dirty Data, and Count Regression",
    "section": "Reflection",
    "text": "Reflection\n\nRe-learning the Poisson and Negative Binomial regression methods is important especially for cases like Crime count data."
  },
  {
    "objectID": "assignments/assignment5/assignment5.html",
    "href": "assignments/assignment5/assignment5.html",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "",
    "text": "Assignment 5 Formal Report (PDF)"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#the-rebalancing-challenge-in-philadelphia",
    "href": "assignments/assignment5/assignment5.html#the-rebalancing-challenge-in-philadelphia",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "",
    "text": "Philadelphia‚Äôs Indego bike share system faces the same operational challenge as every bike share system: rebalancing bikes to meet anticipated demand.\nImagine you‚Äôre an Indego operations manager at 6:00 AM on a Monday morning. You have: - 200 stations across Philadelphia - Limited trucks and staff for moving bikes - 2-3 hours before morning rush hour demand peaks - The question: Which stations will run out of bikes by 8:30 AM?"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#learning-objectives",
    "href": "assignments/assignment5/assignment5.html#learning-objectives",
    "title": "Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "",
    "text": "By the end of this assignment, you will be able to:\n\nUnderstand panel data structure for space-time analysis\nCreate temporal lag variables to capture demand persistence\nBuild multiple predictive models with increasing complexity\nValidate models temporally (train on past, test on future)\nAnalyze prediction errors in both space and time\nEngineer new features based on error patterns\nCritically evaluate when prediction errors matter most"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#load-libraries",
    "href": "assignments/assignment5/assignment5.html#load-libraries",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Load Libraries",
    "text": "Load Libraries\n\n\nCode\n# Core tidyverse\nlibrary(tidyverse)\nlibrary(lubridate)\n\n# Spatial data\nlibrary(sf)\nlibrary(tigris)\n\n# Census data\nlibrary(tidycensus)\n\n# Weather data\nlibrary(riem)  # For Philadelphia weather from ASOS stations\n\n# Visualization\nlibrary(viridis)\nlibrary(grid)\nlibrary(gridExtra)\nlibrary(knitr)\nlibrary(kableExtra)\n\n# here!\nlibrary(here)\n\n# Extra\nlibrary(tidytext)\n\n# Get rid of scientific notation. We gotta look good!\noptions(scipen = 999)"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#define-themes",
    "href": "assignments/assignment5/assignment5.html#define-themes",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Define Themes",
    "text": "Define Themes\n\n\nCode\nplotTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.text.x = element_text(size = 10, angle = 45, hjust = 1),\n  axis.text.y = element_text(size = 10),\n  axis.title = element_text(size = 11, face = \"bold\"),\n  panel.background = element_blank(),\n  panel.grid.major = element_line(colour = \"#D0D0D0\", size = 0.2),\n  panel.grid.minor = element_blank(),\n  axis.ticks = element_blank(),\n  legend.position = \"right\"\n)\n\nmapTheme &lt;- theme(\n  plot.title = element_text(size = 14, face = \"bold\"),\n  plot.subtitle = element_text(size = 10),\n  plot.caption = element_text(size = 8),\n  axis.line = element_blank(),\n  axis.text = element_blank(),\n  axis.ticks = element_blank(),\n  axis.title = element_blank(),\n  panel.background = element_blank(),\n  panel.border = element_blank(),\n  panel.grid.major = element_line(colour = 'transparent'),\n  panel.grid.minor = element_blank(),\n  legend.position = \"right\",\n  plot.margin = margin(1, 1, 1, 1, 'cm'),\n  legend.key.height = unit(1, \"cm\"),\n  legend.key.width = unit(0.2, \"cm\")\n)\n\npalette5 &lt;- c(\"#eff3ff\", \"#bdd7e7\", \"#6baed6\", \"#3182bd\", \"#08519c\")"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#set-census-api-key",
    "href": "assignments/assignment5/assignment5.html#set-census-api-key",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Set Census API Key",
    "text": "Set Census API Key\n\n\nCode\nkey &lt;- Sys.getenv(\"CENSUS_API_KEY\")\ncensus_api_key(key)"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#load-the-data",
    "href": "assignments/assignment5/assignment5.html#load-the-data",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Load the Data",
    "text": "Load the Data\n\n\nCode\n# Read Q1 2025 data\nindego_Q1 &lt;- read_csv(\"/Users/jack/Documents/GitHub/portfolio-setup-jbader14/assignments/assignment5/data/indego-trips-2025-q1.csv\")\n\n# Read Q2 2025 data\nindego_Q2 &lt;- read_csv(\"/Users/jack/Documents/GitHub/portfolio-setup-jbader14/assignments/assignment5/data/indego-trips-2025-q2.csv\")"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#create-hourly-time-bins",
    "href": "assignments/assignment5/assignment5.html#create-hourly-time-bins",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Create Hourly Time Bins",
    "text": "Create Hourly Time Bins\n\n\nCode\nindego_Q1 &lt;- indego_Q1 %&gt;%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\nindego_Q2 &lt;- indego_Q2 %&gt;%\n  mutate(\n    # Parse datetime\n    start_datetime = mdy_hm(start_time),\n    end_datetime = mdy_hm(end_time),\n    \n    # Create hourly bins\n    interval60 = floor_date(start_datetime, unit = \"hour\"),\n    \n    # Extract time features\n    week = week(interval60),\n    month = month(interval60, label = TRUE),\n    dotw = wday(interval60, label = TRUE),\n    hour = hour(interval60),\n    date = as.Date(interval60),\n    \n    # Create useful indicators\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#exploratory-data-analysis-eda",
    "href": "assignments/assignment5/assignment5.html#exploratory-data-analysis-eda",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nTrips Over Time from Jan-July 2025\n\n\nCode\n# Daily trip counts\ndaily_trips_Q1 &lt;- indego_Q1 %&gt;%\n  group_by(date) %&gt;%\n  summarize(trips = n()) |&gt;\n  mutate(quarter = \"Q1 2025\")\n\ndaily_trips_Q2 &lt;- indego_Q2 %&gt;%\n  group_by(date) %&gt;%\n  summarize(trips = n()) |&gt;\n  mutate(quarter = \"Q2 2025\")\n\n# Combine for both quarters\ndaily_trips_combined &lt;- bind_rows(daily_trips_Q1, daily_trips_Q2)\n\n# Plot\nggplot(daily_trips_combined, aes(x = date, y = trips, color = quarter)) +\n  geom_line(linewidth = 1) +\n  geom_smooth(se = FALSE, linetype = \"dashed\") +\n  scale_color_manual(values = c(\"Q1 2025\" = \"#08519c\", \"Q2 2025\" = \"#6baed6\")) +\n  labs(\n    title = \"Indego Daily Ridership ‚Äì First Half of 2025\",\n    subtitle = \"Comparing winter (Q1) and spring/summer (Q2) demand in Philadelphia\",\n    x = \"Date\",\n    y = \"Daily Trips\",\n    color = \"Quarter\",\n    caption = \"Source: Indego bike share\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\n\n\nHourly Patterns on Weekdays vs.¬†Weekends\n\n\nCode\n# Average trips by hour and day type\nhourly_patterns_Q1 &lt;- indego_Q1 %&gt;%\n  group_by(hour, weekend) %&gt;%\n  summarize(avg_trips = n() / n_distinct(date)) %&gt;%\n  mutate(\n    day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"),\n    quarter = \"Q1 2025\"\n    )\n\nhourly_patterns_Q2 &lt;- indego_Q2 %&gt;%\n  group_by(hour, weekend) %&gt;%\n  summarize(avg_trips = n() / n_distinct(date)) %&gt;%\n  mutate(\n    day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"),\n    quarter = \"Q2 2025\"\n    )\n\n# Combine for both quarters\nhourly_patterns_combined &lt;- bind_rows(hourly_patterns_Q1, hourly_patterns_Q2)\n\n# Plot\nggplot(hourly_patterns_combined,\n       aes(x = hour, y = avg_trips, color = quarter)) +\n  geom_line(linewidth = 1.1) +\n  facet_wrap(~ day_type, ncol = 1, scales = \"free_y\") +\n  scale_color_manual(values = c(\"Q1 2025\" = \"#08519c\", \"Q2 2025\" = \"#6baed6\")) +\n  scale_x_continuous(breaks = seq(0, 23, by = 3)) +\n  labs(\n    title = \"Average Hourly Ridership Patterns by Quarter\",\n    subtitle = \"Weekday commute peaks and weekend usage in Q1 vs Q2 2025\",\n    x = \"Hour of Day\",\n    y = \"Average Trips per Station-Hour\",\n    color = \"Quarter\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#top-stations",
    "href": "assignments/assignment5/assignment5.html#top-stations",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Top Stations",
    "text": "Top Stations\n\n\nCode\n# Most popular origin stations\ntop_station_Q1 &lt;- indego_Q1 |&gt;\n  count(start_station, start_lat, start_lon, name = \"trips\") %&gt;%\n  arrange(desc(trips)) %&gt;%\n  head(20)\n  \ntop_stations_Q2 &lt;- indego_Q2 %&gt;%\n  count(start_station, start_lat, start_lon, name = \"trips\") %&gt;%\n  arrange(desc(trips)) %&gt;%\n  head(20)\n\n# Combine top stations for both quarters\ntop_stations_both &lt;- bind_rows(\n  indego_Q1 %&gt;% mutate(quarter = \"Q1 2025\"),\n  indego_Q2 %&gt;% mutate(quarter = \"Q2 2025\")\n) %&gt;%\n  count(quarter, start_station, name = \"trips\") %&gt;%\n  group_by(quarter) %&gt;%\n  slice_max(order_by = trips, n = 10, with_ties = FALSE) %&gt;%\n  ungroup() |&gt;\n  mutate(start_station_re = reorder_within(start_station, trips, quarter))\n\nggplot(top_stations_both,\n       aes(x = start_station_re, y = trips, fill = quarter)) +\n  geom_col(show.legend = FALSE) +\n  coord_flip() +\n  facet_wrap(~ quarter, scales = \"free_y\") +\n  scale_fill_manual(values = c(\"Q1 2025\" = \"#08519c\", \"Q2 2025\" = \"#6baed6\")) +\n  scale_x_reordered() +\n  labs(\n    title = paste(\"Top Indego Origin Stations by Quarter\"),\n    subtitle = \"Comparing the most-used stations in Q1 and Q2 2025\",\n    x = \"Start Station\",\n    y = \"Total Trips\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#join-census-data-to-stations-for-q2-data",
    "href": "assignments/assignment5/assignment5.html#join-census-data-to-stations-for-q2-data",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Join Census Data to Stations for Q2 Data",
    "text": "Join Census Data to Stations for Q2 Data\n\n\nCode\n# Map median income\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = NA) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"Context for understanding bike share demand patterns\"\n  ) +\n  # Stations \n  geom_point(\n    data = indego_Q1,\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 0.25, alpha = 0.6\n  ) +\n  mapTheme\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Create sf object for Q2 stations\nstations_sf_Q2 &lt;- indego_Q2 %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Spatial join to get census tract for each Q2 station\nstations_census_Q2 &lt;- st_join(stations_sf_Q2, philly_census, left = TRUE) %&gt;%\n  st_drop_geometry()\n\n# Prepare data for visualization: which Q2 stations got census data?\nstations_for_map_Q2 &lt;- indego_Q2 %&gt;%\n  distinct(start_station, start_lat, start_lon) %&gt;%\n  filter(!is.na(start_lat), !is.na(start_lon)) %&gt;%\n  left_join(\n    stations_census_Q2 %&gt;% select(start_station, Med_Inc),\n    by = \"start_station\"\n  ) %&gt;%\n  mutate(has_census = !is.na(Med_Inc))\n\n# Create the map showing problem stations for Q2\nggplot() +\n  geom_sf(data = philly_census, aes(fill = Med_Inc), color = \"white\", size = 0.1) +\n  scale_fill_viridis(\n    option = \"viridis\",\n    name = \"Median\\nIncome\",\n    labels = scales::dollar,\n    na.value = \"grey90\"\n  ) +\n  # Stations with census data\n  geom_point(\n    data = stations_for_map_Q2 %&gt;% filter(has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"grey30\", size = 1, alpha = 0.6\n  ) +\n  # Stations WITHOUT census data\n  geom_point(\n    data = stations_for_map_Q2 %&gt;% filter(!has_census),\n    aes(x = start_lon, y = start_lat),\n    color = \"red\", size = 1, shape = 4, stroke = 1.5\n  ) +\n  labs(\n    title = \"Philadelphia Median Household Income by Census Tract\",\n    subtitle = \"indego_Q2 stations shown (RED = no census data match)\",\n    caption = \"Red X marks indicate stations that didn't join to census tracts\"\n  ) +\n  mapTheme\n\n\n\n\n\n\n\n\n\n\n\nCode\n# Identify which stations to keep\nvalid_stations &lt;- stations_census_Q2 %&gt;%\n  filter(!is.na(Med_Inc)) %&gt;%\n  pull(start_station)\n\n# Filter trip data to valid stations only\nindego_census &lt;- indego_Q2 %&gt;%\n  filter(start_station %in% valid_stations) %&gt;%\n  left_join(\n    stations_census_Q2 %&gt;% \n      select(start_station, Med_Inc, Percent_Taking_Transit, \n             Percent_White, Total_Pop),\n    by = \"start_station\"\n  )"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#get-weather-data",
    "href": "assignments/assignment5/assignment5.html#get-weather-data",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Get Weather Data",
    "text": "Get Weather Data\n\n\nCode\n# Get weather from Philadelphia International Airport (KPHL)\n# This covers Q1 2025: January 1 - March 31\nweather_data_Q1 &lt;- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2025-01-01\",\n  date_end = \"2025-03-31\"\n)\n\n# Process weather data\nweather_processed_Q1 &lt;- weather_data_Q1 %&gt;%\n  mutate(\n    interval60 = floor_date(valid, unit = \"hour\"),\n    Temperature = tmpf,  # Temperature in Fahrenheit\n    Precipitation = ifelse(is.na(p01i), 0, p01i),  # Hourly precip in inches\n    Wind_Speed = sknt  # Wind speed in knots\n  ) %&gt;%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %&gt;%\n  distinct()\n\n# Check for missing hours and interpolate if needed\nweather_complete_Q1 &lt;- weather_processed_Q1 %&gt;%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %&gt;%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n\n\n\nCode\n# Get weather for Q2 2025: April 1 - June 30\nweather_data_Q2 &lt;- riem_measures(\n  station = \"PHL\",  # Philadelphia International Airport\n  date_start = \"2025-04-01\",\n  date_end   = \"2025-06-30\"\n)\n\n# Process Q2 weather data\nweather_processed_Q2 &lt;- weather_data_Q2 %&gt;%\n  mutate(\n    interval60   = floor_date(valid, unit = \"hour\"),\n    Temperature  = tmpf,\n    Precipitation = ifelse(is.na(p01i), 0, p01i),\n    Wind_Speed   = sknt\n  ) %&gt;%\n  select(interval60, Temperature, Precipitation, Wind_Speed) %&gt;%\n  distinct()\n\n# Fill in any missing hours\nweather_complete_Q2 &lt;- weather_processed_Q2 %&gt;%\n  complete(interval60 = seq(min(interval60), max(interval60), by = \"hour\")) %&gt;%\n  fill(Temperature, Precipitation, Wind_Speed, .direction = \"down\")\n\n\n\n\nCode\nweather_complete_Q1 &lt;- weather_complete_Q1 %&gt;%\n  mutate(quarter = \"Q1 2025\")\n\nweather_complete_Q2 &lt;- weather_complete_Q2 %&gt;%\n  mutate(quarter = \"Q2 2025\")\n\nweather_both &lt;- bind_rows(weather_complete_Q1, weather_complete_Q2)\n\nggplot(weather_both, aes(x = interval60, y = Temperature, color = quarter)) +\n  geom_line(alpha = 0.7) +\n  geom_smooth(se = FALSE, linetype = \"dashed\") +\n  scale_color_manual(values = c(\"Q1 2025\" = \"#08519c\", \"Q2 2025\" = \"#6baed6\")) +\n  labs(\n    title = \"Philadelphia Temperature ‚Äì Q1 vs Q2 2025\",\n    subtitle = \"Winter to summer transition over the Indego study period\",\n    x = \"Date\",\n    y = \"Temperature (¬∞F)\",\n    color = \"Quarter\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#create-space-time-panel-for-q2",
    "href": "assignments/assignment5/assignment5.html#create-space-time-panel-for-q2",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Create Space-Time Panel for Q2",
    "text": "Create Space-Time Panel for Q2\n\nAggregate Trips to Station-Hour Level\n\n\nCode\n# Count trips by station-hour (Q2)\ntrips_panel_Q2 &lt;- indego_census %&gt;%\n  group_by(\n    interval60, start_station, start_lat, start_lon,\n    Med_Inc, Percent_Taking_Transit, Percent_White, Total_Pop\n  ) %&gt;%\n  summarize(Trip_Count = n(), .groups = \"drop\")\n\n\n\n\nCreate Complete Panel Structure\n\n\nCode\n# Calculate expected panel size for Q2\nn_stations_Q2 &lt;- length(unique(trips_panel_Q2$start_station))\nn_hours_Q2    &lt;- length(unique(trips_panel_Q2$interval60))\nexpected_rows_Q2 &lt;- n_stations_Q2 * n_hours_Q2\n\n# Create complete panel for Q2\nstudy_panel_Q2 &lt;- expand.grid(\n  interval60    = unique(trips_panel_Q2$interval60),\n  start_station = unique(trips_panel_Q2$start_station)\n) %&gt;%\n  # Join trip counts\n  left_join(trips_panel_Q2, by = c(\"interval60\", \"start_station\")) %&gt;%\n  # Replace NA trip counts with 0\n  mutate(Trip_Count = replace_na(Trip_Count, 0))\n\n# Fill in station attributes (same for all hours)\nstation_attributes_Q2 &lt;- trips_panel_Q2 %&gt;%\n  group_by(start_station) %&gt;%\n  summarize(\n    start_lat = first(start_lat),\n    start_lon = first(start_lon),\n    Med_Inc = first(Med_Inc),\n    Percent_Taking_Transit = first(Percent_Taking_Transit),\n    Percent_White = first(Percent_White),\n    Total_Pop = first(Total_Pop),\n    .groups = \"drop\"\n  )\n\nstudy_panel_Q2 &lt;- study_panel_Q2 %&gt;%\n  left_join(station_attributes_Q2, by = \"start_station\")\n\n\n\n\nAdd Time Features\n\n\nCode\nstudy_panel_Q2 &lt;- study_panel_Q2 %&gt;%\n  mutate(\n    week   = week(interval60),\n    month  = month(interval60, label = TRUE),\n    dotw   = wday(interval60, label = TRUE),\n    hour   = hour(interval60),\n    date   = as.Date(interval60),\n    weekend = ifelse(dotw %in% c(\"Sat\", \"Sun\"), 1, 0),\n    rush_hour = ifelse(hour %in% c(7, 8, 9, 16, 17, 18), 1, 0)\n  )\n\n\n\n\nJoin Weather Data\n\n\nCode\nstudy_panel_Q2 &lt;- study_panel_Q2 %&gt;%\n  left_join(weather_complete_Q2, by = \"interval60\")\n\n\n\n\nCreate Temporal Lag Variables\n\n\nCode\n# Sort by station and time for Q2\nstudy_panel_Q2 &lt;- study_panel_Q2 %&gt;%\n  arrange(start_station, interval60)\n\n# Create lag variables WITHIN each station (Q2)\nstudy_panel_Q2 &lt;- study_panel_Q2 %&gt;%\n  group_by(start_station) %&gt;%\n  mutate(\n    lag1Hour   = lag(Trip_Count, 1),\n    lag2Hours  = lag(Trip_Count, 2),\n    lag3Hours  = lag(Trip_Count, 3),\n    lag12Hours = lag(Trip_Count, 12),\n    lag1day    = lag(Trip_Count, 24)\n  ) %&gt;%\n  ungroup()\n\n# Remove rows with NA lags (first 24 hours for each station)\nstudy_panel_complete_Q2 &lt;- study_panel_Q2 %&gt;%\n  filter(!is.na(lag1day))\n\n\n\n\nVisualize Temporal Lag Variables\n\n\nCode\n# Sample one station in Q2 to visualize lag structure\nexample_station_Q2 &lt;- study_panel_complete_Q2 %&gt;%\n  filter(start_station == first(start_station)) %&gt;%\n  arrange(interval60) %&gt;%\n  head(168)  # roughly one week of hourly data\n\n# Plot actual vs lagged demand for Q2\nggplot(example_station_Q2, aes(x = interval60)) +\n  geom_line(aes(y = Trip_Count, color = \"Current\"), linewidth = 1) +\n  geom_line(aes(y = lag1Hour,   color = \"1 Hour Ago\"), linewidth = 1, alpha = 0.7) +\n  geom_line(aes(y = lag1day,    color = \"24 Hours Ago\"), linewidth = 1, alpha = 0.7) +\n  scale_color_manual(values = c(\n    \"Current\"      = \"#08519c\",\n    \"1 Hour Ago\"   = \"#3182bd\",\n    \"24 Hours Ago\" = \"#6baed6\"\n  )) +\n  labs(\n    title = \"Temporal Lag Patterns at One Station ‚Äì Q2 2025\",\n    subtitle = \"Past demand predicts future demand in spring/summer\",\n    x = \"Date-Time\",\n    y = \"Trip Count\",\n    color = \"Time Period\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#temporal-traintest-split",
    "href": "assignments/assignment5/assignment5.html#temporal-traintest-split",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Temporal Train/Test Split",
    "text": "Temporal Train/Test Split\n\n\nCode\nearly_stations_Q2 &lt;- study_panel_complete_Q2 %&gt;%\n  filter(week &lt; 24) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\nlate_stations_Q2 &lt;- study_panel_complete_Q2 %&gt;%\n  filter(week &gt;= 24) %&gt;%\n  filter(Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\n# Keep only stations that appear in BOTH periods\ncommon_stations_Q2 &lt;- intersect(early_stations_Q2, late_stations_Q2)\n\n# Filter panel to only common stations\nstudy_panel_complete_Q2 &lt;- study_panel_complete_Q2 %&gt;%\n  filter(start_station %in% common_stations_Q2)\n\n# NOW create train/test split\ntrain_Q2 &lt;- study_panel_complete_Q2 %&gt;%\n  filter(week &lt; 24)\n\ntest_Q2 &lt;- study_panel_complete_Q2 %&gt;%\n  filter(week &gt;= 24)"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#build-predictive-models",
    "href": "assignments/assignment5/assignment5.html#build-predictive-models",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Build Predictive Models",
    "text": "Build Predictive Models\n\nModel 1: Baseline (Time + Weather)\n\n\nCode\n# Create day-of-week factor with treatment (dummy) coding for Q2\ntrain_Q2 &lt;- train_Q2 %&gt;%\n  mutate(dotw_simple = factor(dotw,\n                              levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(train_Q2$dotw_simple) &lt;- contr.treatment(7)\n\n# Fit baseline model for Q2\nmodel1_Q2 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation,\n  data = train_Q2\n)\n\nsummary(model1_Q2)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation, data = train_Q2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.6662 -0.6681 -0.2166  0.2141 25.5321 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -0.6401338  0.0128755 -49.717  &lt; 2e-16 ***\nas.factor(hour)1  -0.0523209  0.0105611  -4.954 7.27e-07 ***\nas.factor(hour)2  -0.0674401  0.0107101  -6.297 3.04e-10 ***\nas.factor(hour)3  -0.1099673  0.0107783 -10.203  &lt; 2e-16 ***\nas.factor(hour)4  -0.0899380  0.0107798  -8.343  &lt; 2e-16 ***\nas.factor(hour)5   0.0109760  0.0105808   1.037  0.29957    \nas.factor(hour)6   0.2363156  0.0108348  21.811  &lt; 2e-16 ***\nas.factor(hour)7   0.4954238  0.0106028  46.726  &lt; 2e-16 ***\nas.factor(hour)8   0.8313410  0.0105356  78.908  &lt; 2e-16 ***\nas.factor(hour)9   0.5942488  0.0103217  57.573  &lt; 2e-16 ***\nas.factor(hour)10  0.4726058  0.0104668  45.153  &lt; 2e-16 ***\nas.factor(hour)11  0.5179859  0.0103142  50.221  &lt; 2e-16 ***\nas.factor(hour)12  0.5401402  0.0103088  52.396  &lt; 2e-16 ***\nas.factor(hour)13  0.5492886  0.0101829  53.942  &lt; 2e-16 ***\nas.factor(hour)14  0.5821472  0.0101828  57.170  &lt; 2e-16 ***\nas.factor(hour)15  0.6922145  0.0103232  67.054  &lt; 2e-16 ***\nas.factor(hour)16  0.8434144  0.0103646  81.374  &lt; 2e-16 ***\nas.factor(hour)17  1.1356218  0.0102911 110.349  &lt; 2e-16 ***\nas.factor(hour)18  0.9002501  0.0106532  84.505  &lt; 2e-16 ***\nas.factor(hour)19  0.6336711  0.0105063  60.314  &lt; 2e-16 ***\nas.factor(hour)20  0.4005331  0.0107161  37.377  &lt; 2e-16 ***\nas.factor(hour)21  0.2569478  0.0105289  24.404  &lt; 2e-16 ***\nas.factor(hour)22  0.1717711  0.0108530  15.827  &lt; 2e-16 ***\nas.factor(hour)23  0.0803159  0.0104766   7.666 1.77e-14 ***\ndotw_simple2       0.0429457  0.0055832   7.692 1.45e-14 ***\ndotw_simple3      -0.0257147  0.0056504  -4.551 5.34e-06 ***\ndotw_simple4       0.0430157  0.0055569   7.741 9.88e-15 ***\ndotw_simple5      -0.0150977  0.0055021  -2.744  0.00607 ** \ndotw_simple6      -0.0500331  0.0056600  -8.840  &lt; 2e-16 ***\ndotw_simple7      -0.0612792  0.0058641 -10.450  &lt; 2e-16 ***\nTemperature        0.0134250  0.0001574  85.273  &lt; 2e-16 ***\nPrecipitation     -0.0685715  0.0215163  -3.187  0.00144 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.115 on 557968 degrees of freedom\nMultiple R-squared:  0.1081,    Adjusted R-squared:  0.1081 \nF-statistic:  2182 on 31 and 557968 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nModel 2: Add Temporal Lags\n\n\nCode\nmodel2_Q2 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train_Q2\n)\nsummary(model2_Q2)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train_Q2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.8852  -0.4027  -0.1169   0.1130  20.0773 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -0.2411982  0.0108952 -22.138  &lt; 2e-16 ***\nas.factor(hour)1  -0.0198581  0.0089064  -2.230 0.025772 *  \nas.factor(hour)2  -0.0044708  0.0090347  -0.495 0.620706    \nas.factor(hour)3  -0.0331073  0.0090949  -3.640 0.000272 ***\nas.factor(hour)4  -0.0130003  0.0091032  -1.428 0.153262    \nas.factor(hour)5   0.0675403  0.0089373   7.557 4.13e-14 ***\nas.factor(hour)6   0.2360966  0.0091544  25.790  &lt; 2e-16 ***\nas.factor(hour)7   0.3757814  0.0089637  41.923  &lt; 2e-16 ***\nas.factor(hour)8   0.5668671  0.0089190  63.557  &lt; 2e-16 ***\nas.factor(hour)9   0.2219475  0.0087441  25.383  &lt; 2e-16 ***\nas.factor(hour)10  0.1731892  0.0088502  19.569  &lt; 2e-16 ***\nas.factor(hour)11  0.2376620  0.0087227  27.246  &lt; 2e-16 ***\nas.factor(hour)12  0.2796895  0.0087132  32.100  &lt; 2e-16 ***\nas.factor(hour)13  0.2981749  0.0086038  34.656  &lt; 2e-16 ***\nas.factor(hour)14  0.3145055  0.0086057  36.546  &lt; 2e-16 ***\nas.factor(hour)15  0.3892441  0.0087282  44.596  &lt; 2e-16 ***\nas.factor(hour)16  0.4867317  0.0087719  55.488  &lt; 2e-16 ***\nas.factor(hour)17  0.6898972  0.0087287  79.038  &lt; 2e-16 ***\nas.factor(hour)18  0.3669791  0.0090541  40.532  &lt; 2e-16 ***\nas.factor(hour)19  0.2052942  0.0089093  23.043  &lt; 2e-16 ***\nas.factor(hour)20  0.0719676  0.0090776   7.928 2.23e-15 ***\nas.factor(hour)21  0.0575834  0.0088959   6.473 9.62e-11 ***\nas.factor(hour)22  0.0499386  0.0091583   5.453 4.96e-08 ***\nas.factor(hour)23  0.0232192  0.0088347   2.628 0.008584 ** \ndotw_simple2       0.0160822  0.0047079   3.416 0.000636 ***\ndotw_simple3      -0.0292650  0.0047674  -6.139 8.33e-10 ***\ndotw_simple4       0.0215806  0.0046856   4.606 4.11e-06 ***\ndotw_simple5      -0.0112621  0.0046398  -2.427 0.015213 *  \ndotw_simple6      -0.0245716  0.0047746  -5.146 2.66e-07 ***\ndotw_simple7      -0.0399982  0.0049467  -8.086 6.19e-16 ***\nTemperature        0.0036379  0.0001347  27.004  &lt; 2e-16 ***\nPrecipitation     -0.0947865  0.0181435  -5.224 1.75e-07 ***\nlag1Hour           0.4292114  0.0012345 347.687  &lt; 2e-16 ***\nlag3Hours          0.1203061  0.0012187  98.713  &lt; 2e-16 ***\nlag1day            0.1309464  0.0011289 115.996  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9398 on 557965 degrees of freedom\nMultiple R-squared:  0.3659,    Adjusted R-squared:  0.3659 \nF-statistic:  9471 on 34 and 557965 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nModel 3: Add Demographics\n\n\nCode\nmodel3_Q2 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y,\n  data = train_Q2\n)\nsummary(model3_Q2)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + Med_Inc.x + \n    Percent_Taking_Transit.y + Percent_White.y, data = train_Q2)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-7.3562 -0.6590 -0.2515  0.4050 20.3836 \n\nCoefficients:\n                           Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)               4.723e-01  3.397e-02  13.904  &lt; 2e-16 ***\nas.factor(hour)1          1.633e-02  3.623e-02   0.451  0.65228    \nas.factor(hour)2          7.157e-02  3.912e-02   1.829  0.06735 .  \nas.factor(hour)3         -9.004e-02  4.972e-02  -1.811  0.07017 .  \nas.factor(hour)4         -5.921e-02  4.712e-02  -1.257  0.20887    \nas.factor(hour)5          9.883e-03  3.279e-02   0.301  0.76307    \nas.factor(hour)6          2.976e-01  2.876e-02  10.349  &lt; 2e-16 ***\nas.factor(hour)7          3.902e-01  2.666e-02  14.638  &lt; 2e-16 ***\nas.factor(hour)8          6.055e-01  2.572e-02  23.544  &lt; 2e-16 ***\nas.factor(hour)9          8.146e-02  2.572e-02   3.168  0.00154 ** \nas.factor(hour)10         8.360e-02  2.622e-02   3.189  0.00143 ** \nas.factor(hour)11         1.274e-01  2.579e-02   4.938 7.89e-07 ***\nas.factor(hour)12         1.776e-01  2.564e-02   6.924 4.40e-12 ***\nas.factor(hour)13         2.133e-01  2.545e-02   8.381  &lt; 2e-16 ***\nas.factor(hour)14         1.875e-01  2.528e-02   7.417 1.21e-13 ***\nas.factor(hour)15         2.971e-01  2.526e-02  11.762  &lt; 2e-16 ***\nas.factor(hour)16         4.462e-01  2.519e-02  17.715  &lt; 2e-16 ***\nas.factor(hour)17         7.088e-01  2.491e-02  28.450  &lt; 2e-16 ***\nas.factor(hour)18         2.642e-01  2.534e-02  10.426  &lt; 2e-16 ***\nas.factor(hour)19         1.024e-01  2.556e-02   4.006 6.17e-05 ***\nas.factor(hour)20        -6.404e-03  2.639e-02  -0.243  0.80824    \nas.factor(hour)21        -3.878e-02  2.678e-02  -1.448  0.14755    \nas.factor(hour)22        -1.557e-03  2.801e-02  -0.056  0.95567    \nas.factor(hour)23        -1.039e-02  2.873e-02  -0.362  0.71750    \ndotw_simple2              3.120e-02  1.051e-02   2.969  0.00299 ** \ndotw_simple3             -3.879e-02  1.103e-02  -3.515  0.00044 ***\ndotw_simple4              7.470e-03  1.060e-02   0.705  0.48106    \ndotw_simple5             -5.320e-02  1.045e-02  -5.093 3.52e-07 ***\ndotw_simple6              2.737e-02  1.099e-02   2.490  0.01279 *  \ndotw_simple7              1.548e-02  1.147e-02   1.350  0.17701    \nTemperature               6.590e-03  3.199e-04  20.597  &lt; 2e-16 ***\nPrecipitation            -3.528e-01  3.575e-02  -9.867  &lt; 2e-16 ***\nlag1Hour                  3.194e-01  2.023e-03 157.861  &lt; 2e-16 ***\nlag3Hours                 8.769e-02  2.079e-03  42.171  &lt; 2e-16 ***\nlag1day                   1.171e-01  1.984e-03  59.041  &lt; 2e-16 ***\nMed_Inc.x                 3.267e-07  1.014e-07   3.221  0.00128 ** \nPercent_Taking_Transit.y -3.744e-03  3.801e-04  -9.851  &lt; 2e-16 ***\nPercent_White.y           2.506e-03  1.868e-04  13.417  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.208 on 178266 degrees of freedom\n  (379696 observations deleted due to missingness)\nMultiple R-squared:  0.2578,    Adjusted R-squared:  0.2577 \nF-statistic:  1674 on 37 and 178266 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nModel 4: Add Station Fixed Effects\n\n\nCode\nmodel4_Q2 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station),\n  data = train_Q2\n)\n\ncat(\"Model 4_Q2 R-squared:\", summary(model4_Q2)$r.squared, \"\\n\")\n\n\nModel 4_Q2 R-squared: 0.2801177 \n\n\nCode\ncat(\"Model 4_Q2 Adj R-squared:\", summary(model4_Q2)$adj.r.squared, \"\\n\")\n\n\nModel 4_Q2 Adj R-squared: 0.2789814 \n\n\n\n\nModel 5: Add Rush Hour Interaction\n\n\nCode\nmodel5_Q2 &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + rush_hour + as.factor(month) +\n    Med_Inc.x + Percent_Taking_Transit.y + Percent_White.y +\n    as.factor(start_station) +\n    rush_hour * weekend,\n  data = train_Q2\n)\n\ncat(\"Model 5_Q2 R-squared:\", summary(model5_Q2)$r.squared, \"\\n\")\n\n\nModel 5_Q2 R-squared: 0.2836837 \n\n\nCode\ncat(\"Model 5_Q2 Adj R-squared:\", summary(model5_Q2)$adj.r.squared, \"\\n\")\n\n\nModel 5_Q2 Adj R-squared: 0.282541"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#model-evaluation-on-mae",
    "href": "assignments/assignment5/assignment5.html#model-evaluation-on-mae",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Model Evaluation (on MAE)",
    "text": "Model Evaluation (on MAE)\n\n\nCode\n# Create day-of-week factor with treatment (dummy) coding for Q2\ntest_Q2 &lt;- test_Q2 %&gt;%\n  mutate(dotw_simple = factor(dotw,\n                              levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\n# Set contrasts to treatment coding (dummy variables)\ncontrasts(test_Q2$dotw_simple) &lt;- contr.treatment(7)\n\n# Add model predictions for Q2\ntest_Q2 &lt;- test_Q2 %&gt;%\n  mutate(\n    pred1_Q2 = predict(model1_Q2, newdata = test_Q2),\n    pred2_Q2 = predict(model2_Q2, newdata = test_Q2),\n    pred3_Q2 = predict(model3_Q2, newdata = test_Q2),\n    pred4_Q2 = predict(model4_Q2, newdata = test_Q2),\n    pred5_Q2 = predict(model5_Q2, newdata = test_Q2)\n  )\n\n# Calculate MAE for each Q2 model\nmae_results_Q2 &lt;- data.frame(\n  Model = c(\n    \"1. Time + Weather\",\n    \"2. + Temporal Lags\",\n    \"3. + Demographics\",\n    \"4. + Station FE\",\n    \"5. + Rush Hour Interaction\"\n  ),\n  MAE = c(\n    mean(abs(test_Q2$Trip_Count - test_Q2$pred1_Q2), na.rm = TRUE),\n    mean(abs(test_Q2$Trip_Count - test_Q2$pred2_Q2), na.rm = TRUE),\n    mean(abs(test_Q2$Trip_Count - test_Q2$pred3_Q2), na.rm = TRUE),\n    mean(abs(test_Q2$Trip_Count - test_Q2$pred4_Q2), na.rm = TRUE),\n    mean(abs(test_Q2$Trip_Count - test_Q2$pred5_Q2), na.rm = TRUE)\n  )\n)\n\nkable(mae_results_Q2,\n      digits = 2,\n      caption = \"Mean Absolute Error by Model (Test Set, Q2 2025)\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nMean Absolute Error by Model (Test Set, Q2 2025)\n\n\nModel\nMAE (trips)\n\n\n\n\n1. Time + Weather\n0.81\n\n\n2. + Temporal Lags\n0.62\n\n\n3. + Demographics\n0.86\n\n\n4. + Station FE\n0.86\n\n\n5. + Rush Hour Interaction\n0.85"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#visualize-model-comparison",
    "href": "assignments/assignment5/assignment5.html#visualize-model-comparison",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Visualize Model Comparison",
    "text": "Visualize Model Comparison\n\n\nCode\nggplot(mae_results_Q2, aes(x = reorder(Model, -MAE), y = MAE)) +\n  geom_col(fill = \"#3182bd\", alpha = 0.8) +\n  geom_text(aes(label = round(MAE, 2)), vjust = 1.5) +\n  labs(\n    title = \"Model Performance Comparison\",\n    subtitle = \"Lower MAE = Better Predictions\",\n    x = \"Model\",\n    y = \"Mean Absolute Error (trips)\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteComparison of Model Results to Q1 2025:\n\n\n\nThe MAE results from the same five regression model families perform notably worse across the board for the 2025 Q2 data than for the Q1 data. We found that model 2 performed best for the Q2 data with the lowest MAE, which is the same as for Q1, indicating that the temporal lags are the most important features to consider. Added model complexity in models 3-5 caused the model to perform worse. These additional predictors were either not influential in predicting bike-share demand or caused the model to overfit to the training data.\nThe main intuition behind the models performing worse for Q2 lie in the increased demand for bikes in this time period compared to Q1. In our analysis, we found several connections tying higher demand to worse model performance, which we delve more into in our formal report. The primary driving factor for increased demand lie in the temporal patterns in Q2 where weather is warmer in Philadelphia, increasing the demand for bike-share. A supplementary factor could also lie in temporal events during this quarter such as the end of the semester for colleges such as UPenn. These schools go on Summer break in early May, leading to less foot traffic on a day-to-day basis, especially on weekdays, which may have not been accurately captured in these models."
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#observed-vs.-predicted",
    "href": "assignments/assignment5/assignment5.html#observed-vs.-predicted",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Observed vs.¬†Predicted",
    "text": "Observed vs.¬†Predicted\n\n\nCode\ntest_Q2 &lt;- test_Q2 %&gt;%\n  mutate(\n    error = Trip_Count - pred2_Q2,\n    abs_error = abs(error),\n    time_of_day = case_when(\n      hour &lt; 7 ~ \"Overnight\",\n      hour &gt;= 7 & hour &lt; 10 ~ \"AM Rush\",\n      hour &gt;= 10 & hour &lt; 15 ~ \"Mid-Day\",\n      hour &gt;= 15 & hour &lt;= 18 ~ \"PM Rush\",\n      hour &gt; 18 ~ \"Evening\"\n    )\n  )\n\nggplot(test_Q2, aes(x = Trip_Count, y = pred2_Q2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips ‚Äì Model 2 (Q2 2025)\",\n    subtitle = \"Performance by time-of-day and weekday/weekend\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#spatial-error-patterns",
    "href": "assignments/assignment5/assignment5.html#spatial-error-patterns",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Spatial Error Patterns",
    "text": "Spatial Error Patterns\n\n\nCode\n# MAE by station (Q2, Model 2)\nstation_errors_Q2 &lt;- test_Q2 %&gt;%\n  filter(!is.na(pred2_Q2)) %&gt;%\n  group_by(start_station, start_lat.x, start_lon.x) %&gt;%\n  summarize(\n    MAE        = mean(abs(Trip_Count - pred2_Q2), na.rm = TRUE),\n    avg_demand = mean(Trip_Count, na.rm = TRUE),\n    .groups    = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.x), !is.na(start_lon.x))\n\n# Map 1: Prediction errors\np1_Q2 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors_Q2,\n    aes(x = start_lon.x, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name = \"MAE (trips)\",\n    direction = -1\n  ) +\n  labs(\n    title = \"Prediction Errors ‚Äì Q2 2025\",\n    subtitle = \"Station-level MAE (Model 2)\"\n  ) +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title    = element_text(size = 10, face = \"bold\"),\n    legend.text     = element_text(size = 9),\n    plot.title      = element_text(size = 14, face = \"bold\"),\n    plot.subtitle   = element_text(size = 10)\n  )\n\n# Map 2: Average demand\np2_Q2 &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors_Q2,\n    aes(x = start_lon.x, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name = \"Avg Demand\\n(trips/hour)\",\n    direction = -1\n  ) +\n  labs(\n    title = \"Average Demand ‚Äì Q2 2025\",\n    subtitle = \"Mean trips per station-hour\"\n  ) +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title    = element_text(size = 10, face = \"bold\"),\n    legend.text     = element_text(size = 9),\n    plot.title      = element_text(size = 14, face = \"bold\"),\n    plot.subtitle   = element_text(size = 10)\n  )\n\n# Side-by-side layout\ngrid.arrange(\n  p1_Q2, p2_Q2,\n  ncol = 2,\n  top = grid::textGrob(\n    \"Model 2 Performance in Q2: Prediction Errors vs Average Demand\",\n    gp = grid::gpar(fontsize = 16, fontface = \"bold\")\n  )\n)"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#temporal-error-predictions",
    "href": "assignments/assignment5/assignment5.html#temporal-error-predictions",
    "title": "Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Temporal Error Predictions",
    "text": "Temporal Error Predictions\n\n\nCode\ntemporal_errors_Q2 &lt;- test_Q2 %&gt;%\n  group_by(time_of_day, weekend) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors_Q2, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period ‚Äì Q2 2025\",\n    subtitle = \"Model 2 mean absolute error across time-of-day and day type\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#errors-and-demographics",
    "href": "assignments/assignment5/assignment5.html#errors-and-demographics",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Errors and Demographics",
    "text": "Errors and Demographics\n\n\nCode\nstation_errors_demo_Q2 &lt;- station_errors_Q2 %&gt;%\n  left_join(\n    station_attributes_Q2 %&gt;%\n      select(start_station, Med_Inc, Percent_Taking_Transit, Percent_White),\n    by = \"start_station\"\n  ) %&gt;%\n  filter(!is.na(Med_Inc))\n\np1_Q2 &lt;- ggplot(station_errors_demo_Q2, aes(x = Med_Inc, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  scale_x_continuous(labels = scales::dollar) +\n  labs(\n    title = \"Errors vs. Median Income ‚Äì Q2 2025\",\n    x = \"Median Income\",\n    y = \"MAE (trips)\"\n  ) +\n  plotTheme\n\np2_Q2 &lt;- ggplot(station_errors_demo_Q2, aes(x = Percent_Taking_Transit, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Errors vs. Transit Usage ‚Äì Q2 2025\",\n    x = \"% Taking Transit\",\n    y = \"MAE (trips)\"\n  ) +\n  plotTheme\n\np3_Q2 &lt;- ggplot(station_errors_demo_Q2, aes(x = Percent_White, y = MAE)) +\n  geom_point(alpha = 0.5, color = \"#3182bd\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Errors vs. Race ‚Äì Q2 2025\",\n    x = \"% White\",\n    y = \"MAE (trips)\"\n  ) +\n  plotTheme\n\ngrid.arrange(p1_Q2, p2_Q2, p3_Q2, ncol = 2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteError Analysis\n\n\n\nFrom our error analysis, there is substantial evidence of a direct relationship between bike-share demand and model errors:\n\nHigh demand neighborhoods, notably Center City, had the highest errors from our spatial error analysis.\nHigh demand times such as the PM rush had the highest errors from our temporal error analysis. Conversely, low demand times during overnight had very low demand and errors.\nThere don‚Äôt seem to be any strong relationships between demographic data and model errors, suggesting that these variables are not strong predictors of bike-share demand. This supports our claim that adding these variables harm model performance.\n\nWe go into more detail on this error analysis in our formal report."
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#feature-engineering",
    "href": "assignments/assignment5/assignment5.html#feature-engineering",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\n\nCode\n# Nice weather = 60‚Äì75¬∞F and no rain\nstudy_panel_Q2 &lt;- study_panel_Q2 %&gt;%\n  mutate(\n    nice_weather = ifelse(\n      Temperature &gt;= 60 & Temperature &lt;= 75 & Precipitation == 0,\n      1, 0\n    )\n  )\n\n\n\n\nCode\n# Approximate coordinates for City Hall as proxy for Center City\ncity_hall &lt;- st_sfc(\n  st_point(c(-75.1635996, 39.9523789)),\n  crs = 4326\n)\n\nstation_sf_Q2 &lt;- station_attributes_Q2 %&gt;%\n  st_as_sf(coords = c(\"start_lon\", \"start_lat\"), crs = 4326)\n\n# Compute distance to City Hall in meters\nstation_sf_Q2 &lt;- station_sf_Q2 %&gt;%\n  mutate(\n    dist_cc = as.numeric(st_distance(., city_hall)[, 1])\n  )\n\n# Drop geometry and update station_attributes_Q2\nstation_attributes_Q2 &lt;- station_sf_Q2 %&gt;%\n  st_drop_geometry()\n\n# Join distance back into the Q2 panel\nstudy_panel_Q2 &lt;- study_panel_Q2 %&gt;%\n  left_join(\n    station_attributes_Q2 %&gt;% select(start_station, dist_cc),\n    by = \"start_station\"\n  )\n\n\n\n\nCode\n# Order rows by station and time\nstudy_panel_Q2 &lt;- study_panel_Q2 %&gt;%\n  arrange(start_station, interval60)\n\n# Add lag for same hour one week ago\nstudy_panel_Q2 &lt;- study_panel_Q2 %&gt;%\n  group_by(start_station) %&gt;%\n  mutate(\n    lag1week = lag(Trip_Count, 24 * 7)\n  ) %&gt;%\n  ungroup()\n\n# Reassign to complete panel, filtering out NA\nstudy_panel_complete_Q2 &lt;- study_panel_Q2 %&gt;%\n  filter(!is.na(lag1day), !is.na(lag1week))"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#temporal-traintest-split-1",
    "href": "assignments/assignment5/assignment5.html#temporal-traintest-split-1",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Temporal Train/Test Split",
    "text": "Temporal Train/Test Split\n\n\nCode\nearly_stations_Q2 &lt;- study_panel_complete_Q2 %&gt;%\n  filter(week &lt; 24, Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\nlate_stations_Q2 &lt;- study_panel_complete_Q2 %&gt;%\n  filter(week &gt;= 24, Trip_Count &gt; 0) %&gt;%\n  distinct(start_station) %&gt;%\n  pull(start_station)\n\ncommon_stations_Q2 &lt;- intersect(early_stations_Q2, late_stations_Q2)\n\nstudy_panel_complete_Q2 &lt;- study_panel_complete_Q2 %&gt;%\n  filter(start_station %in% common_stations_Q2)\n\ntrain_Q2 &lt;- study_panel_complete_Q2 %&gt;%\n  filter(week &lt; 24)\n\ntest_Q2 &lt;- study_panel_complete_Q2 %&gt;%\n  filter(week &gt;= 24)"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#reuse-model-2-as-new-baseline",
    "href": "assignments/assignment5/assignment5.html#reuse-model-2-as-new-baseline",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Reuse Model 2 as New Baseline",
    "text": "Reuse Model 2 as New Baseline\n\n\nCode\ntrain_Q2 &lt;- train_Q2 %&gt;%\n  mutate(dotw_simple = factor(dotw,\n                              levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\ncontrasts(train_Q2$dotw_simple) &lt;- contr.treatment(7)\n\nmodel2_Q2_base &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  data = train_Q2\n)\n\nsummary(model2_Q2_base)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, data = train_Q2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11.8547  -0.4042  -0.1167   0.1119  20.0975 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -0.2581615  0.0112619 -22.923  &lt; 2e-16 ***\nas.factor(hour)1  -0.0196589  0.0091634  -2.145 0.031923 *  \nas.factor(hour)2  -0.0050599  0.0093001  -0.544 0.586392    \nas.factor(hour)3  -0.0327438  0.0093655  -3.496 0.000472 ***\nas.factor(hour)4  -0.0144725  0.0093787  -1.543 0.122802    \nas.factor(hour)5   0.0688439  0.0091958   7.486 7.09e-14 ***\nas.factor(hour)6   0.2395925  0.0094581  25.332  &lt; 2e-16 ***\nas.factor(hour)7   0.3773235  0.0092495  40.794  &lt; 2e-16 ***\nas.factor(hour)8   0.5805500  0.0092820  62.546  &lt; 2e-16 ***\nas.factor(hour)9   0.2264119  0.0090783  24.940  &lt; 2e-16 ***\nas.factor(hour)10  0.1829488  0.0092278  19.826  &lt; 2e-16 ***\nas.factor(hour)11  0.2426113  0.0090756  26.732  &lt; 2e-16 ***\nas.factor(hour)12  0.2787691  0.0090450  30.820  &lt; 2e-16 ***\nas.factor(hour)13  0.2985413  0.0089435  33.381  &lt; 2e-16 ***\nas.factor(hour)14  0.3115643  0.0089242  34.912  &lt; 2e-16 ***\nas.factor(hour)15  0.3862567  0.0090399  42.728  &lt; 2e-16 ***\nas.factor(hour)16  0.4877136  0.0091316  53.410  &lt; 2e-16 ***\nas.factor(hour)17  0.6878987  0.0090359  76.129  &lt; 2e-16 ***\nas.factor(hour)18  0.3735467  0.0093407  39.991  &lt; 2e-16 ***\nas.factor(hour)19  0.2044666  0.0091859  22.259  &lt; 2e-16 ***\nas.factor(hour)20  0.0763686  0.0093408   8.176 2.95e-16 ***\nas.factor(hour)21  0.0579960  0.0091498   6.339 2.32e-10 ***\nas.factor(hour)22  0.0479427  0.0094308   5.084 3.70e-07 ***\nas.factor(hour)23  0.0220346  0.0091099   2.419 0.015574 *  \ndotw_simple2       0.0126323  0.0047871   2.639 0.008320 ** \ndotw_simple3      -0.0336343  0.0048766  -6.897 5.31e-12 ***\ndotw_simple4       0.0236330  0.0048598   4.863 1.16e-06 ***\ndotw_simple5      -0.0065508  0.0047638  -1.375 0.169094    \ndotw_simple6      -0.0204325  0.0048812  -4.186 2.84e-05 ***\ndotw_simple7      -0.0405864  0.0049601  -8.183 2.78e-16 ***\nTemperature        0.0039142  0.0001399  27.976  &lt; 2e-16 ***\nPrecipitation     -0.0932830  0.0200683  -4.648 3.35e-06 ***\nlag1Hour           0.4250951  0.0012778 332.668  &lt; 2e-16 ***\nlag3Hours          0.1206244  0.0012610  95.659  &lt; 2e-16 ***\nlag1day            0.1324015  0.0011674 113.415  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9422 on 522253 degrees of freedom\nMultiple R-squared:  0.3646,    Adjusted R-squared:  0.3646 \nF-statistic:  8816 on 34 and 522253 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#fit-new-model-w-added-features",
    "href": "assignments/assignment5/assignment5.html#fit-new-model-w-added-features",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Fit New Model w/ Added Features",
    "text": "Fit New Model w/ Added Features\n\n\nCode\nmodel2_Q2_new &lt;- lm(\n  Trip_Count ~ as.factor(hour) + dotw_simple +\n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + lag1week +\n    nice_weather + dist_cc,\n  data = train_Q2\n)\n\nsummary(model2_Q2_new)\n\n\n\nCall:\nlm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + lag1week + \n    nice_weather + dist_cc, data = train_Q2)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-10.8471  -0.4200  -0.1114   0.1864  20.5394 \n\nCoefficients:\n                    Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       -1.397e-01  1.153e-02 -12.118  &lt; 2e-16 ***\nas.factor(hour)1  -2.755e-02  9.098e-03  -3.028 0.002463 ** \nas.factor(hour)2  -1.628e-02  9.234e-03  -1.762 0.077986 .  \nas.factor(hour)3  -4.652e-02  9.299e-03  -5.003 5.65e-07 ***\nas.factor(hour)4  -2.564e-02  9.311e-03  -2.753 0.005899 ** \nas.factor(hour)5   5.708e-02  9.131e-03   6.251 4.08e-10 ***\nas.factor(hour)6   2.295e-01  9.394e-03  24.434  &lt; 2e-16 ***\nas.factor(hour)7   3.739e-01  9.185e-03  40.706  &lt; 2e-16 ***\nas.factor(hour)8   5.927e-01  9.219e-03  64.284  &lt; 2e-16 ***\nas.factor(hour)9   2.458e-01  9.019e-03  27.254  &lt; 2e-16 ***\nas.factor(hour)10  2.026e-01  9.171e-03  22.091  &lt; 2e-16 ***\nas.factor(hour)11  2.609e-01  9.020e-03  28.925  &lt; 2e-16 ***\nas.factor(hour)12  2.925e-01  8.985e-03  32.556  &lt; 2e-16 ***\nas.factor(hour)13  3.073e-01  8.881e-03  34.606  &lt; 2e-16 ***\nas.factor(hour)14  3.228e-01  8.869e-03  36.400  &lt; 2e-16 ***\nas.factor(hour)15  4.027e-01  8.993e-03  44.778  &lt; 2e-16 ***\nas.factor(hour)16  5.096e-01  9.083e-03  56.100  &lt; 2e-16 ***\nas.factor(hour)17  7.176e-01  9.010e-03  79.645  &lt; 2e-16 ***\nas.factor(hour)18  4.113e-01  9.338e-03  44.050  &lt; 2e-16 ***\nas.factor(hour)19  2.392e-01  9.180e-03  26.054  &lt; 2e-16 ***\nas.factor(hour)20  1.058e-01  9.326e-03  11.348  &lt; 2e-16 ***\nas.factor(hour)21  7.473e-02  9.122e-03   8.192 2.57e-16 ***\nas.factor(hour)22  5.886e-02  9.365e-03   6.285 3.27e-10 ***\nas.factor(hour)23  2.755e-02  9.042e-03   3.047 0.002314 ** \ndotw_simple2       1.520e-02  4.791e-03   3.173 0.001511 ** \ndotw_simple3      -3.067e-02  4.897e-03  -6.263 3.77e-10 ***\ndotw_simple4       2.219e-02  4.867e-03   4.559 5.15e-06 ***\ndotw_simple5       2.021e-04  4.751e-03   0.043 0.966068    \ndotw_simple6      -1.732e-02  4.895e-03  -3.538 0.000404 ***\ndotw_simple7      -3.551e-02  4.929e-03  -7.205 5.81e-13 ***\nTemperature        4.469e-03  1.543e-04  28.959  &lt; 2e-16 ***\nPrecipitation     -8.259e-02  2.010e-02  -4.110 3.97e-05 ***\nlag1Hour           4.059e-01  1.287e-03 315.497  &lt; 2e-16 ***\nlag3Hours          1.006e-01  1.272e-03  79.067  &lt; 2e-16 ***\nlag1day            1.104e-01  1.185e-03  93.163  &lt; 2e-16 ***\nlag1week           4.398e-02  1.159e-03  37.938  &lt; 2e-16 ***\nnice_weather       4.707e-03  3.060e-03   1.538 0.124000    \ndist_cc           -5.622e-05  7.829e-07 -71.807  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9352 on 522250 degrees of freedom\nMultiple R-squared:  0.3741,    Adjusted R-squared:  0.3741 \nF-statistic:  8436 on 37 and 522250 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#compare-mae-between-m2-baseline-and-new-model",
    "href": "assignments/assignment5/assignment5.html#compare-mae-between-m2-baseline-and-new-model",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Compare MAE between M2 baseline and new model",
    "text": "Compare MAE between M2 baseline and new model\n\n\nCode\ntest_Q2 &lt;- test_Q2 %&gt;%\n  mutate(dotw_simple = factor(dotw,\n                              levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\n\ncontrasts(test_Q2$dotw_simple) &lt;- contr.treatment(7)\n\ntest_Q2 &lt;- test_Q2 %&gt;%\n  mutate(\n    pred2_base_Q2 = predict(model2_Q2_base, newdata = test_Q2),\n    pred2_new_Q2 = predict(model2_Q2_new, newdata = test_Q2)\n  )\n\nmae_model2_base_Q2 &lt;- mean(abs(test_Q2$Trip_Count - test_Q2$pred2_base_Q2), na.rm = TRUE)\nmae_model2_new_Q2 &lt;- mean(abs(test_Q2$Trip_Count - test_Q2$pred2_new_Q2), na.rm = TRUE)\n\nmae_compare_Q2 &lt;- tibble::tibble(\n  Model = c(\"Model 2: Baseline\", \"Model 2: New Features\"),\n  MAE   = c(mae_model2_base_Q2, mae_model2_new_Q2)\n)\n\nkable(mae_compare_Q2,\n      digits = 3,\n      caption = \"Q2 Mean Absolute Error: Baseline Model 2 vs New Model 2+\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nQ2 Mean Absolute Error: Baseline Model 2 vs New Model 2+\n\n\nModel\nMAE (trips)\n\n\n\n\nModel 2: Baseline\n0.625\n\n\nModel 2: New Features\n0.625"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#error-analysis-for-new-model",
    "href": "assignments/assignment5/assignment5.html#error-analysis-for-new-model",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Error Analysis for New Model",
    "text": "Error Analysis for New Model\n\n\nCode\ntest_Q2 &lt;- test_Q2 %&gt;%\n  mutate(\n    # errors for new Model 2\n    error_new     = Trip_Count - pred2_new_Q2,\n    abs_error_new = abs(error_new),\n    time_of_day = case_when(\n      hour &lt; 7              ~ \"Overnight\",\n      hour &gt;= 7 & hour &lt; 10 ~ \"AM Rush\",\n      hour &gt;= 10 & hour &lt; 15 ~ \"Mid-Day\",\n      hour &gt;= 15 & hour &lt;= 18 ~ \"PM Rush\",\n      hour &gt; 18             ~ \"Evening\"\n    )\n  )\n\nggplot(test_Q2, aes(x = Trip_Count, y = pred2_new_Q2)) +\n  geom_point(alpha = 0.2, color = \"#3182bd\") +\n  geom_abline(slope = 1, intercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"darkgreen\") +\n  facet_grid(weekend ~ time_of_day) +\n  labs(\n    title = \"Observed vs. Predicted Bike Trips ‚Äì New Model 2 (Q2 2025)\",\n    subtitle = \"Performance by time-of-day and weekday/weekend\",\n    x = \"Observed Trips\",\n    y = \"Predicted Trips\",\n    caption = \"Red line = perfect predictions; Green line = actual model fit\"\n  ) +\n  plotTheme\n\n\n\n\n\n\n\n\n\n\n\nCode\n## Spatial Error Patterns ‚Äì New Model 2\n# MAE by station (Q2, Model 2 with new features)\nstation_errors_Q2_new &lt;- test_Q2 %&gt;%\n  filter(!is.na(pred2_new_Q2)) %&gt;%\n  group_by(start_station, start_lat.x, start_lon.x) %&gt;%\n  summarize(\n    MAE        = mean(abs_error_new, na.rm = TRUE),\n    avg_demand = mean(Trip_Count,    na.rm = TRUE),\n    .groups    = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.x), !is.na(start_lon.x))\n\n# Map 1: Prediction errors (new model)\np1_Q2_new &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors_Q2_new,\n    aes(x = start_lon.x, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name   = \"MAE (trips)\",\n    direction = -1\n  ) +\n  labs(\n    title    = \"Prediction Errors ‚Äì Q2 2025\",\n    subtitle = \"Station-level MAE with engineered features\"\n  ) +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title    = element_text(size = 10, face = \"bold\"),\n    legend.text     = element_text(size = 9),\n    plot.title      = element_text(size = 14, face = \"bold\"),\n    plot.subtitle   = element_text(size = 10)\n  )\n\n# Map 2: Average demand (same, just reusing the new MAE table)\np2_Q2_new &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors_Q2_new,\n    aes(x = start_lon.x, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name   = \"Avg Demand\\n(trips/hour)\",\n    direction = -1\n  ) +\n  labs(\n    title    = \"Average Demand ‚Äì Q2 2025\",\n    subtitle = \"Mean trips per station-hour\"\n  ) +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title    = element_text(size = 10, face = \"bold\"),\n    legend.text     = element_text(size = 9),\n    plot.title      = element_text(size = 14, face = \"bold\"),\n    plot.subtitle   = element_text(size = 10)\n  )\n\n# Side-by-side layout\ngrid.arrange(\n  p1_Q2_new, p2_Q2_new,\n  ncol = 2,\n  top = grid::textGrob(\n    \"New Model 2 Performance in Q2: Prediction Errors vs Average Demand\",\n    gp = grid::gpar(fontsize = 14, fontface = \"bold\")\n  )\n)\n\n\n\n\n\n\n\n\n\n\n\nCode\n## Temporal Error Patterns ‚Äì New Model 2\ntemporal_errors_Q2_new &lt;- test_Q2 %&gt;%\n  group_by(time_of_day, weekend) %&gt;%\n  summarize(\n    MAE = mean(abs_error_new, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors_Q2_new, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period ‚Äì Q2 2025 (New Model 2)\",\n    subtitle = \"Mean absolute error across time-of-day and day type\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteFeature Engineering\n\n\n\nWe decided to add these features to better capture high demand areas in Philadelphia. As we saw from our error analysis, these cases were the most difficult to predict where our model struggled the most with the highest errors. We incorporated a week-lag to capture high temporal demand, distance to city hall to capture high spatial demand, and good weather to capture high weather-related demand.\nWe found no improvement in overall MAE to our original model 2 with temporal lags. We surmise that the reasoning lies in these variables did not offer enough marginal predictive power to improve prior model performance. Across all of our new error analysis, the new model performs relatively the same, indicating that this added complexity does not benefit model accuracy.\nWe go into more detail on our feature engineering steps in the formal report."
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#spatial-error-patterns-1",
    "href": "assignments/assignment5/assignment5.html#spatial-error-patterns-1",
    "title": "Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Spatial Error Patterns",
    "text": "Spatial Error Patterns\n\n\nCode\n## Spatial Error Patterns ‚Äì New Model 2\n# MAE by station (Q2, Model 2 with new features)\nstation_errors_Q2_new &lt;- test_Q2 %&gt;%\n  filter(!is.na(pred2_new_Q2)) %&gt;%\n  group_by(start_station, start_lat.x, start_lon.x) %&gt;%\n  summarize(\n    MAE        = mean(abs_error_new, na.rm = TRUE),\n    avg_demand = mean(Trip_Count,    na.rm = TRUE),\n    .groups    = \"drop\"\n  ) %&gt;%\n  filter(!is.na(start_lat.x), !is.na(start_lon.x))\n\n# Map 1: Prediction errors (new model)\np1_Q2_new &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors_Q2_new,\n    aes(x = start_lon.x, y = start_lat.x, color = MAE),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"plasma\",\n    name   = \"MAE (trips)\",\n    direction = -1\n  ) +\n  labs(\n    title    = \"Prediction Errors ‚Äì Q2 2025\",\n    subtitle = \"Station-level MAE with engineered features\"\n  ) +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title    = element_text(size = 10, face = \"bold\"),\n    legend.text     = element_text(size = 9),\n    plot.title      = element_text(size = 14, face = \"bold\"),\n    plot.subtitle   = element_text(size = 10)\n  )\n\n# Map 2: Average demand (same, just reusing the new MAE table)\np2_Q2_new &lt;- ggplot() +\n  geom_sf(data = philly_census, fill = \"grey95\", color = \"white\", size = 0.2) +\n  geom_point(\n    data = station_errors_Q2_new,\n    aes(x = start_lon.x, y = start_lat.x, color = avg_demand),\n    size = 3.5,\n    alpha = 0.7\n  ) +\n  scale_color_viridis(\n    option = \"viridis\",\n    name   = \"Avg Demand\\n(trips/hour)\",\n    direction = -1\n  ) +\n  labs(\n    title    = \"Average Demand ‚Äì Q2 2025\",\n    subtitle = \"Mean trips per station-hour\"\n  ) +\n  mapTheme +\n  theme(\n    legend.position = \"right\",\n    legend.title    = element_text(size = 10, face = \"bold\"),\n    legend.text     = element_text(size = 9),\n    plot.title      = element_text(size = 14, face = \"bold\"),\n    plot.subtitle   = element_text(size = 10)\n  )\n\n# Side-by-side layout\ngrid.arrange(\n  p1_Q2, p2_Q2,\n  ncol = 2,\n  top = grid::textGrob(\n    \"New Model 2 Performance in Q2: Prediction Errors vs Average Demand\",\n    gp = grid::gpar(fontsize = 14, fontface = \"bold\")\n  )\n)"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#temporal-error-predictions-1",
    "href": "assignments/assignment5/assignment5.html#temporal-error-predictions-1",
    "title": "Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Temporal Error Predictions",
    "text": "Temporal Error Predictions\n\n\nCode\n## Temporal Error Patterns ‚Äì New Model 2\ntemporal_errors_Q2_new &lt;- test_Q2 %&gt;%\n  group_by(time_of_day, weekend) %&gt;%\n  summarize(\n    MAE = mean(abs_error_new, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors_Q2_new, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period ‚Äì Q2 2025 (New Model 2)\",\n    subtitle = \"Mean absolute error across time-of-day and day type\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteFeature Engineering\n\n\n\nWe decided to add these features to better capture high demand areas in Philadelphia. As we saw from our error analysis, these cases were the most difficult to predict where our model struggled the most with the highest errors. We incorporated a week-lag to capture high temporal demand, distance to city hall to capture high spatial demand, and good weather to capture high weather-related demand.\nWe found no improvement in overall MAE to our original model 2 with temporal lags. We surmise that the reasoning lies in these variables did not offer enough marginal predictive power to improve prior model performance. Across all of our new error analysis, the new model performs relatively the same, indicating that this added complexity does not benefit model accuracy.\nWe go into more detail on our feature engineering steps in the formal report."
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#poisson-models",
    "href": "assignments/assignment5/assignment5.html#poisson-models",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Poisson Models",
    "text": "Poisson Models\n\n\nCode\n# Fit Poisson Model to M2 Baseline \nmodel2_Q2_pois_base &lt;- glm(\n  Trip_Count ~ as.factor(hour) + dotw_simple +\n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day,\n  family = poisson(link = \"log\"),\n  data = train_Q2\n)\n\nsummary(model2_Q2_pois_base)\n\n\n\nCall:\nglm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day, family = poisson(link = \"log\"), \n    data = train_Q2)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-52.064   -1.004   -0.565    0.254    8.651  \n\nCoefficients:\n                    Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept)       -2.6168507  0.0208920 -125.256  &lt; 2e-16 ***\nas.factor(hour)1  -0.4607638  0.0258514  -17.824  &lt; 2e-16 ***\nas.factor(hour)2  -0.5997450  0.0278015  -21.572  &lt; 2e-16 ***\nas.factor(hour)3  -1.3835791  0.0381521  -36.265  &lt; 2e-16 ***\nas.factor(hour)4  -1.2888566  0.0369150  -34.914  &lt; 2e-16 ***\nas.factor(hour)5  -0.1598510  0.0242414   -6.594 4.28e-11 ***\nas.factor(hour)6   0.7510022  0.0200626   37.433  &lt; 2e-16 ***\nas.factor(hour)7   1.2137872  0.0182847   66.383  &lt; 2e-16 ***\nas.factor(hour)8   1.5440347  0.0175049   88.206  &lt; 2e-16 ***\nas.factor(hour)9   1.1527966  0.0178596   64.548  &lt; 2e-16 ***\nas.factor(hour)10  1.0452447  0.0183104   57.085  &lt; 2e-16 ***\nas.factor(hour)11  1.1256447  0.0179751   62.623  &lt; 2e-16 ***\nas.factor(hour)12  1.1690927  0.0178175   65.615  &lt; 2e-16 ***\nas.factor(hour)13  1.1784647  0.0176524   66.759  &lt; 2e-16 ***\nas.factor(hour)14  1.1968011  0.0175195   68.312  &lt; 2e-16 ***\nas.factor(hour)15  1.2806756  0.0174070   73.572  &lt; 2e-16 ***\nas.factor(hour)16  1.3793236  0.0172744   79.848  &lt; 2e-16 ***\nas.factor(hour)17  1.4823364  0.0169846   87.275  &lt; 2e-16 ***\nas.factor(hour)18  1.1329340  0.0174479   64.932  &lt; 2e-16 ***\nas.factor(hour)19  1.0557576  0.0175660   60.102  &lt; 2e-16 ***\nas.factor(hour)20  0.8058963  0.0182284   44.211  &lt; 2e-16 ***\nas.factor(hour)21  0.7039571  0.0186013   37.844  &lt; 2e-16 ***\nas.factor(hour)22  0.5896291  0.0195332   30.186  &lt; 2e-16 ***\nas.factor(hour)23  0.3595681  0.0201793   17.819  &lt; 2e-16 ***\ndotw_simple2       0.0047081  0.0063916    0.737    0.461    \ndotw_simple3      -0.0585408  0.0068369   -8.562  &lt; 2e-16 ***\ndotw_simple4       0.0392889  0.0066281    5.928 3.07e-09 ***\ndotw_simple5       0.0012214  0.0064837    0.188    0.851    \ndotw_simple6      -0.0381569  0.0067981   -5.613 1.99e-08 ***\ndotw_simple7      -0.1167984  0.0070184  -16.642  &lt; 2e-16 ***\nTemperature        0.0128813  0.0002045   62.985  &lt; 2e-16 ***\nPrecipitation     -0.1986618  0.0226113   -8.786  &lt; 2e-16 ***\nlag1Hour           0.2066096  0.0008076  255.833  &lt; 2e-16 ***\nlag3Hours          0.0960102  0.0009718   98.800  &lt; 2e-16 ***\nlag1day            0.0860289  0.0009678   88.890  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 846615  on 522287  degrees of freedom\nResidual deviance: 568772  on 522253  degrees of freedom\nAIC: 968208\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\nCode\n# Poisson version of new  Model 2 (with extra features)\nmodel2_Q2_pois_new &lt;- glm(\n  Trip_Count ~ as.factor(hour) + dotw_simple +\n    Temperature + Precipitation +\n    lag1Hour + lag3Hours + lag1day + lag1week +\n    nice_weather + dist_cc,\n  family = poisson(link = \"log\"),\n  data = train_Q2\n)\n\nsummary(model2_Q2_pois_new)\n\n\n\nCall:\nglm(formula = Trip_Count ~ as.factor(hour) + dotw_simple + Temperature + \n    Precipitation + lag1Hour + lag3Hours + lag1day + lag1week + \n    nice_weather + dist_cc, family = poisson(link = \"log\"), data = train_Q2)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-31.3277   -0.8901   -0.5273    0.1359    8.8708  \n\nCoefficients:\n                    Estimate Std. Error  z value Pr(&gt;|z|)    \n(Intercept)       -2.036e+00  2.114e-02  -96.333  &lt; 2e-16 ***\nas.factor(hour)1  -4.680e-01  2.585e-02  -18.101  &lt; 2e-16 ***\nas.factor(hour)2  -6.099e-01  2.780e-02  -21.934  &lt; 2e-16 ***\nas.factor(hour)3  -1.394e+00  3.815e-02  -36.522  &lt; 2e-16 ***\nas.factor(hour)4  -1.293e+00  3.692e-02  -35.031  &lt; 2e-16 ***\nas.factor(hour)5  -1.650e-01  2.425e-02   -6.804 1.02e-11 ***\nas.factor(hour)6   7.484e-01  2.008e-02   37.277  &lt; 2e-16 ***\nas.factor(hour)7   1.210e+00  1.830e-02   66.127  &lt; 2e-16 ***\nas.factor(hour)8   1.571e+00  1.752e-02   89.670  &lt; 2e-16 ***\nas.factor(hour)9   1.187e+00  1.788e-02   66.408  &lt; 2e-16 ***\nas.factor(hour)10  1.079e+00  1.833e-02   58.869  &lt; 2e-16 ***\nas.factor(hour)11  1.158e+00  1.799e-02   64.349  &lt; 2e-16 ***\nas.factor(hour)12  1.183e+00  1.783e-02   66.339  &lt; 2e-16 ***\nas.factor(hour)13  1.178e+00  1.766e-02   66.695  &lt; 2e-16 ***\nas.factor(hour)14  1.208e+00  1.753e-02   68.918  &lt; 2e-16 ***\nas.factor(hour)15  1.300e+00  1.743e-02   74.596  &lt; 2e-16 ***\nas.factor(hour)16  1.401e+00  1.730e-02   80.966  &lt; 2e-16 ***\nas.factor(hour)17  1.520e+00  1.703e-02   89.264  &lt; 2e-16 ***\nas.factor(hour)18  1.219e+00  1.748e-02   69.777  &lt; 2e-16 ***\nas.factor(hour)19  1.111e+00  1.762e-02   63.032  &lt; 2e-16 ***\nas.factor(hour)20  8.855e-01  1.825e-02   48.536  &lt; 2e-16 ***\nas.factor(hour)21  7.437e-01  1.864e-02   39.905  &lt; 2e-16 ***\nas.factor(hour)22  6.055e-01  1.954e-02   30.996  &lt; 2e-16 ***\nas.factor(hour)23  3.660e-01  2.018e-02   18.137  &lt; 2e-16 ***\ndotw_simple2       1.159e-02  6.488e-03    1.786   0.0741 .  \ndotw_simple3      -4.552e-02  6.954e-03   -6.545 5.93e-11 ***\ndotw_simple4       3.926e-02  6.721e-03    5.842 5.15e-09 ***\ndotw_simple5       1.109e-02  6.526e-03    1.700   0.0892 .  \ndotw_simple6      -1.765e-02  6.865e-03   -2.570   0.0102 *  \ndotw_simple7      -6.415e-02  7.000e-03   -9.165  &lt; 2e-16 ***\nTemperature        1.273e-02  2.140e-04   59.482  &lt; 2e-16 ***\nPrecipitation     -1.002e-01  2.271e-02   -4.414 1.01e-05 ***\nlag1Hour           1.873e-01  8.527e-04  219.671  &lt; 2e-16 ***\nlag3Hours          6.366e-02  1.060e-03   60.064  &lt; 2e-16 ***\nlag1day            5.914e-02  1.001e-03   59.073  &lt; 2e-16 ***\nlag1week           4.675e-02  1.164e-03   40.164  &lt; 2e-16 ***\nnice_weather       7.010e-02  3.928e-03   17.847  &lt; 2e-16 ***\ndist_cc           -2.654e-04  1.447e-06 -183.410  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 846615  on 522287  degrees of freedom\nResidual deviance: 523582  on 522250  degrees of freedom\nAIC: 923024\n\nNumber of Fisher Scoring iterations: 6\n\n\n\n\nCode\n# Calculate dispersion parameter for Poisson Model 2 (Q2)\ndispersion_Q2 &lt;- sum(residuals(model2_Q2_pois_base, type = \"pearson\")^2) /\n                 model2_Q2_pois_base$df.residual\n\ncat(\"Dispersion parameter (Q2 Poisson Model 2):\", round(dispersion_Q2, 2), \"\\n\")\n\n\nDispersion parameter (Q2 Poisson Model 2): 1.39 \n\n\nCode\ncat(\"Rule of thumb: &gt;1.5 suggests overdispersion\\n\")\n\n\nRule of thumb: &gt;1.5 suggests overdispersion\n\n\nCode\nif (dispersion_Q2 &gt; 1.5) {\n  cat(\"‚ö† Overdispersion detected! Consider Negative Binomial model.\\n\")\n} else {\n  cat(\"‚úì Dispersion looks okay for Poisson model.\\n\")\n}\n\n\n‚úì Dispersion looks okay for Poisson model.\n\n\n\n\nCode\n# Make sure dotw_simple is present in test_Q2\ntest_Q2 &lt;- test_Q2 %&gt;%\n  mutate(dotw_simple = factor(dotw,\n                              levels = c(\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\")))\ncontrasts(test_Q2$dotw_simple) &lt;- contr.treatment(7)\n\n# Predictions from all four models\ntest_Q2 &lt;- test_Q2 %&gt;%\n  mutate(\n    pred2_linear_base_Q2 = predict(model2_Q2_base,      newdata = test_Q2),\n    pred2_linear_new_Q2  = predict(model2_Q2_new,       newdata = test_Q2),\n    pred2_pois_base_Q2   = predict(model2_Q2_pois_base, newdata = test_Q2, type = \"response\"),\n    pred2_pois_new_Q2    = predict(model2_Q2_pois_new,  newdata = test_Q2, type = \"response\")\n  )\n\n# MAE for each\nmae_linear_base_Q2 &lt;- mean(abs(test_Q2$Trip_Count - test_Q2$pred2_linear_base_Q2), na.rm = TRUE)\nmae_linear_new_Q2  &lt;- mean(abs(test_Q2$Trip_Count - test_Q2$pred2_linear_new_Q2),  na.rm = TRUE)\nmae_pois_base_Q2   &lt;- mean(abs(test_Q2$Trip_Count - test_Q2$pred2_pois_base_Q2),   na.rm = TRUE)\nmae_pois_new_Q2    &lt;- mean(abs(test_Q2$Trip_Count - test_Q2$pred2_pois_new_Q2),    na.rm = TRUE)\n\nmae_poisson_compare_Q2 &lt;- tibble::tibble(\n  Model = c(\n    \"Model 2 (Linear, baseline)\",\n    \"Model 2 (Linear, new features)\",\n    \"Model 2 (Poisson, baseline)\",\n    \"Model 2 (Poisson, new features)\"\n  ),\n  MAE   = c(\n    mae_linear_base_Q2,\n    mae_linear_new_Q2,\n    mae_pois_base_Q2,\n    mae_pois_new_Q2\n  )\n)\n\nkable(mae_poisson_compare_Q2,\n      digits = 3,\n      caption = \"Q2 Mean Absolute Error: Linear vs Poisson, Baseline vs New Features\",\n      col.names = c(\"Model\", \"MAE (trips)\")) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\"))\n\n\n\nQ2 Mean Absolute Error: Linear vs Poisson, Baseline vs New Features\n\n\nModel\nMAE (trips)\n\n\n\n\nModel 2 (Linear, baseline)\n0.625\n\n\nModel 2 (Linear, new features)\n0.625\n\n\nModel 2 (Poisson, baseline)\n0.691\n\n\nModel 2 (Poisson, new features)\n0.651\n\n\n\n\n\n\n\n\n\n\n\nNotePoisson Model Comparison\n\n\n\nSwitching to the Poisson model did not improve either the baseline model 2 or the new model.\nWe go into more detail on our Poisson model comparison in the formal report."
  },
  {
    "objectID": "weekly-notes/week-10-notes.html",
    "href": "weekly-notes/week-10-notes.html",
    "title": "Week 10 Notes - Space-Time Modeling: Temporal Analysis",
    "section": "",
    "text": "Panel Data: Data that follows the same units over multiple time periods.\nSpace-Time Interaction: Different patterns by location and time!\nBin (uniform time intervals) the data: Aggregate or find patterns at second-level since each observation has unique timestamp.\n\nSmaller bins = more sparse data\n\nTime features: rush hour, weekend, holidays, etc.\nTemporal lags: Past demand predicts future demand.\n\n1 hour: short term persistence\n3 hour: medium term trend (morning rush)\n12 hour: half-day cycle: am vs pm\n24 hour: Daily periodicity\n\nTrain on early months/hours etc, then test on later ones"
  },
  {
    "objectID": "weekly-notes/week-10-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-10-notes.html#key-concepts-learned",
    "title": "Week 10 Notes - Space-Time Modeling: Temporal Analysis",
    "section": "",
    "text": "Panel Data: Data that follows the same units over multiple time periods.\nSpace-Time Interaction: Different patterns by location and time!\nBin (uniform time intervals) the data: Aggregate or find patterns at second-level since each observation has unique timestamp.\n\nSmaller bins = more sparse data\n\nTime features: rush hour, weekend, holidays, etc.\nTemporal lags: Past demand predicts future demand.\n\n1 hour: short term persistence\n3 hour: medium term trend (morning rush)\n12 hour: half-day cycle: am vs pm\n24 hour: Daily periodicity\n\nTrain on early months/hours etc, then test on later ones"
  },
  {
    "objectID": "weekly-notes/week-10-notes.html#coding-techniques",
    "href": "weekly-notes/week-10-notes.html#coding-techniques",
    "title": "Week 10 Notes - Space-Time Modeling: Temporal Analysis",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nExtract time features: week, wday, hour, floor_date\nCreate lag variable: lag(var, lag_num)\nEnsure every hour bin exists: expand.grid()"
  },
  {
    "objectID": "weekly-notes/week-10-notes.html#questions-challenges",
    "href": "weekly-notes/week-10-notes.html#questions-challenges",
    "title": "Week 10 Notes - Space-Time Modeling: Temporal Analysis",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNone!"
  },
  {
    "objectID": "weekly-notes/week-10-notes.html#connections-to-policy",
    "href": "weekly-notes/week-10-notes.html#connections-to-policy",
    "title": "Week 10 Notes - Space-Time Modeling: Temporal Analysis",
    "section": "Connections to Policy",
    "text": "Connections to Policy"
  },
  {
    "objectID": "weekly-notes/week-10-notes.html#reflection",
    "href": "weekly-notes/week-10-notes.html#reflection",
    "title": "Week 10 Notes - Space-Time Modeling: Temporal Analysis",
    "section": "Reflection",
    "text": "Reflection"
  },
  {
    "objectID": "assignments/assignment5/assignment5.html#temporal-error-patterns",
    "href": "assignments/assignment5/assignment5.html#temporal-error-patterns",
    "title": "Assignment 5: Space-Time Modeling of Indego Bike Share Demand in Philadelphia",
    "section": "Temporal Error Patterns",
    "text": "Temporal Error Patterns\n\n\nCode\ntemporal_errors_Q2 &lt;- test_Q2 %&gt;%\n  group_by(time_of_day, weekend) %&gt;%\n  summarize(\n    MAE = mean(abs_error, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  mutate(day_type = ifelse(weekend == 1, \"Weekend\", \"Weekday\"))\n\nggplot(temporal_errors_Q2, aes(x = time_of_day, y = MAE, fill = day_type)) +\n  geom_col(position = \"dodge\") +\n  scale_fill_manual(values = c(\"Weekday\" = \"#08519c\", \"Weekend\" = \"#6baed6\")) +\n  labs(\n    title = \"Prediction Errors by Time Period ‚Äì Q2 2025\",\n    subtitle = \"Model 2 mean absolute error across time-of-day and day type\",\n    x = \"Time of Day\",\n    y = \"Mean Absolute Error (trips)\",\n    fill = \"Day Type\"\n  ) +\n  plotTheme +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  }
]