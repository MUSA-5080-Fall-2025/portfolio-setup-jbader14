[
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "weekly-notes/index.html",
    "href": "weekly-notes/index.html",
    "title": "Weekly Notes",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nWeek 3: EDA & ggplot2\n\n\nSep 22, 2025\n\n\n\n\n\n\nWeek 2 - Algorithmic Decision Making & Census Data\n\n\nSep 15, 2025\n\n\n\n\n\n\nWeek 1 - Course Introduction\n\n\nSep 8, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3: EDA & ggplot2",
    "section": "",
    "text": "EDA:\n\nCreate summary statistics table\n\nBut can hide underlying data distribution\n\nOutliers, non-linear relationships\n\nSo data visualization is important\nCan have same mean, var, cov, regression but diff patterns in visualization\n\nBad / Misleading data visualizations\n\nMisleading scales or axes\nCherry-picked time periods\nHidden or ignored uncertainty\nMissing context about data reliability\nResult: Poor policy decisions\n\nGoal: Understand your data before making decisions or building models\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?\n\nWorkflow w/ Data Quality Focus: Enhanced process for policy analysis\n\nLoad and inspect - dimensions, variable types, missing data\nAssess reliability - examine margins of error, calculate coefficients of variation\nVisualize distributions - histograms, boxplots for each variable\nExplore relationships - scatter plots, correlations\nIdentify patterns - grouping, clustering, geographical patterns\nQuestion anomalies - investigate outliers and unusual patterns\nDocument limitations - prepare honest communication about data quality\n\nJOINS - join predicate does not need to be same name, NEED SAME TYPE\n\nleft (most-common) / right: keeps everything from left/right\nfull: keeps everything from both columns\ninner: keeps only matches"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "title": "Week 3: EDA & ggplot2",
    "section": "",
    "text": "EDA:\n\nCreate summary statistics table\n\nBut can hide underlying data distribution\n\nOutliers, non-linear relationships\n\nSo data visualization is important\nCan have same mean, var, cov, regression but diff patterns in visualization\n\nBad / Misleading data visualizations\n\nMisleading scales or axes\nCherry-picked time periods\nHidden or ignored uncertainty\nMissing context about data reliability\nResult: Poor policy decisions\n\nGoal: Understand your data before making decisions or building models\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?\n\nWorkflow w/ Data Quality Focus: Enhanced process for policy analysis\n\nLoad and inspect - dimensions, variable types, missing data\nAssess reliability - examine margins of error, calculate coefficients of variation\nVisualize distributions - histograms, boxplots for each variable\nExplore relationships - scatter plots, correlations\nIdentify patterns - grouping, clustering, geographical patterns\nQuestion anomalies - investigate outliers and unusual patterns\nDocument limitations - prepare honest communication about data quality\n\nJOINS - join predicate does not need to be same name, NEED SAME TYPE\n\nleft (most-common) / right: keeps everything from left/right\nfull: keeps everything from both columns\ninner: keeps only matches"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03-notes.html#coding-techniques",
    "title": "Week 3: EDA & ggplot2",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nggplot2\n\nData -&gt; Aesthetics -&gt; Geometry -&gt; Visual\n\nData: Your dataset (census data, survey responses, etc.)\nAesthetics: What variables map to visual properties (x, y, color, size)\nGeometries: How to display the data (points, bars, lines)\nAdditional layers: Scales, themes, facets, annotations\nGeneral syntax: ggplot(data = your_data) + aes(x = variable1, y = variable2) + geom_something() + additional_layers()\n\nAesthetic Mappings: maps data to visual properties\n\nx, y - position\ncolor - point/line color\nfill - area fill color\nsize - point/line size\nshape - point shape\nalpha - transparency\n\nstr_detect(), str_extract(), regex() to search within strings"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3: EDA & ggplot2",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3: EDA & ggplot2",
    "section": "Connections to Policy",
    "text": "Connections to Policy"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3: EDA & ggplot2",
    "section": "Reflection",
    "text": "Reflection"
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\nAssignment 1: Census Data Quality for Policy Decisions\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\n\nName: Henry (Jack) Bader\nWhy I am taking this course: I want to deepen my understanding of geospatial data science techniques in the public sector that provides a different perspective than typical engineering data science courses.\n\n\n\n\n\nEmail: jbader14@seas.upenn.edu\nGitHub: jbader14"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Name: Henry (Jack) Bader\nWhy I am taking this course: I want to deepen my understanding of geospatial data science techniques in the public sector that provides a different perspective than typical engineering data science courses."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: jbader14@seas.upenn.edu\nGitHub: jbader14"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "",
    "text": "Clone Repo Workflow:\n\nNavigate to repository\nClick &lt;&gt; code: green button\nOpen with Github Desktop\nCopy over data, template scripts, etc. over from cloned class repo to personal repo\nOTHER METHOD: Use terminal\n\ncd ~/file location\ngit clone https://github.com/MUSA-5080-Fall-2025/MUSA-5080-Fall-2025.git\n\n\nALGORITHM: Set of rules/instructions for solving a problem or completing a task.\n\nIn Government: Systems used to assist / replace human decision-makers.\n\nBased on predictions from models that process historical data:\nInputs: features/predictors/independent variables/x\nOutputs: labels/outcome/dependent variable/y\nHelps eliminate human bias / subjectivity\nExamples: criminal justice (bail, sentencing), housing and finance (whether or not to rent), healthcare (prioritization, resource allocation)\n\nHealthcare Algorithmic Bias: Algorithm used to identify high-risk patients for additional care systematically discriminated against Black patients.\n\nAlgorithm used healthcare costs as a proxy for need / risk.\nHistorical inequity: Black patients typically incur lower costs due to systematic inequities in access.\nResult: Black patients under-prioritized despite equivalent levels of illness.\nScale: Used by hospitals and insurers for over 200 million people annually.\n\nCOMPAS Recidivism Prediction (Criminal Justice):\n\nAlgorithm twice as likely to flag Black defendants as high risk.\nHistorical arrest data reflects biased policing patterns.\n\nDutch Welfare Fraud Detection:\n\n“Black Box” secret system\nDisproportionally targeted vulnerable populations\n\n\n\n\nTerms:\n\nData Science: Computer science/engineering focus on algorithms and methods.\nData Analytics: Application of data science methods to other disciplines.\nMachine Learning: Algorithms for classification and prediction that learn from data.\nAI: Algorithms that adjust and improve across iterations (neural nets, etc.).\n\nPublic Sector context:\n\nLong history of govt data collection:\n\nCivic registration systems\nCensus data\nAdministrative records\nOperations research (post-WWII)\n\nWhat’s new?\n\nMore data (official and “accidental” such as social media data)\nFocus on prediction rather than explanation\nHarder to interpret and explain\n\nWhy govt use algorithms:\n\nGovts have limited budgets and need to serve everyone.\nAlgorithmic decision making is appealing because it promises:\nEfficiency: faster\nConsistency: established methods\nObjectivity: less human bias\nCost savings: less staff\n\n\nData Analytics is subjective:\n\nEvery step involves human choices (embedded values and biases):\n\nData cleaning\nData coding / classification\nData collection - use of imperfect proxies\nHow you interpret results\nWhat variables you use in the model\n\n\nScenario Example: Housing assistance allocation\n\nProxy: median household income\nBlind spots:\n\nLower income communities: median is a central measure.\nHigh rent/demand communities (NYC): median is a central measure.\n\nHarm + Fixes:\n\nUpper bound for hh income (well under city median income) to provide more housing assistance.\nNeighborhood housing price scale (divide income by housing price) to see how well off a hh is compared to the relative neighborhood.\nGuardrail:\n\nRent control\nHousing stipends / upper bounded interest rates\n\n\n\nCENSUS:\n\nFoundation for:\n\nUnderstanding community demographics\nAllocating government resources\nTracking neighborhood change\nDesigning fair algorithms (like those we just discussed)\n\nDecennial Census (2020)\n\nEveryone counted every 10 years (full population)\n9 basic questions: age, race, sex, housing\nConstitutional requirement\nDetermines political representation\n\n\nAmerican Community Survey (ACS):\n\nWhat it is:\n\n3% of households surveyed annually\nDetailed questions: income, education, employment, housing costs\nReplaced old “long form” in 2005\n\n1-Year estimates (areas &gt; 65K people)\n\nMost current data, smallest sample\nTake with grain of salt (only aggregate levels, not neighborhood)\n\n5-Year estimates (all areas including census tracts)\n\nMost reliable data, largest sample\nKey point: All ACS data comes with margins of error (since samples) - uncertainty\n\n\nCensus Geography Hierarchy:\n\nCounty level: state and regional planning\nCensus tract level: neighborhood analysis (1500 - 8000 people)\nBlock group level: very local analysis (huge MOEs) (600 - 3000)\n\nBlocks: decennial only, 85 people\n\n\n2020 Census Innovation: Differential Privacy\n\nChallenge: Modern computing can re-identify individuals from census data.\nSolution: Add mathematical noise to protect privacy while preserving overall patterns.\nControversy: Some places now show impossible results in populations (ex. living underwater).\nWhy this matters: Even objective data involves subjective choices about privacy vs. accuracy (can cause errors).\n\nMargin of Error (MOE)\n\nLarge MOE relative to estimate = less reliable\nIn analysis:\n\nAlways report MOE alongside estimates.\nBe cautious comparing estimates with overlapping error margins.\nConsider using 5-year estimates for greater reliability\n\nDate intervals can be affected by new advancements during that time.\n\n\n\nTwo Types of Census Data:\n\nSummary Tables (what we’ll use mostly)\n\nPre-calculated statistics by geography\nMedian income, percent college-educated, etc.\nGood for: Mapping, comparing places\n\nPUMS: Individual Records\n\nAnonymous individual/household responses\nGood for: Custom analysis, regression models\nMore complex but more flexible\n\n\nData Sources you’ll use:\n\nTIGER/Line Files\n\nGeographic boundaries (shapefiles)\nCensus tracts, counties, states\nNow released as shapefiles (easier to use!)\n\nHistorical Data Sources:\n\nNHGIS (nhgis.org): Historical census data\nNeighborhood Change Database\nLongitudinal Tract Database: Track changes over time’"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "",
    "text": "Clone Repo Workflow:\n\nNavigate to repository\nClick &lt;&gt; code: green button\nOpen with Github Desktop\nCopy over data, template scripts, etc. over from cloned class repo to personal repo\nOTHER METHOD: Use terminal\n\ncd ~/file location\ngit clone https://github.com/MUSA-5080-Fall-2025/MUSA-5080-Fall-2025.git\n\n\nALGORITHM: Set of rules/instructions for solving a problem or completing a task.\n\nIn Government: Systems used to assist / replace human decision-makers.\n\nBased on predictions from models that process historical data:\nInputs: features/predictors/independent variables/x\nOutputs: labels/outcome/dependent variable/y\nHelps eliminate human bias / subjectivity\nExamples: criminal justice (bail, sentencing), housing and finance (whether or not to rent), healthcare (prioritization, resource allocation)\n\nHealthcare Algorithmic Bias: Algorithm used to identify high-risk patients for additional care systematically discriminated against Black patients.\n\nAlgorithm used healthcare costs as a proxy for need / risk.\nHistorical inequity: Black patients typically incur lower costs due to systematic inequities in access.\nResult: Black patients under-prioritized despite equivalent levels of illness.\nScale: Used by hospitals and insurers for over 200 million people annually.\n\nCOMPAS Recidivism Prediction (Criminal Justice):\n\nAlgorithm twice as likely to flag Black defendants as high risk.\nHistorical arrest data reflects biased policing patterns.\n\nDutch Welfare Fraud Detection:\n\n“Black Box” secret system\nDisproportionally targeted vulnerable populations\n\n\n\n\nTerms:\n\nData Science: Computer science/engineering focus on algorithms and methods.\nData Analytics: Application of data science methods to other disciplines.\nMachine Learning: Algorithms for classification and prediction that learn from data.\nAI: Algorithms that adjust and improve across iterations (neural nets, etc.).\n\nPublic Sector context:\n\nLong history of govt data collection:\n\nCivic registration systems\nCensus data\nAdministrative records\nOperations research (post-WWII)\n\nWhat’s new?\n\nMore data (official and “accidental” such as social media data)\nFocus on prediction rather than explanation\nHarder to interpret and explain\n\nWhy govt use algorithms:\n\nGovts have limited budgets and need to serve everyone.\nAlgorithmic decision making is appealing because it promises:\nEfficiency: faster\nConsistency: established methods\nObjectivity: less human bias\nCost savings: less staff\n\n\nData Analytics is subjective:\n\nEvery step involves human choices (embedded values and biases):\n\nData cleaning\nData coding / classification\nData collection - use of imperfect proxies\nHow you interpret results\nWhat variables you use in the model\n\n\nScenario Example: Housing assistance allocation\n\nProxy: median household income\nBlind spots:\n\nLower income communities: median is a central measure.\nHigh rent/demand communities (NYC): median is a central measure.\n\nHarm + Fixes:\n\nUpper bound for hh income (well under city median income) to provide more housing assistance.\nNeighborhood housing price scale (divide income by housing price) to see how well off a hh is compared to the relative neighborhood.\nGuardrail:\n\nRent control\nHousing stipends / upper bounded interest rates\n\n\n\nCENSUS:\n\nFoundation for:\n\nUnderstanding community demographics\nAllocating government resources\nTracking neighborhood change\nDesigning fair algorithms (like those we just discussed)\n\nDecennial Census (2020)\n\nEveryone counted every 10 years (full population)\n9 basic questions: age, race, sex, housing\nConstitutional requirement\nDetermines political representation\n\n\nAmerican Community Survey (ACS):\n\nWhat it is:\n\n3% of households surveyed annually\nDetailed questions: income, education, employment, housing costs\nReplaced old “long form” in 2005\n\n1-Year estimates (areas &gt; 65K people)\n\nMost current data, smallest sample\nTake with grain of salt (only aggregate levels, not neighborhood)\n\n5-Year estimates (all areas including census tracts)\n\nMost reliable data, largest sample\nKey point: All ACS data comes with margins of error (since samples) - uncertainty\n\n\nCensus Geography Hierarchy:\n\nCounty level: state and regional planning\nCensus tract level: neighborhood analysis (1500 - 8000 people)\nBlock group level: very local analysis (huge MOEs) (600 - 3000)\n\nBlocks: decennial only, 85 people\n\n\n2020 Census Innovation: Differential Privacy\n\nChallenge: Modern computing can re-identify individuals from census data.\nSolution: Add mathematical noise to protect privacy while preserving overall patterns.\nControversy: Some places now show impossible results in populations (ex. living underwater).\nWhy this matters: Even objective data involves subjective choices about privacy vs. accuracy (can cause errors).\n\nMargin of Error (MOE)\n\nLarge MOE relative to estimate = less reliable\nIn analysis:\n\nAlways report MOE alongside estimates.\nBe cautious comparing estimates with overlapping error margins.\nConsider using 5-year estimates for greater reliability\n\nDate intervals can be affected by new advancements during that time.\n\n\n\nTwo Types of Census Data:\n\nSummary Tables (what we’ll use mostly)\n\nPre-calculated statistics by geography\nMedian income, percent college-educated, etc.\nGood for: Mapping, comparing places\n\nPUMS: Individual Records\n\nAnonymous individual/household responses\nGood for: Custom analysis, regression models\nMore complex but more flexible\n\n\nData Sources you’ll use:\n\nTIGER/Line Files\n\nGeographic boundaries (shapefiles)\nCensus tracts, counties, states\nNow released as shapefiles (easier to use!)\n\nHistorical Data Sources:\n\nNHGIS (nhgis.org): Historical census data\nNeighborhood Change Database\nLongitudinal Tract Database: Track changes over time’"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nnew dplyr functions:\n\nglimpse(): brief overview of df (num rows/cols, col name/type, some row examples)\ncolnames(): column names of df\nbetween(x, lower bound, upper bound): use w/ case_when + mutate\nfilter(): Joint string conditions\n\nfilter(Manufacturer %in% c(“Honda”, “Nissan”))\n\nPiping: |&gt; chain together dplyr commands on a single dataframe\n\nAccessing Census Data in R:\n\nModern approach: use R packages to access data directly.\n\nAlways get latest data\nReproducible workflows\nAutomatic geographic boundaries\nBuilt-in error handling\n\nUse tidycensus package\nTable organization:\n\nB19013: Median Household Income\nB25003: Housing Tenure (Own/Rent)\nB15003: Educational Attainment\nB08301: Commuting to Work\n\nVariable Examples: E (Estimate), M (Margin of Error)\n\nB19013_001E = Median household income (estimate)\nB19013_001M = Median household income (margin of error)\n\n\nSAMPLE CODE:\n\ncensus_api_key(): access data\nget_acs(geography, variables, year, state, survey, output): get the data into file\nstr_remove(var, str_to_remove): remove substrings of original string in column"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nThe examples provided context to how we use data science skills and techniques to apply in a real-world setting.\nThe conversations around algorithmic biases and biased data is important to consider when making policy considerations. It is extremely important to understand our data and recognize flaws in algorithms that can perpetuate bias, which usually harm marginalized groups of people.\nThe Census data provides valuable information (ACS) for us to analyze with data-driven methods in formulating policies that solve problems in bettering communities."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "Reflection",
    "text": "Reflection\n\nI had not had a formal introduction to Census data before. Sometimes, working with assigned datasets in class overlook a key aspect being data collection, which is a key decision in framing an analytics problem with human choice.\nI think that it is very valuable to go over biases in algorithms and data. One saying that I was taught is “Data is never neutral”, highlighting that there are many subjective human decisions that go into creating data-driven solutions. It is crucial that we recognize these risks in our problem-solving process to mitigate risk of biases in our policy decisions to protect at-risk groups of people."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 - Course Introduction",
    "section": "",
    "text": "Upcoming assignments: lab0: 9/15 create Quatro portfolio; lab1: 9/27 EDA\nGit: Version-control system - tracks changes in files\n\nCollaboration tool for people teams to work\n\nGithub: cloud host for Git repos (projects)\n\nBackup work in cloud\nShare / collaborate on projects with other\nGithub pages hosts websites\n\nGithub Concepts:\n\nRepo: folder containing proj files\nCommit: snapshot of work at point in time\nPush: Send changes to Github cloud\nPull: Get latest changes from Github cloud\n\nWorkflow each week:\n\nEdit files in RStudio (local)\nCommit changes w/ message\nPush to Github\nPortfolio propogates changes automatically\n\nGithub Classroom:\n\nCreates individual repos for each student\nDistributes assignments automatically\nTAs can provide feedback\n\nLists syntax\nLinks / Images syntax"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 - Course Introduction",
    "section": "",
    "text": "Upcoming assignments: lab0: 9/15 create Quatro portfolio; lab1: 9/27 EDA\nGit: Version-control system - tracks changes in files\n\nCollaboration tool for people teams to work\n\nGithub: cloud host for Git repos (projects)\n\nBackup work in cloud\nShare / collaborate on projects with other\nGithub pages hosts websites\n\nGithub Concepts:\n\nRepo: folder containing proj files\nCommit: snapshot of work at point in time\nPush: Send changes to Github cloud\nPull: Get latest changes from Github cloud\n\nWorkflow each week:\n\nEdit files in RStudio (local)\nCommit changes w/ message\nPush to Github\nPortfolio propogates changes automatically\n\nGithub Classroom:\n\nCreates individual repos for each student\nDistributes assignments automatically\nTAs can provide feedback\n\nLists syntax\nLinks / Images syntax"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nR (open software - people create packages free to use) and dplyr:\n\ntidyverse: Data Science packages\n\nConsistent syntax across functions\nReadable code tells story\nEfficient workflows for common tasks\n\nTibbles: enhanced data frames\n\nTraditional df: class(data)\nTibble: car_data &lt;- as_tibble(data)\nPretty print: Shows ONLY first 10 rows, displays cleaner than traditional df\nread_csv (read.csv for traditional df)\n\nEssential dplyr functions (data cleaning):\nselect(): choose columns - select(df, col1, col2)\nfilter(): choose rows - filter(df, condition)\n\nUse &, | symbol to join conditions (and/or)\n\nmutate(): create new variables - mutate(df, new_col = old_col / 1000)\n\nCan use case_when (if statement) argument: ex) car_df = mutate(car_df, case_when(age &gt; 20 ~ “old”, TRUE ~ “not old”))\n\nsummarize(): calculate stats\ngroup_by(): operate on groups (usually goes together w/ summarize())\n\ngroup_by(col1) |&gt; summarize (n = n()) |&gt; mutate(freq = n / sum(n)) -&gt; calculates proportions (frequencies) for groups\n\nMust assign back to original or new df name to save the operation (always start with dataframe operating on in syntax)\nOthers:\n\nnames(): column names\nglimpse(): rows, columnns, some entries etc.\nClick on df to view in table format\nRename(): renames column: rename(df, new = old)\nRemove a column: select(df, -col)\n\n%&gt;% or |&gt;: Pipes -&gt; combines lines of codes, pass outputs through each pipe\n\nQuarto (better version of R Markdown) - publishing system that combines:\n\nCode (R, Python, etc.), Text (explanations, analysis, etc.), Output (plots, tables results, etc.)\nBenefits:\n\nReproducible research: Ccde and explanation in one place, others can re-run analysis, professional presentation\n\nComponents:\n\nYAML header: title, author, date, format\nR code chunks: load library, read_csv, etc.\nText (formatting: bold, italic, etc.)"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nWe learned how to clean data, transforming it to drive actionable insights. Datasets can be messy, and by learning these skills, we build foundational knowledge on how we can improve the quality of data for future analyses.\nWe learned how to create a Quarto portfolio as a professional way to organize and present our work. This is highly valuable for making our findings professional, clean, and easily accessible to peers, future employers, etc."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nI found the Quarto portfolio the most fascinating part of this lecture. It displays our work in a much cleaner format than in R-Markdown.\nI will be using this portfolio to highlight the work in this course. Additionally, I can use skills in making a Quatro portfolio for personal projects as well!"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html",
    "href": "assignments/assignment1/assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the NJ Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#scenario",
    "href": "assignments/assignment1/assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the NJ Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#learning-objectives",
    "href": "assignments/assignment1/assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#submission-instructions",
    "href": "assignments/assignment1/assignment1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#data-retrieval",
    "href": "assignments/assignment1/assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\nnj_inc_pop &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n        median_income = \"B19013_001\",\n        total_pop = \"B01003_001\"\n  ),\n  state = my_state,\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\nnj_inc_pop &lt;- nj_inc_pop |&gt;\n  # Remove state name and \"County\"\n  mutate(\n    county_name = str_remove(NAME, \", New Jersey\"),\n    county_name = str_remove(county_name, \" County\")\n  ) |&gt;\n  # Drop NAME column\n  select(-NAME)\n\n# Display the first few rows\nhead(nj_inc_pop, 5)\n\n# A tibble: 5 × 6\n  GEOID median_incomeE median_incomeM total_popE total_popM county_name\n  &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      \n1 34001          73113           1917     274339         NA Atlantic   \n2 34003         118714           1607     953243         NA Bergen     \n3 34005         102615           1436     461853         NA Burlington \n4 34007          82005           1414     522581         NA Camden     \n5 34009          83870           3707      95456         NA Cape May"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#data-quality-assessment",
    "href": "assignments/assignment1/assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\nnj_reli &lt;- nj_inc_pop |&gt;\n  mutate(\n    # Compute MOE percentage\n    moe_percent = round((median_incomeM / median_incomeE) * 100, 2),\n    # Create reliability categories\n    reliability = case_when(\n      moe_percent &lt; 5 ~ \"High Confidence\",\n      moe_percent &gt;= 5 & moe_percent &lt;= 10 ~ \"Moderate\",\n      moe_percent &gt; 10 ~ \"Low Confidence\"\n    ),\n    # Flag when MOE is greater than 10%\n    moe_flag = moe_percent &gt; 10\n  )\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\nnj_reli_summary &lt;- nj_reli |&gt;\n  # Count the number of counties per reliability category\n  count(reliability, name = \"n\") |&gt;\n  # Add percentages - count(county) / count(all)\n  mutate(\n  reliability_percent = round(100 * n / sum(n), 1),\n  reliability_percent = paste0(reliability_percent, \"%\")\n  ) \n\n# Display summary table\nkable(nj_reli_summary,\n    caption = \"County Reliability Summary\",\n    col.names = c(\"Reliability\", \"Count\", \"Percent\"))\n\n\nCounty Reliability Summary\n\n\nReliability\nCount\nPercent\n\n\n\n\nHigh Confidence\n21\n100%"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#high-uncertainty-counties",
    "href": "assignments/assignment1/assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\nnj_top_moe &lt;- nj_reli |&gt;\n  # Sort by DESC MOE percent\n  arrange(desc(moe_percent)) |&gt;\n  # Select top 5 counties\n  slice_head(n = 5) |&gt;\n  # Select columns\n  select(county_name, median_incomeE, median_incomeM, moe_percent, reliability)\n\n# Format as table with kable() - include appropriate column names and caption\nkable(nj_top_moe,\n      col.names = c(\"County\", \"Median Income\", \"MOE\", \"MOE %\", \"Reliabilty\"),\n      caption = \"NJ Counties with the Highest Income Data Uncertainty\",\n      format.args = list(big.mark = \",\"))\n\n\nNJ Counties with the Highest Income Data Uncertainty\n\n\nCounty\nMedian Income\nMOE\nMOE %\nReliabilty\n\n\n\n\nCape May\n83,870\n3,707\n4.42\nHigh Confidence\n\n\nSalem\n73,378\n3,047\n4.15\nHigh Confidence\n\n\nCumberland\n62,310\n2,205\n3.54\nHigh Confidence\n\n\nAtlantic\n73,113\n1,917\n2.62\nHigh Confidence\n\n\nGloucester\n99,668\n2,605\n2.61\nHigh Confidence\n\n\n\n\n\nData Quality Commentary:\nIn New Jersey, all counties have a median income MOE % below our threshold of 5%, indicating that the reporting of income date is highly reliable based on our standards. Even though these fall within high confidence, we should still proceed with caution when using this data for algorithmic decision-making. A pattern amongst the top highest MOE % revealed that all five counties make up the most southern counties in New Jersey.\nFor Cape May and Atlantic counties, a potential cause for higher MOE % stems from their economies being highly reliant on tourism for many residents. Especially, when you consider their location at the Jersey Shore, the flow of tourists is highly dependent on the season (notably, Summer), which can lead to further uncertainty in income reporting.\nCumberland, Salem, and Atlantic counties represent the three lowest counties in median income from this data. It may be the case that these counties may exhibit larger margins of error due to more residents working lower paying, hourly jobs that skew the reporting of median income, leading to higher MOE %."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#focus-area-selection",
    "href": "assignments/assignment1/assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\nNote from Jack: All counties in NJ have high confidence levels. I checked with Prof Delmelle, and she said that it was fine to continue with my analysis for NJ. I chose Cape May (MOE = 4.42%, Beach), Essex (2.00%. North Jersey), and Burlington (MOE = 1.40%, South Jersey / Philly) counties for this section to get a diverse selection of MOE percentages and geographical locations.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\nselected_countries &lt;- nj_reli |&gt;\n  filter(county_name %in% c(\"Cape May\", \"Essex\", \"Burlington\")) |&gt;\n  # Select county name, median income, MOE percentage, reliability category\n  select(county_name, median_incomeE, moe_percent, reliability)\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nkable(selected_countries,\n      caption = \"Selected Counties\",\n      col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\"))\n\n\nSelected Counties\n\n\nCounty\nMedian Income\nMOE %\nReliability\n\n\n\n\nBurlington\n102615\n1.40\nHigh Confidence\n\n\nCape May\n83870\n4.42\nHigh Confidence\n\n\nEssex\n73785\n2.00\nHigh Confidence\n\n\n\n\n\nComment on the output: Burlington county has the highest median income with the lowest MOE %, indicating that it is the safest out of the three for algorithmic decision-making. As stated previously, Cape May has a high MOE %, relative to the rest of NJ, due to its tourist driven economy and volatility in income during a calendar year. Essex county has a moderately low MOE % and median income, revealing that lower income communities may not always have higher MOE %. Although, Essex could be an outlier."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#tract-level-demographics",
    "href": "assignments/assignment1/assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\nwhite &lt;- \"B03002_003\"\nblack &lt;- \"B03002_004\"\nhispanic &lt;- \"B03002_012\"\ntotal_pop &lt;- \"B03002_001\"\n\n# Set county codes (GEOID): Burlington, Cape May, Essex\n# Remove \"34\" from start of GEOID code\ngeoid_codes &lt;- c(\"005\", \"009\", \"013\")\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\nnj_tract &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n        white = white,\n        black = black,\n        hispanic = hispanic,\n        total_pop = total_pop\n  ),\n  state = my_state, \n  county = geoid_codes,\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n) \n\n# Calculate percentage of each group using mutate()\nnj_tract &lt;- nj_tract |&gt;\n  # Create percentages for white, Black, and Hispanic populations\n  mutate(\n    white_percent = round(100 * whiteE / total_popE, 2),\n    black_percent = round(100 * blackE / total_popE, 2),\n    hispanic_percent = round(100 * hispanicE / total_popE, 2)) |&gt;\n    # Add readable tract and county name columns using str_extract() or similar                \n    mutate(\n      # Tract (RegEx)\n      tract = str_remove(str_extract(NAME, \"Census Tract\\\\s*[0-9]+(?:\\\\.[0-9]+)?\"),\"^Census Tract\\\\s*\"),\n\n      # County (RegEx)\n      county = str_extract(NAME, \";\\\\s*[^;]+\\\\s*;\") |&gt;\n               str_remove(\"^;\\\\s*\") |&gt;\n               str_remove(\"\\\\s*;$\") |&gt;\n               str_trim(),\n      \n      county = str_remove(county, \"\\\\s+County\\\\b\") |&gt;\n               str_trim()\n  ) \n\n# head(nj_tract)"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#demographic-analysis",
    "href": "assignments/assignment1/assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\nhigh_hispanic &lt;- nj_tract |&gt;\n  arrange(desc(hispanic_percent)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(county, tract, hispanic_percent)\n\nkable(high_hispanic,\n    caption = \"Highest Percentage of Hispanic Residents\",\n    col.names = c(\"County\", \"Tract\", \"Hispanic %\"))\n\n\nHighest Percentage of Hispanic Residents\n\n\nCounty\nTract\nHispanic %\n\n\n\n\nEssex\n97\n89.51\n\n\n\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\navg_demo &lt;- nj_tract |&gt;\n  group_by(county) |&gt;\n  summarize(\n    num_tracts = n(),\n    avg_white = round(mean(white_percent, na.rm = TRUE), 2),\n    avg_black = round(mean(black_percent, na.rm = TRUE), 2),\n    avg_hispanic = round(mean(hispanic_percent, na.rm = TRUE), 2)\n  )\n\n# Create a nicely formatted table of your results using kable()\nkable(avg_demo,\n      col.names = c(\"County\", \"Number of Tracts\", \"Average White %\", \"Average Black %\", \"Average Hispanic %\"),\n      caption = \"Average Demographics for Burlington, Cape May, and Essex Counties (NJ)\",\n      format.args = list(big.mark = \",\"))\n\n\nAverage Demographics for Burlington, Cape May, and Essex Counties (NJ)\n\n\n\n\n\n\n\n\n\nCounty\nNumber of Tracts\nAverage White %\nAverage Black %\nAverage Hispanic %\n\n\n\n\nBurlington\n117\n62.77\n17.41\n9.79\n\n\nCape May\n33\n86.01\n2.83\n7.66\n\n\nEssex\n211\n24.98\n41.78\n23.42"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\nnj_tract &lt;- nj_tract |&gt;\n  mutate(\n    # Calculate MOE percentages for white, Black, and Hispanic variables\n    # Hint: use the same formula as before (margin/estimate * 100)\n    # Need if/else some tracts show 0 population for a demographic\n    white_moe_percent = if_else(whiteE &gt; 0, round(100 * whiteM / whiteE, 2), NA_real_),\n    black_moe_percent = if_else(blackE &gt; 0, round((blackM / blackE) * 100, 2), NA_real_),\n    hispanic_moe_percent = if_else(hispanicE &gt; 0, round((whiteM / whiteE) * 100, 2), NA_real_)\n  )\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\nflag_tracts &lt;- nj_tract |&gt;\n   mutate(\n     flag_moe = ifelse(\n      coalesce(white_moe_percent &gt; 10, FALSE) |\n      coalesce(black_moe_percent &gt; 10, FALSE) |\n      coalesce(hispanic_moe_percent &gt; 10, FALSE),\n      \"High MOE\", \"Low MOE\"\n     )\n   )\n\n# Create summary statistics showing how many tracts have data quality issues\nflag_tracts_summary &lt;- flag_tracts |&gt;\n  summarize(\n    total_tracts = n(),\n    flagged_tracts = sum(flag_moe == \"High MOE\", na.rm = TRUE),\n    flagged_percents = round(100 * flagged_tracts / total_tracts, 2)\n  )\n\nkable(flag_tracts_summary, \n      col.names = c(\"Total Tracts\", \"Flagged Tracts\", \"Flagged %\"),\n      caption = \"Summary of Tracts in Burlington, Cape May, Essex counties with High MOE (Any Demo &gt; 15%)\",\n      format.args = list(big.mark = \",\"))\n\n\nSummary of Tracts in Burlington, Cape May, Essex counties with High MOE (Any Demo &gt; 15%)\n\n\nTotal Tracts\nFlagged Tracts\nFlagged %\n\n\n\n\n361\n355\n98.34"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#pattern-analysis",
    "href": "assignments/assignment1/assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Calculate population percentages for each tract\nflag_tracts &lt;- flag_tracts %&gt;%\n  mutate(\n    white_pop_percent = if_else(total_popE &gt; 0, round(100 * whiteE / total_popE, 2), NA_real_),\n    black_pop_percent = if_else(total_popE &gt; 0, round(100 * blackE / total_popE, 2), NA_real_),\n    hispanic_pop_percent = if_else(total_popE &gt; 0, round(100 * hispanicE / total_popE, 2), NA_real_)\n  )\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\nflag_tracts_group &lt;- flag_tracts |&gt;\n  # Group by MOE threshold\n  group_by(flag_moe) |&gt;\n  summarize(\n    # Total number of tracts\n    num_tracts = n(),\n    \n    # Avg population sizes for each demographic group\n    avg_white_pop = round(mean(whiteE, na.rm = TRUE), 2),\n    avg_black_pop = round(mean(blackE, na.rm = TRUE), 2),\n    avg_hispanic_pop = round(mean(hispanicE, na.rm = TRUE), 2),\n    \n    # Avg Population percentage for each demographic group\n    avg_white_pop_percent = round(mean(white_pop_percent, na.rm = TRUE), 2),\n    avg_black_pop_percent = round(mean(black_pop_percent, na.rm = TRUE), 2),\n    avg_hispanic_pop_percent = round(mean(hispanic_pop_percent, na.rm = TRUE), 2),\n  )\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\nkable(\n  flag_tracts_group,\n  col.names = c(\n    \"MOE Flag Indicator\",\n    \"Number of Tracts\",\n    \"Avg White Population\", \"Avg Black Population\", \"Avg Hispanic Population\",\n    \"Avg White Pop %\", \"Avg Black Pop %\", \"Avg Hispanic Pop %\"\n  ),\n  caption = \"Tract Demographic Statistics Based on MOE Flag Indicator\",\n  format.args = list(big.mark = \",\")\n)\n\n\nTract Demographic Statistics Based on MOE Flag Indicator\n\n\n\n\n\n\n\n\n\n\n\n\nMOE Flag Indicator\nNumber of Tracts\nAvg White Population\nAvg Black Population\nAvg Hispanic Population\nAvg White Pop %\nAvg Black Pop %\nAvg Hispanic Pop %\n\n\n\n\nHigh MOE\n355\n1,713.62\n1,096.94\n713.41\n42.21\n30.71\n17.72\n\n\nLow MOE\n6\n2,383.67\n0.00\n130.17\n88.89\n0.00\n4.86\n\n\n\n\n\nPattern Analysis:\nOne pattern is that an overwhelming majority of tracts are flagged with high MOE (for at least one demographic) of around 98.34%. There are a few reasons why we observe this. For one, tracts are much smaller than counties and when we have a smaller sample size, this can lead to more variation, hence less reliability (higher margins of error throughout the data). Secondly, since we set our MOE indicator to be classified as “High MOE” if one or more demographic had a higher MOE than 10%. As a result, this design decision leads to more tracts being categorized as “High MOE” than if we made a stricter boundary of all three demographics must have a higher MOE than 10%. Lastly, the tract data was much sparser than the county level data with many demographic estimates and margins being zero for certain tracks, which could possibly be a reporting error.\nFrom the clear imbalance of our high and low MOE classes for the tracts in our selected counties, we should approach drawing conclusions about demographics and reliability with caution. Nevertheless, it seems that our six “Low MOE” tracts are much less diverse and predominantly White (88.89% compared to 42.21%). It is possible that these communities may have better access to reporting their statistics or are potentially more well informed about the Census and obligation to report. But, it is important to question the oddity that there average Black population is 0% across these tracts. This leads me to believe that this is due to the sparsity issue in the data itself. If the data showed that there are no Black residents (and very little Hispanic residents), then these tracts theoretically would only have to worry about passing the MOE threshold for the White demographic. It is important to consider all possibilities when trying to analyze and uncover patterns within data."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements:\n\nOverall Pattern Identification: What are the systematic patterns across all your analyses?\nEquity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings?\nRoot Cause Analysis: What underlying factors drive both data quality issues and bias risk?\nStrategic Recommendations: What should the Department implement to address these systematic issues?\n\nExecutive Summary:\nOverall Pattern Identification: Three significant systematic patterns arose across our analyses. The first pattern was that there was a drastic decline in data reliability when moving from county to tract-level data. In our county analysis, all counties were classified as “high confidence” in MOE % for median income data. However, in our tract data for selected counties, less than 2% of tracts were classified as “high confidence” for demographic data. The second pattern came from our tract-level analysis where we saw that more diverse tracts were much less reliable in terms of collective demographic MOE %. The small percentage of “high confidence” tracts were predominantly white (88.89% compared to 42.21%). The third trend was revealed in geographic location in county analysis. Specifically, the highest MOE % of income data mainly resided in South Jersey, revealing clusters of uncertainty among these counties.\nEquity Assessment: Diverse communities with higher population percentages of people of color (Black, Hispanic) are most at risk of algorithmic biases. In our tract-level analysis, we saw more confident aggregate results among communities that were predominantly White rather than diverse. In addition, a majority of the missing data in these tracts came from the Black and Hispanic population estimates and margins. These discrepancies could cause further inequity in these diverse communities when using this tract-level data without caution in our algorithmic decision-making.\nRoot Cause Analysis:\n\nCounties with more variable income flow such as Cape May or Atlantic are likely to have less reliable survey responses, possibly skewing our data depending on when they are surveyed. For example, imagine an ice cream salesman at the Beach is asked about his income in July vs. December.\nThe effect of a smaller sample size in our tract data compared to county data is the main cause of less reliability in our MOE estimates. Smaller sample sizes leads to fewer survey responses, which leads to greater variability causing more uncertainty.\nThe effect of sparse or missing data at the tract-level is another potential cause for biased estimates in the reliability of our tract analysis. Since most of the “low MOE” tracts had missing demographic data for either the Black or Hispanic demographics, this made it easier for these tracts to pass our threshold test.\n\nStrategic Recommendations:\n\nReliability Thresholds: We should establish more realistic reliability thresholds at both the county and tract level. It is evident that our current decision boundary is not insightful with all counties having high reliability and 98% of tracts having low reliability. We should tighten the threshold for county data in New Jersey. For tract data, it would be valuable to split up reliability by race rather than testing if at least one race fails the threshold. It is also important to highlight which counties have missing data and exclude them from this analysis or state their omission.\nImproved Efforts in Surveying Diverse Tracts & Country Clusters: We saw that diverse tracts had less reliability than predominantly White tracts. Policymakers should make a stronger effort at reaching these communities to ensure that we are obtaining accurate data to prevent drawing biased conclusions about these demographic groups without proper representation. Additionally, we saw that there were clusters of counties that showed higher uncertainty than the rest of the state. It would be beneficial to address these clusters by promoting survey participation to aid policymakers.\nHigher Frequency Monitoring: For immediate policy considerations, it would be helpful to have more frequent data collection such as every three months or even every month if possible. This would be very beneficial to counties like Cape May or Atlantic but others as well. It would identify problems in communities much faster and allow for more actionable, targeted intervention in policy making."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#specific-recommendations",
    "href": "assignments/assignment1/assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\nfinal_df &lt;- nj_reli |&gt;\n  # Add a new column with algorithm recommendations using case_when():\n  # - High Confidence: \"Safe for algorithmic decisions\"\n  # - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n  # - Low Confidence: \"Requires manual review or additional data\"\n  mutate(\n    reli_cat = case_when(\n      reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  ) |&gt;\n  # Sort by MOE percent (DESC)\n  arrange(desc(moe_percent)) |&gt;\n  # Select relevant columns to include\n  select(county_name, median_incomeE, moe_percent, reliability, reli_cat) |&gt;\n  # Add $ and % symbols to median_incomeE and moe_percent\n  mutate(\n    median_incomeE = dollar(median_incomeE),\n    moe_percent = paste0(number(moe_percent, accuracy = 0.01), \"%\")\n  )\n\n# Format as a professional table with kable()\nkable(\n  final_df,\n  col.names = c(\"County\", \"Median Income\", \"MOE Percent\", \"Reliability\", \"Recommendation\"),\n  caption = \"Median Income Decision Framework for NJ Counties\",\n  format.args = list(big.mark = \",\")\n)\n\n\nMedian Income Decision Framework for NJ Counties\n\n\n\n\n\n\n\n\n\nCounty\nMedian Income\nMOE Percent\nReliability\nRecommendation\n\n\n\n\nCape May\n$83,870\n4.42%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSalem\n$73,378\n4.15%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCumberland\n$62,310\n3.54%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nAtlantic\n$73,113\n2.62%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nGloucester\n$99,668\n2.61%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWarren\n$92,620\n2.60%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSomerset\n$131,948\n2.49%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSussex\n$111,094\n2.46%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHunterdon\n$133,534\n2.42%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMercer\n$92,697\n2.33%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nUnion\n$95,000\n2.33%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMorris\n$130,808\n2.08%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHudson\n$86,854\n2.05%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nEssex\n$73,785\n2.00%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPassaic\n$84,465\n1.85%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOcean\n$82,379\n1.77%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCamden\n$82,005\n1.72%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMonmouth\n$118,527\n1.61%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMiddlesex\n$105,206\n1.46%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBurlington\n$102,615\n1.40%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBergen\n$118,714\n1.35%\nHigh Confidence\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: All counties in NJ are considered suitable for algorithmic implementation. They all have MOE less than our 5% threshold, making them high confidence for algorithmic decision making. However, if we wanted to examine tracts in individual counties, based on our tract-level analysis of three NJ counties, our data is much less reliable to use. Therefore, we should proceed with more caution for any kind of tract-level algorithmic implementation.\nCounties requiring additional oversight: No counties were classified as moderate confidence in NJ. However, I would say that counties near the bound of the 5% threshold like Cape May and Salem could be more closely monitored given their relatively high MOE above 4%. Additionally, as expressed earlier, counties such as Cape May and Atlantic that are highly reliant on tourism in generating income for its residents could use some supplementary data collection for analysis. Since income flow is extremely time-dependent, we should look to monitor the changes in income related to each season (or month) to get a better understanding of the income data. This would lead to more consistent comparisons across years through potential time-series modelling methods like a seasonal ARIMA model or something similar.\nCounties needing alternative approaches: No counties were classified as low confidence in NJ, so no specific alternatives are needed. However, it may be worthwhile to monitor the percent change in some of the higher MOE counties if they were to exhibit drastic increases in MOE % in future years. As mentioned previously, tract data within each county are much less reliable and could definitely benefit from precautionary measures such as manual data validation or increased data collection through additional surveys to give analysts more confidence in deriving algorithms to address issues at this level."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#questions-for-further-investigation",
    "href": "assignments/assignment1/assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nHow does MOE % for each county trend over time? Do counties with lower MOE % or higher MOE % fluctuate over time? In the specific case of tourism-driven economies like Cape May or Atlantic, does data exist on their economic output across each month / season?\nHow does geographical location factor into MOE % within counties? We saw how there can be small clusters of counties that share higher MOE % relative to the rest of NJ? Is this a common occurrence that we see in most other states as well? What factors can cause these clusters other than income and demographic trends?\nFor tract-level data, why did we see a lot more missing data for people of color than the White demographic? What possible solutions are there to mitigating this issue for future surveys?"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#submission-checklist",
    "href": "assignments/assignment1/assignment1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  }
]