[
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "weekly-notes/index.html",
    "href": "weekly-notes/index.html",
    "title": "Weekly Notes",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nWeek 3: EDA & ggplot2\n\n\nSep 22, 2025\n\n\n\n\n\n\nWeek 2 - Algorithmic Decision Making & Census Data\n\n\nSep 15, 2025\n\n\n\n\n\n\nWeek 1 - Course Introduction\n\n\nSep 8, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3: EDA & ggplot2",
    "section": "",
    "text": "EDA:\n\nCreate summary statistics table\n\nBut can hide underlying data distribution\n\nOutliers, non-linear relationships\n\nSo data visualization is important\nCan have same mean, var, cov, regression but diff patterns in visualization\n\nBad / Misleading data visualizations\n\nMisleading scales or axes\nCherry-picked time periods\nHidden or ignored uncertainty\nMissing context about data reliability\nResult: Poor policy decisions\n\nGoal: Understand your data before making decisions or building models\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?\n\nWorkflow w/ Data Quality Focus: Enhanced process for policy analysis\n\nLoad and inspect - dimensions, variable types, missing data\nAssess reliability - examine margins of error, calculate coefficients of variation\nVisualize distributions - histograms, boxplots for each variable\nExplore relationships - scatter plots, correlations\nIdentify patterns - grouping, clustering, geographical patterns\nQuestion anomalies - investigate outliers and unusual patterns\nDocument limitations - prepare honest communication about data quality\n\nJOINS - join predicate does not need to be same name, NEED SAME TYPE\n\nleft (most-common) / right: keeps everything from left/right\nfull: keeps everything from both columns\ninner: keeps only matches"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "title": "Week 3: EDA & ggplot2",
    "section": "",
    "text": "EDA:\n\nCreate summary statistics table\n\nBut can hide underlying data distribution\n\nOutliers, non-linear relationships\n\nSo data visualization is important\nCan have same mean, var, cov, regression but diff patterns in visualization\n\nBad / Misleading data visualizations\n\nMisleading scales or axes\nCherry-picked time periods\nHidden or ignored uncertainty\nMissing context about data reliability\nResult: Poor policy decisions\n\nGoal: Understand your data before making decisions or building models\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?\n\nWorkflow w/ Data Quality Focus: Enhanced process for policy analysis\n\nLoad and inspect - dimensions, variable types, missing data\nAssess reliability - examine margins of error, calculate coefficients of variation\nVisualize distributions - histograms, boxplots for each variable\nExplore relationships - scatter plots, correlations\nIdentify patterns - grouping, clustering, geographical patterns\nQuestion anomalies - investigate outliers and unusual patterns\nDocument limitations - prepare honest communication about data quality\n\nJOINS - join predicate does not need to be same name, NEED SAME TYPE\n\nleft (most-common) / right: keeps everything from left/right\nfull: keeps everything from both columns\ninner: keeps only matches"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03-notes.html#coding-techniques",
    "title": "Week 3: EDA & ggplot2",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nggplot2\n\nData -&gt; Aesthetics -&gt; Geometry -&gt; Visual\n\nData: Your dataset (census data, survey responses, etc.)\nAesthetics: What variables map to visual properties (x, y, color, size)\nGeometries: How to display the data (points, bars, lines)\nAdditional layers: Scales, themes, facets, annotations\nGeneral syntax: ggplot(data = your_data) + aes(x = variable1, y = variable2) + geom_something() + additional_layers()\n\nAesthetic Mappings: maps data to visual properties\n\nx, y - position\ncolor - point/line color\nfill - area fill color\nsize - point/line size\nshape - point shape\nalpha - transparency\n\nstr_detect(), str_extract(), regex() to search within strings"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3: EDA & ggplot2",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3: EDA & ggplot2",
    "section": "Connections to Policy",
    "text": "Connections to Policy"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3: EDA & ggplot2",
    "section": "Reflection",
    "text": "Reflection"
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\nAssignment 1: Census Data Quality for Policy Decisions\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "labs/index.html",
    "href": "labs/index.html",
    "title": "Labs",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\nLab 0: Getting Started with dplyr\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "labs/lab0/lab0.html",
    "href": "labs/lab0/lab0.html",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "",
    "text": "Welcome to your first lab! In this (not graded) assignment, you’ll practice the fundamental dplyr operations I overviewed in class using car sales data. This lab will help you get comfortable with:\n\nBasic data exploration\nColumn selection and manipulation\n\nCreating new variables\nFiltering data\nGrouping and summarizing\n\nInstructions: Copy this template into your portfolio repository under a lab_0/ folder, then complete each section with your code and answers. You will write the code under the comment section in each chunk. Be sure to also copy the data folder into your lab_0 folder."
  },
  {
    "objectID": "labs/lab0/lab0.html#data-structure-exploration",
    "href": "labs/lab0/lab0.html#data-structure-exploration",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.1 Data Structure Exploration",
    "text": "1.1 Data Structure Exploration\nExplore the structure of your data and answer these questions:\n\n# Use glimpse() to see the data structure\nglimpse(car_data)\n\nRows: 50,000\nColumns: 7\n$ Manufacturer          &lt;chr&gt; \"Ford\", \"Porsche\", \"Ford\", \"Toyota\", \"VW\", \"Ford…\n$ Model                 &lt;chr&gt; \"Fiesta\", \"718 Cayman\", \"Mondeo\", \"RAV4\", \"Polo\"…\n$ `Engine size`         &lt;dbl&gt; 1.0, 4.0, 1.6, 1.8, 1.0, 1.4, 1.8, 1.4, 1.2, 2.0…\n$ `Fuel type`           &lt;chr&gt; \"Petrol\", \"Petrol\", \"Diesel\", \"Hybrid\", \"Petrol\"…\n$ `Year of manufacture` &lt;dbl&gt; 2002, 2016, 2014, 1988, 2006, 2018, 2010, 2015, …\n$ Mileage               &lt;dbl&gt; 127300, 57850, 39190, 210814, 127869, 33603, 866…\n$ Price                 &lt;dbl&gt; 3074, 49704, 24072, 1705, 4101, 29204, 14350, 30…\n\n# Check the column names\ncolnames(car_data)\n\n[1] \"Manufacturer\"        \"Model\"               \"Engine size\"        \n[4] \"Fuel type\"           \"Year of manufacture\" \"Mileage\"            \n[7] \"Price\"              \n\n# Look at the first few rows\nhead(car_data)\n\n# A tibble: 6 × 7\n  Manufacturer Model     `Engine size` `Fuel type` `Year of manufacture` Mileage\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n1 Ford         Fiesta              1   Petrol                       2002  127300\n2 Porsche      718 Caym…           4   Petrol                       2016   57850\n3 Ford         Mondeo              1.6 Diesel                       2014   39190\n4 Toyota       RAV4                1.8 Hybrid                       1988  210814\n5 VW           Polo                1   Petrol                       2006  127869\n6 Ford         Focus               1.4 Petrol                       2018   33603\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestions to answer: - How many rows and columns does the dataset have? - What types of variables do you see (numeric, character, etc.)? - Are there any column names that might cause problems? Why?\nYour answers: - Rows: 50,000 - Columns: 7\n- Variable types: Character, double - Problematic names:“Engine size”, “Fuel type”, “Year of manufacture” are problematic since they have spaces in their names. When we call them, we must wrap these names in quotation marks as shown above."
  },
  {
    "objectID": "labs/lab0/lab0.html#tibble-vs-data-frame",
    "href": "labs/lab0/lab0.html#tibble-vs-data-frame",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.2 Tibble vs Data Frame",
    "text": "1.2 Tibble vs Data Frame\nCompare how tibbles and data frames display:\n\n# Look at the tibble version (what we have)\ncar_data\n\n# A tibble: 50,000 × 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay…           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ℹ 49,990 more rows\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n# Convert to regular data frame and display\ncar_df &lt;- as.data.frame(car_data)\nhead(car_df, 50) # Changed for better render\n\n   Manufacturer      Model Engine size Fuel type Year of manufacture Mileage\n1          Ford     Fiesta         1.0    Petrol                2002  127300\n2       Porsche 718 Cayman         4.0    Petrol                2016   57850\n3          Ford     Mondeo         1.6    Diesel                2014   39190\n4        Toyota       RAV4         1.8    Hybrid                1988  210814\n5            VW       Polo         1.0    Petrol                2006  127869\n6          Ford      Focus         1.4    Petrol                2018   33603\n7          Ford     Mondeo         1.8    Diesel                2010   86686\n8        Toyota      Prius         1.4    Hybrid                2015   30663\n9            VW       Polo         1.2    Petrol                2012   73470\n10         Ford      Focus         2.0    Diesel                1992  262514\n11           VW       Golf         2.0    Diesel                2014   83047\n12          BMW         Z4         2.0    Petrol                1990  293666\n13           VW       Golf         1.2    Diesel                2007   92697\n14       Toyota       RAV4         2.2    Petrol                2007   79393\n15       Toyota      Yaris         1.4    Petrol                1998   97286\n16           VW       Golf         1.6    Diesel                1989  222390\n17       Toyota       RAV4         2.4    Hybrid                2003  117425\n18       Toyota      Yaris         1.2    Petrol                1992  245990\n19       Toyota       RAV4         2.0    Hybrid                2018   28381\n20           VW       Polo         1.2    Petrol                1998  155038\n21           VW       Golf         1.2    Hybrid                1987  121744\n22         Ford     Mondeo         1.6    Diesel                1996   77584\n23       Toyota      Prius         1.0    Hybrid                2003  115291\n24       Toyota      Prius         1.0    Hybrid                1990  238571\n25      Porsche        911         2.6    Petrol                2009   66273\n26       Toyota      Prius         1.8    Hybrid                2017   31958\n27      Porsche        911         3.5    Petrol                2005  151556\n28       Toyota      Yaris         1.2    Petrol                2002  179097\n29           VW       Golf         2.0    Petrol                2020   18985\n30       Toyota       RAV4         1.8    Hybrid                2002   66990\n31         Ford      Focus         1.0    Hybrid                2010   85131\n32         Ford     Fiesta         1.0    Petrol                2001  144731\n33           VW       Polo         2.0    Diesel                2008   97001\n34           VW       Polo         1.6    Petrol                2016   52409\n35         Ford     Fiesta         1.4    Petrol                2010  112714\n36           VW     Passat         2.0    Diesel                1992  198540\n37           VW     Passat         1.8    Diesel                1989  213162\n38          BMW         Z4         2.2    Petrol                2005  133174\n39           VW       Polo         1.6    Petrol                2008  129588\n40          BMW         Z4         2.0    Petrol                1990  148586\n41         Ford      Focus         2.0    Petrol                1995   91173\n42          BMW         M5         4.0    Petrol                2017   22759\n43           VW       Polo         1.4    Petrol                1990  261526\n44           VW     Passat         1.4    Diesel                1995  235594\n45          BMW         Z4         2.0    Petrol                1995   42759\n46           VW     Passat         1.8    Diesel                2005  103352\n47           VW     Passat         1.6    Petrol                2003   63372\n48       Toyota      Yaris         1.0    Petrol                2009   51787\n49         Ford     Fiesta         1.4    Petrol                2014   41495\n50       Toyota      Yaris         1.4    Petrol                2010   43111\n   Price\n1   3074\n2  49704\n3  24072\n4   1705\n5   4101\n6  29204\n7  14350\n8  30297\n9   9977\n10  1049\n11 17173\n12   719\n13  7792\n14 16026\n15  4046\n16   933\n17 11667\n18   720\n19 52671\n20  2118\n21  1890\n22  5667\n23  6512\n24   961\n25 41963\n26 38961\n27 19747\n28  2548\n29 36387\n30 13553\n31 12472\n32  2503\n33  8784\n34 17257\n35  6936\n36  1964\n37  1340\n38  8511\n39  5848\n40  2637\n41  5181\n42 97758\n43   522\n44  1439\n45  7873\n46  9633\n47 10001\n48  9689\n49 14721\n50 12928\n\n\nQuestion: What differences do you notice in how they print?\nYour answer: Tibble will only display the first 10 rows with total number of rows and columns as well as column types as additional info. It also only shows a certain number of columns (in this case, only 6 of 7 columns displayed)."
  },
  {
    "objectID": "labs/lab0/lab0.html#selecting-columns",
    "href": "labs/lab0/lab0.html#selecting-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.1 Selecting Columns",
    "text": "2.1 Selecting Columns\nPractice selecting different combinations of columns:\n\n# Select just Model and Mileage columns\nmodel_mile &lt;- select(car_data, Model, Mileage)\nhead(model_mile, 5)\n\n# A tibble: 5 × 2\n  Model      Mileage\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Fiesta      127300\n2 718 Cayman   57850\n3 Mondeo       39190\n4 RAV4        210814\n5 Polo        127869\n\n# Select Manufacturer, Price, and Fuel type\nmanuf_price_fuel &lt;- select(car_data, Manufacturer, Price, \"Fuel type\")\nhead(manuf_price_fuel, 5)\n\n# A tibble: 5 × 3\n  Manufacturer Price `Fuel type`\n  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;      \n1 Ford          3074 Petrol     \n2 Porsche      49704 Petrol     \n3 Ford         24072 Diesel     \n4 Toyota        1705 Hybrid     \n5 VW            4101 Petrol     \n\n# Challenge: Select all columns EXCEPT Engine Size\nexcept_engine &lt;- select(car_data, -\"Engine size\")\nhead(except_engine, 5)\n\n# A tibble: 5 × 6\n  Manufacturer Model      `Fuel type` `Year of manufacture` Mileage Price\n  &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Ford         Fiesta     Petrol                       2002  127300  3074\n2 Porsche      718 Cayman Petrol                       2016   57850 49704\n3 Ford         Mondeo     Diesel                       2014   39190 24072\n4 Toyota       RAV4       Hybrid                       1988  210814  1705\n5 VW           Polo       Petrol                       2006  127869  4101"
  },
  {
    "objectID": "labs/lab0/lab0.html#renaming-columns",
    "href": "labs/lab0/lab0.html#renaming-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.2 Renaming Columns",
    "text": "2.2 Renaming Columns\nLet’s fix a problematic column name:\n\n# Rename 'Year of manufacture' to year\ncar_data &lt;- car_data |&gt;\n    rename(\n    year = `Year of manufacture`,\n    engine_size = `Engine size`,\n    fuel = `Fuel type`\n  )\n\n# Check that it worked\nnames(car_data)\n\n[1] \"Manufacturer\" \"Model\"        \"engine_size\"  \"fuel\"         \"year\"        \n[6] \"Mileage\"      \"Price\"       \n\n\nQuestion: Why did we need backticks around Year of manufacture but not around year?\nYour answer: Because ‘year’ has no spaces in its variable name."
  },
  {
    "objectID": "labs/lab0/lab0.html#calculate-car-age",
    "href": "labs/lab0/lab0.html#calculate-car-age",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.1 Calculate Car Age",
    "text": "3.1 Calculate Car Age\n\n# Create an 'age' column (2025 minus year of manufacture)\n# Create a mileage_per_year column \ncar_data &lt;- car_data |&gt;\n  mutate(age = 2025 - year,\n         mileage_per_year = Mileage / age)\n\n# Look at your new columns\nselect(car_data, Model, year, age, Mileage, mileage_per_year)\n\n# A tibble: 50,000 × 5\n   Model       year   age Mileage mileage_per_year\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1 Fiesta      2002    23  127300            5535.\n 2 718 Cayman  2016     9   57850            6428.\n 3 Mondeo      2014    11   39190            3563.\n 4 RAV4        1988    37  210814            5698.\n 5 Polo        2006    19  127869            6730.\n 6 Focus       2018     7   33603            4800.\n 7 Mondeo      2010    15   86686            5779.\n 8 Prius       2015    10   30663            3066.\n 9 Polo        2012    13   73470            5652.\n10 Focus       1992    33  262514            7955.\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/lab0.html#categorize-cars",
    "href": "labs/lab0/lab0.html#categorize-cars",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.2 Categorize Cars",
    "text": "3.2 Categorize Cars\n\n# Create a price_category column where if price is &lt; 15000, its is coded as budget, between 15000 and 30000 is midrange and greater than 30000 is mid-range (use case_when)\ncar_data &lt;- car_data |&gt;\n  mutate(price_category = case_when(\n    Price &lt; 15000 ~ \"budget\", \n    between(Price, 15000, 30000) ~ \"midrange\",\n    TRUE ~ \"luxury\"))\n\n# Check your categories select the new column and show it\nselect(car_data, price_category)\n\n# A tibble: 50,000 × 1\n   price_category\n   &lt;chr&gt;         \n 1 budget        \n 2 luxury        \n 3 midrange      \n 4 budget        \n 5 budget        \n 6 midrange      \n 7 budget        \n 8 luxury        \n 9 budget        \n10 budget        \n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/lab0.html#basic-filtering",
    "href": "labs/lab0/lab0.html#basic-filtering",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.1 Basic Filtering",
    "text": "4.1 Basic Filtering\n\n# Find all Toyota cars\ntoyota &lt;- car_data |&gt;\n  filter(Manufacturer == \"Toyota\")\nhead(toyota, 5)\n\n# A tibble: 5 × 10\n  Manufacturer Model engine_size fuel    year Mileage Price   age\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Toyota       RAV4          1.8 Hybrid  1988  210814  1705    37\n2 Toyota       Prius         1.4 Hybrid  2015   30663 30297    10\n3 Toyota       RAV4          2.2 Petrol  2007   79393 16026    18\n4 Toyota       Yaris         1.4 Petrol  1998   97286  4046    27\n5 Toyota       RAV4          2.4 Hybrid  2003  117425 11667    22\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with mileage less than 30,000\nmile_less_30000 &lt;- car_data |&gt;\n  filter(Mileage &lt; 30000)\nhead(mile_less_30000, 5)\n\n# A tibble: 5 × 10\n  Manufacturer Model engine_size fuel    year Mileage Price   age\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Toyota       RAV4          2   Hybrid  2018   28381 52671     7\n2 VW           Golf          2   Petrol  2020   18985 36387     5\n3 BMW          M5            4   Petrol  2017   22759 97758     8\n4 Toyota       RAV4          2.4 Petrol  2018   24588 49125     7\n5 VW           Golf          2   Hybrid  2018   25017 36957     7\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find luxury cars (from price category) with low mileage\n# ASSUMING low mileage is less than 30000 from part 2\nluxury_low_mile &lt;- mile_less_30000 |&gt;\n  filter(price_category == \"luxury\")\nhead(luxury_low_mile, 5)\n\n# A tibble: 5 × 10\n  Manufacturer Model engine_size fuel    year Mileage Price   age\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Toyota       RAV4          2   Hybrid  2018   28381 52671     7\n2 VW           Golf          2   Petrol  2020   18985 36387     5\n3 BMW          M5            4   Petrol  2017   22759 97758     8\n4 Toyota       RAV4          2.4 Petrol  2018   24588 49125     7\n5 VW           Golf          2   Hybrid  2018   25017 36957     7\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "labs/lab0/lab0.html#multiple-conditions",
    "href": "labs/lab0/lab0.html#multiple-conditions",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.2 Multiple Conditions",
    "text": "4.2 Multiple Conditions\n\n# Find cars that are EITHER Honda OR Nissan\nhonda_nissan &lt;- car_data |&gt;\n    filter(Manufacturer %in% c(\"Honda\", \"Nissan\"))\nhead(honda_nissan, 5)\n\n# A tibble: 0 × 10\n# ℹ 10 variables: Manufacturer &lt;chr&gt;, Model &lt;chr&gt;, engine_size &lt;dbl&gt;,\n#   fuel &lt;chr&gt;, year &lt;dbl&gt;, Mileage &lt;dbl&gt;, Price &lt;dbl&gt;, age &lt;dbl&gt;,\n#   mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# NO NISSAN OR HONDA IN DATASET\n\n# Find cars with price between $20,000 and $35,000\nprice_20k_35k &lt;- car_data |&gt;\n  filter(Price &gt; 20000 & Price &lt; 35000)\nhead(price_20k_35k, 5)\n\n# A tibble: 5 × 10\n  Manufacturer Model  engine_size fuel    year Mileage Price   age\n  &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Ford         Mondeo         1.6 Diesel  2014   39190 24072    11\n2 Ford         Focus          1.4 Petrol  2018   33603 29204     7\n3 Toyota       Prius          1.4 Hybrid  2015   30663 30297    10\n4 Toyota       Prius          1.4 Hybrid  2016   43893 29946     9\n5 Toyota       Prius          1.4 Hybrid  2016   43130 30085     9\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find diesel cars less than 10 years old\ndiesel_less_10_year &lt;- car_data |&gt;\n  filter(fuel == \"Diesel\" & age &lt; 10)\nhead(diesel_less_10_year, 5)\n\n# A tibble: 5 × 10\n  Manufacturer Model  engine_size fuel    year Mileage Price   age\n  &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Ford         Fiesta         1   Diesel  2017   38370 16257     8\n2 VW           Passat         1.6 Diesel  2018   22122 36634     7\n3 VW           Passat         1.4 Diesel  2020   21413 39310     5\n4 BMW          X3             2   Diesel  2018   27389 44018     7\n5 Ford         Mondeo         2   Diesel  2016   51724 28482     9\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\nglimpse(diesel_less_10_year)\n\nRows: 2,040\nColumns: 10\n$ Manufacturer     &lt;chr&gt; \"Ford\", \"VW\", \"VW\", \"BMW\", \"Ford\", \"Porsche\", \"VW\", \"…\n$ Model            &lt;chr&gt; \"Fiesta\", \"Passat\", \"Passat\", \"X3\", \"Mondeo\", \"Cayenn…\n$ engine_size      &lt;dbl&gt; 1.0, 1.6, 1.4, 2.0, 2.0, 2.6, 1.2, 1.8, 1.4, 1.4, 1.4…\n$ fuel             &lt;chr&gt; \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Di…\n$ year             &lt;dbl&gt; 2017, 2018, 2020, 2018, 2016, 2019, 2018, 2016, 2020,…\n$ Mileage          &lt;dbl&gt; 38370, 22122, 21413, 27389, 51724, 20147, 37411, 2943…\n$ Price            &lt;dbl&gt; 16257, 36634, 39310, 44018, 28482, 76182, 19649, 3088…\n$ age              &lt;dbl&gt; 8, 7, 5, 7, 9, 6, 7, 9, 5, 7, 7, 9, 8, 3, 7, 3, 9, 7,…\n$ mileage_per_year &lt;dbl&gt; 4796.2500, 3160.2857, 4282.6000, 3912.7143, 5747.1111…\n$ price_category   &lt;chr&gt; \"midrange\", \"luxury\", \"luxury\", \"luxury\", \"midrange\",…\n\n\nQuestion: How many diesel cars are less than 10 years old?\nYour answer: 2040"
  },
  {
    "objectID": "labs/lab0/lab0.html#basic-summaries",
    "href": "labs/lab0/lab0.html#basic-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.1 Basic Summaries",
    "text": "5.1 Basic Summaries\n\n# Calculate average price by manufacturer\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer avg_price\n  &lt;chr&gt;            &lt;dbl&gt;\n1 BMW             24429.\n2 Ford            10672.\n3 Porsche         29104.\n4 Toyota          14340.\n5 VW              10363.\n\n# Calculate average mileage by fuel type\navg_mile_by_fuel &lt;- car_data |&gt;\n  group_by(fuel) |&gt;\n  summarize(avg_mile = mean(Mileage, na.rm = T))\n\navg_mile_by_fuel\n\n# A tibble: 3 × 2\n  fuel   avg_mile\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Diesel  112667.\n2 Hybrid  111622.\n3 Petrol  112795.\n\n# Count cars by manufacturer\ncount_cars_by_brand &lt;- car_data |&gt;\n  group_by(Manufacturer) |&gt;\n  summarize(count_car = n())\n\ncount_cars_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer count_car\n  &lt;chr&gt;            &lt;int&gt;\n1 BMW               4965\n2 Ford             14959\n3 Porsche           2609\n4 Toyota           12554\n5 VW               14913"
  },
  {
    "objectID": "labs/lab0/lab0.html#categorical-summaries",
    "href": "labs/lab0/lab0.html#categorical-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.2 Categorical Summaries",
    "text": "5.2 Categorical Summaries\n\n# Frequency table for price categories\ncar_data |&gt;\n  count(price_category) |&gt;\n  mutate(percent = 100 * n / sum(n)) |&gt;\n  select(-n)\n\n# A tibble: 3 × 2\n  price_category percent\n  &lt;chr&gt;            &lt;dbl&gt;\n1 budget            68.1\n2 luxury            12.4\n3 midrange          19.6\n\ncar_data\n\n# A tibble: 50,000 × 10\n   Manufacturer Model      engine_size fuel    year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol  2002  127300  3074    23\n 2 Porsche      718 Cayman         4   Petrol  2016   57850 49704     9\n 3 Ford         Mondeo             1.6 Diesel  2014   39190 24072    11\n 4 Toyota       RAV4               1.8 Hybrid  1988  210814  1705    37\n 5 VW           Polo               1   Petrol  2006  127869  4101    19\n 6 Ford         Focus              1.4 Petrol  2018   33603 29204     7\n 7 Ford         Mondeo             1.8 Diesel  2010   86686 14350    15\n 8 Toyota       Prius              1.4 Hybrid  2015   30663 30297    10\n 9 VW           Polo               1.2 Petrol  2012   73470  9977    13\n10 Ford         Focus              2   Diesel  1992  262514  1049    33\n# ℹ 49,990 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html",
    "href": "assignments/assignment1/assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the NJ Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#scenario",
    "href": "assignments/assignment1/assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the NJ Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#learning-objectives",
    "href": "assignments/assignment1/assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#submission-instructions",
    "href": "assignments/assignment1/assignment1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an “Assignments” menu."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#data-retrieval",
    "href": "assignments/assignment1/assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\nnj_inc_pop &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n        median_income = \"B19013_001\",\n        total_pop = \"B01003_001\"\n  ),\n  state = my_state,\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\nnj_inc_pop &lt;- nj_inc_pop |&gt;\n  # Remove state name and \"County\"\n  mutate(\n    county_name = str_remove(NAME, \", New Jersey\"),\n    county_name = str_remove(county_name, \" County\")\n  ) |&gt;\n  # Drop NAME column\n  select(-NAME)\n\n# Display the first few rows\nhead(nj_inc_pop, 5)\n\n# A tibble: 5 × 6\n  GEOID median_incomeE median_incomeM total_popE total_popM county_name\n  &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      \n1 34001          73113           1917     274339         NA Atlantic   \n2 34003         118714           1607     953243         NA Bergen     \n3 34005         102615           1436     461853         NA Burlington \n4 34007          82005           1414     522581         NA Camden     \n5 34009          83870           3707      95456         NA Cape May"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#data-quality-assessment",
    "href": "assignments/assignment1/assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\nnj_reli &lt;- nj_inc_pop |&gt;\n  mutate(\n    # Compute MOE percentage\n    moe_percent = round((median_incomeM / median_incomeE) * 100, 2),\n    # Create reliability categories\n    reliability = case_when(\n      moe_percent &lt; 5 ~ \"High Confidence\",\n      moe_percent &gt;= 5 & moe_percent &lt;= 10 ~ \"Moderate\",\n      moe_percent &gt; 10 ~ \"Low Confidence\"\n    ),\n    # Flag when MOE is greater than 10%\n    moe_flag = moe_percent &gt; 10\n  )\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\nnj_reli_summary &lt;- nj_reli |&gt;\n  # Count the number of counties per reliability category\n  count(reliability, name = \"n\") |&gt;\n  # Add percentages - count(county) / count(all)\n  mutate(\n  reliability_percent = round(100 * n / sum(n), 1),\n  reliability_percent = paste0(reliability_percent, \"%\")\n  ) \n\n# Display summary table\nkable(nj_reli_summary,\n    caption = \"County Reliability Summary\",\n    col.names = c(\"Reliability\", \"Count\", \"Percent\"))\n\n\nCounty Reliability Summary\n\n\nReliability\nCount\nPercent\n\n\n\n\nHigh Confidence\n21\n100%"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#high-uncertainty-counties",
    "href": "assignments/assignment1/assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\nnj_top_moe &lt;- nj_reli |&gt;\n  # Sort by DESC MOE percent\n  arrange(desc(moe_percent)) |&gt;\n  # Select top 5 counties\n  slice_head(n = 5) |&gt;\n  # Select columns\n  select(county_name, median_incomeE, median_incomeM, moe_percent, reliability)\n\n# Format as table with kable() - include appropriate column names and caption\nkable(nj_top_moe,\n      col.names = c(\"County\", \"Median Income\", \"MOE\", \"MOE %\", \"Reliabilty\"),\n      caption = \"NJ Counties with the Highest Income Data Uncertainty\",\n      format.args = list(big.mark = \",\"))\n\n\nNJ Counties with the Highest Income Data Uncertainty\n\n\nCounty\nMedian Income\nMOE\nMOE %\nReliabilty\n\n\n\n\nCape May\n83,870\n3,707\n4.42\nHigh Confidence\n\n\nSalem\n73,378\n3,047\n4.15\nHigh Confidence\n\n\nCumberland\n62,310\n2,205\n3.54\nHigh Confidence\n\n\nAtlantic\n73,113\n1,917\n2.62\nHigh Confidence\n\n\nGloucester\n99,668\n2,605\n2.61\nHigh Confidence\n\n\n\n\n\nData Quality Commentary:\n[Write 2-3 sentences explaining what these results mean for algorithmic decision-making. Consider: Which counties might be poorly served by algorithms that rely on this income data? What factors might contribute to higher uncertainty?]\nIn New Jersey, all counties have a median income MOE % below our threshold of 5%, indicating that the reporting of income date is highly reliable based on our standards. Even though these fall within high confidence, we should still proceed with caution when using this data for algorithmic decision-making. A pattern amongst the top highest MOE % revealed that all five counties make up the most southern counties in New Jersey.\nFor Cape May and Atlantic counties, a potential cause for higher MOE % stems from their economies being highly reliant on tourism for many residents. Especially, when you consider their location at the Jersey Shore, the flow of tourists is highly dependent on the season (notably, Summer), which can lead to further uncertainty in income reporting.\nCumberland, Salem, and Atlantic counties represent the three lowest counties in median income from this data. It may be the case that these counties may exhibit larger margins of error due to more residents working lower paying, hourly jobs that skew the reporting of median income, leading to higher MOE %."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#focus-area-selection",
    "href": "assignments/assignment1/assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\nNote from Jack: All counties in NJ have high confidence levels. I checked with Prof Delmelle, and she said that it was fine to continue with my analysis for NJ. I chose Cape May (MOE = 4.42%, Beach), Essex (2.00%. North Jersey), and Burlington (MOE = 1.40%, South Jersey / Philly) counties for this section to get a diverse selection of MOE percentages and geographical locations.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\nselected_countries &lt;- nj_reli |&gt;\n  filter(county_name %in% c(\"Cape May\", \"Essex\", \"Burlington\")) |&gt;\n  # Select county name, median income, MOE percentage, reliability category\n  select(county_name, median_incomeE, moe_percent, reliability)\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nkable(selected_countries,\n      caption = \"Selected Counties\",\n      col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\"))\n\n\nSelected Counties\n\n\nCounty\nMedian Income\nMOE %\nReliability\n\n\n\n\nBurlington\n102615\n1.40\nHigh Confidence\n\n\nCape May\n83870\n4.42\nHigh Confidence\n\n\nEssex\n73785\n2.00\nHigh Confidence\n\n\n\n\n\nComment on the output: Burlington county has the highest median income with the lowest MOE %, indicating that it is the safest out of the three for algorithmic decision-making. As stated previously, Cape May has a high MOE %, relative to the rest of NJ, due to its tourist driven economy and volatility in income during a calendar year. Essex county has a moderately low MOE % and median income, revealing that lower income communities may not always have higher MOE %. Although, Essex could be an outlier."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#tract-level-demographics",
    "href": "assignments/assignment1/assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\nwhite &lt;- \"B03002_003\"\nblack &lt;- \"B03002_004\"\nhispanic &lt;- \"B03002_012\"\ntotal_pop &lt;- \"B03002_001\"\n\n# Set county codes (GEOID): Burlington, Cape May, Essex\n# Remove \"34\" from start of GEOID code\ngeoid_codes &lt;- c(\"005\", \"009\", \"013\")\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\nnj_tract &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n        white = white,\n        black = black,\n        hispanic = hispanic,\n        total_pop = total_pop\n  ),\n  state = my_state, \n  county = geoid_codes,\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n) \n\n# Calculate percentage of each group using mutate()\nnj_tract &lt;- nj_tract |&gt;\n  # Create percentages for white, Black, and Hispanic populations\n  mutate(\n    white_percent = round(100 * whiteE / total_popE, 2),\n    black_percent = round(100 * blackE / total_popE, 2),\n    hispanic_percent = round(100 * hispanicE / total_popE, 2)) |&gt;\n    # Add readable tract and county name columns using str_extract() or similar                \n    mutate(\n      # Tract (RegEx)\n      tract = str_remove(str_extract(NAME, \"Census Tract\\\\s*[0-9]+(?:\\\\.[0-9]+)?\"),\"^Census Tract\\\\s*\"),\n\n      # County (RegEx)\n      county = str_extract(NAME, \";\\\\s*[^;]+\\\\s*;\") |&gt;\n               str_remove(\"^;\\\\s*\") |&gt;\n               str_remove(\"\\\\s*;$\") |&gt;\n               str_trim(),\n      \n      county = str_remove(county, \"\\\\s+County\\\\b\") |&gt;\n               str_trim()\n  ) \n\n# head(nj_tract)"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#demographic-analysis",
    "href": "assignments/assignment1/assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\nhigh_hispanic &lt;- nj_tract |&gt;\n  arrange(desc(hispanic_percent)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(county, tract, hispanic_percent)\n\nkable(high_hispanic,\n    caption = \"Highest Percentage of Hispanic Residents\",\n    col.names = c(\"County\", \"Tract\", \"Hispanic %\"))\n\n\nHighest Percentage of Hispanic Residents\n\n\nCounty\nTract\nHispanic %\n\n\n\n\nEssex\n97\n89.51\n\n\n\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\navg_demo &lt;- nj_tract |&gt;\n  group_by(county) |&gt;\n  summarize(\n    num_tracts = n(),\n    avg_white = round(mean(white_percent, na.rm = TRUE), 2),\n    avg_black = round(mean(black_percent, na.rm = TRUE), 2),\n    avg_hispanic = round(mean(hispanic_percent, na.rm = TRUE), 2)\n  )\n\n# Create a nicely formatted table of your results using kable()\nkable(avg_demo,\n      col.names = c(\"County\", \"Number of Tracts\", \"Average White %\", \"Average Black %\", \"Average Hispanic %\"),\n      caption = \"Average Demographics for Burlington, Cape May, and Essex Counties (NJ)\",\n      format.args = list(big.mark = \",\"))\n\n\nAverage Demographics for Burlington, Cape May, and Essex Counties (NJ)\n\n\n\n\n\n\n\n\n\nCounty\nNumber of Tracts\nAverage White %\nAverage Black %\nAverage Hispanic %\n\n\n\n\nBurlington\n117\n62.77\n17.41\n9.79\n\n\nCape May\n33\n86.01\n2.83\n7.66\n\n\nEssex\n211\n24.98\n41.78\n23.42"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\nnj_tract &lt;- nj_tract |&gt;\n  mutate(\n    # Calculate MOE percentages for white, Black, and Hispanic variables\n    # Hint: use the same formula as before (margin/estimate * 100)\n    # Need if/else some tracts show 0 population for a demographic\n    white_moe_percent = if_else(whiteE &gt; 0, round(100 * whiteM / whiteE, 2), NA_real_),\n    black_moe_percent = if_else(blackE &gt; 0, round((blackM / blackE) * 100, 2), NA_real_),\n    hispanic_moe_percent = if_else(hispanicE &gt; 0, round((whiteM / whiteE) * 100, 2), NA_real_)\n  )\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\nflag_tracts &lt;- nj_tract |&gt;\n   mutate(\n     flag_moe = ifelse(\n      coalesce(white_moe_percent &gt; 10, FALSE) |\n      coalesce(black_moe_percent &gt; 10, FALSE) |\n      coalesce(hispanic_moe_percent &gt; 10, FALSE),\n      \"High MOE\", \"Low MOE\"\n     )\n   )\n\n# Create summary statistics showing how many tracts have data quality issues\nflag_tracts_summary &lt;- flag_tracts |&gt;\n  summarize(\n    total_tracts = n(),\n    flagged_tracts = sum(flag_moe == \"High MOE\", na.rm = TRUE),\n    flagged_percents = round(100 * flagged_tracts / total_tracts, 2)\n  )\n\nkable(flag_tracts_summary, \n      col.names = c(\"Total Tracts\", \"Flagged Tracts\", \"Flagged %\"),\n      caption = \"Summary of Tracts in Burlington, Cape May, Essex counties with High MOE (Any Demo &gt; 15%)\",\n      format.args = list(big.mark = \",\"))\n\n\nSummary of Tracts in Burlington, Cape May, Essex counties with High MOE (Any Demo &gt; 15%)\n\n\nTotal Tracts\nFlagged Tracts\nFlagged %\n\n\n\n\n361\n355\n98.34"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#pattern-analysis",
    "href": "assignments/assignment1/assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Calculate population percentages for each tract\nflag_tracts &lt;- flag_tracts %&gt;%\n  mutate(\n    white_pop_percent = if_else(total_popE &gt; 0, round(100 * whiteE / total_popE, 2), NA_real_),\n    black_pop_percent = if_else(total_popE &gt; 0, round(100 * blackE / total_popE, 2), NA_real_),\n    hispanic_pop_percent = if_else(total_popE &gt; 0, round(100 * hispanicE / total_popE, 2), NA_real_)\n  )\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\nflag_tracts_group &lt;- flag_tracts |&gt;\n  # Group by MOE threshold\n  group_by(flag_moe) |&gt;\n  summarize(\n    # Total number of tracts\n    num_tracts = n(),\n    \n    # Avg population sizes for each demographic group\n    avg_white_pop = round(mean(whiteE, na.rm = TRUE), 2),\n    avg_black_pop = round(mean(blackE, na.rm = TRUE), 2),\n    avg_hispanic_pop = round(mean(hispanicE, na.rm = TRUE), 2),\n    \n    # Avg Population percentage for each demographic group\n    avg_white_pop_percent = round(mean(white_pop_percent, na.rm = TRUE), 2),\n    avg_black_pop_percent = round(mean(black_pop_percent, na.rm = TRUE), 2),\n    avg_hispanic_pop_percent = round(mean(hispanic_pop_percent, na.rm = TRUE), 2),\n  )\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\nkable(\n  flag_tracts_group,\n  col.names = c(\n    \"MOE Flag Indicator\",\n    \"Number of Tracts\",\n    \"Avg White Population\", \"Avg Black Population\", \"Avg Hispanic Population\",\n    \"Avg White Pop %\", \"Avg Black Pop %\", \"Avg Hispanic Pop %\"\n  ),\n  caption = \"Tract Demographic Statistics Based on MOE Flag Indicator\",\n  format.args = list(big.mark = \",\")\n)\n\n\nTract Demographic Statistics Based on MOE Flag Indicator\n\n\n\n\n\n\n\n\n\n\n\n\nMOE Flag Indicator\nNumber of Tracts\nAvg White Population\nAvg Black Population\nAvg Hispanic Population\nAvg White Pop %\nAvg Black Pop %\nAvg Hispanic Pop %\n\n\n\n\nHigh MOE\n355\n1,713.62\n1,096.94\n713.41\n42.21\n30.71\n17.72\n\n\nLow MOE\n6\n2,383.67\n0.00\n130.17\n88.89\n0.00\n4.86\n\n\n\n\n\nPattern Analysis:\nOne pattern is that an overwhelming majority of tracts are flagged with high MOE (for at least one demographic) of around 98.34%. There are a few reasons why we observe this. For one, tracts are much smaller than counties and when we have a smaller sample size, this can lead to more variation, hence less reliability (higher margins of error throughout the data). Secondly, since we set our MOE indicator to be classified as “High MOE” if one or more demographic had a higher MOE than 10%. As a result, this design decision leads to more tracts being categorized as “High MOE” than if we made a stricter boundary of all three demographics must have a higher MOE than 10%. Lastly, the tract data was much sparser than the county level data with many demographic estimates and margins being zero for certain tracks, which could possibly be a reporting error.\nFrom the clear imbalance of our high and low MOE classes for the tracts in our selected counties, we should approach drawing conclusions about demographics and reliability with caution. Nevertheless, it seems that our six “Low MOE” tracts are much less diverse and predominantly White (88.89% compared to 42.21%). It is possible that these communities may have better access to reporting their statistics or are potentially more well informed about the Census and obligation to report. But, it is important to question the oddity that there average Black population is 0% across these tracts. This leads me to believe that this is due to the sparsity issue in the data itself. If the data showed that there are no Black residents (and very little Hispanic residents), then these tracts theoretically would only have to worry about passing the MOE threshold for the White demographic. It is important to consider all possibilities when trying to analyze and uncover patterns within data."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\nOverall Pattern Identification: Three significant systematic patterns arose across our analyses. The first pattern was that there was a drastic decline in data reliability when moving from county to tract-level data. In our county analysis, all counties were classified as “high confidence” in MOE % for median income data. However, in our tract data for selected counties, less than 2% of tracts were classified as “high confidence” for demographic data. The second pattern came from our tract-level analysis where we saw that more diverse tracts were much less reliable in terms of collective demographic MOE %. The small percentage of “high confidence” tracts were predominantly white (88.89% compared to 42.21%). The third trend was revealed in geographic location in county analysis. Specifically, the highest MOE % of income data mainly resided in South Jersey, revealing clusters of uncertainty among these counties.\nEquity Assessment: Diverse communities with higher population percentages of people of color (Black, Hispanic) are most at risk of algorithmic biases. In our tract-level analysis, we saw more confident aggregate results among communities that were predominantly White rather than diverse. In addition, a majority of the missing data in these tracts came from the Black and Hispanic population estimates and margins. These discrepancies could cause further inequity in these diverse communities when using this tract-level data without caution in our algorithmic decision-making.\nRoot Cause Analysis: - Counties with more variable income flow such as Cape May or Atlantic are likely to have less reliable survey responses, possibly skewing our data depending on when they are surveyed. For example, imagine an ice cream salesman at the Beach is asked about his income in July vs. December. - The effect of a smaller sample size in our tract data compared to county data is the main cause of less reliability in our MOE estimates. Smaller sample sizes leads to fewer survey responses, which leads to greater variability causing more uncertainty. - The effect of sparse or missing data at the tract-level is another potential cause for biased estimates in the reliability of our tract analysis. Since most of the “low MOE” tracts had missing demographic data for either the Black or Hispanic demographics, this made it easier for these tracts to pass our threshold test.\nStrategic Recommendations: - Reliability Thresholds: We should establish more realistic reliability thresholds at both the county and tract level. It is evident that our current decision boundary is not insightful with all counties having high reliability and 98% of tracts having low reliability. We should tighten the threshold for county data in New Jersey. For tract data, it would be valuable to split up reliability by race rather than testing if at least one race fails the threshold. It is also important to highlight which counties have missing data and exclude them from this analysis or state their omission. - Improved Efforts in Surveying Diverse Tracts & Country Clusters: We saw that diverse tracts had less reliability than predominantly White tracts. Policymakers should make a stronger effort at reaching these communities to ensure that we are obtaining accurate data to prevent drawing biased conclusions about these demographic groups without proper representation. Additionally, we saw that there were clusters of counties that showed higher uncertainty than the rest of the state. It would be beneficial to address these clusters by promoting survey participation to aid policymakers. - Higher Frequency Monitoring: For immediate policy considerations, it would be helpful to have more frequent data collection such as every three months or even every month if possible. This would be very beneficial to counties like Cape May or Atlantic but others as well. It would identify problems in communities much faster and allow for more actionable, targeted intervention in policy making."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#specific-recommendations",
    "href": "assignments/assignment1/assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\nfinal_df &lt;- nj_reli |&gt;\n  # Add a new column with algorithm recommendations using case_when():\n  # - High Confidence: \"Safe for algorithmic decisions\"\n  # - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n  # - Low Confidence: \"Requires manual review or additional data\"\n  mutate(\n    reli_cat = case_when(\n      reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  ) |&gt;\n  # Sort by MOE percent (DESC)\n  arrange(desc(moe_percent)) |&gt;\n  # Select relevant columns to include\n  select(county_name, median_incomeE, moe_percent, reliability, reli_cat) |&gt;\n  # Add $ and % symbols to median_incomeE and moe_percent\n  mutate(\n    median_incomeE = dollar(median_incomeE),\n    moe_percent = paste0(number(moe_percent, accuracy = 0.01), \"%\")\n  )\n\n# Format as a professional table with kable()\nkable(\n  final_df,\n  col.names = c(\"County\", \"Median Income\", \"MOE Percent\", \"Reliability\", \"Recommendation\"),\n  caption = \"Median Income Decision Framework for NJ Counties\",\n  format.args = list(big.mark = \",\")\n)\n\n\nMedian Income Decision Framework for NJ Counties\n\n\n\n\n\n\n\n\n\nCounty\nMedian Income\nMOE Percent\nReliability\nRecommendation\n\n\n\n\nCape May\n$83,870\n4.42%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSalem\n$73,378\n4.15%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCumberland\n$62,310\n3.54%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nAtlantic\n$73,113\n2.62%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nGloucester\n$99,668\n2.61%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWarren\n$92,620\n2.60%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSomerset\n$131,948\n2.49%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSussex\n$111,094\n2.46%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHunterdon\n$133,534\n2.42%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMercer\n$92,697\n2.33%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nUnion\n$95,000\n2.33%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMorris\n$130,808\n2.08%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHudson\n$86,854\n2.05%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nEssex\n$73,785\n2.00%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPassaic\n$84,465\n1.85%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOcean\n$82,379\n1.77%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCamden\n$82,005\n1.72%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMonmouth\n$118,527\n1.61%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMiddlesex\n$105,206\n1.46%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBurlington\n$102,615\n1.40%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBergen\n$118,714\n1.35%\nHigh Confidence\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: All counties in NJ are considered suitable for algorithmic implementation. They all have MOE less than our 5% threshold, making them high confidence for algorithmic decision making. However, if we wanted to examine tracts in individual counties, based on our tract-level analysis of three NJ counties, our data is much less reliable to use. Therefore, we should proceed with more caution for any kind of tract-level algorithmic implementation.\nCounties requiring additional oversight: No counties were classified as moderate confidence in NJ. However, I would say that counties near the bound of the 5% threshold like Cape May and Salem could be more closely monitored given their relatively high MOE above 4%. Additionally, as expressed earlier, counties such as Cape May and Atlantic that are highly reliant on tourism in generating income for its residents could use some supplementary data collection for analysis. Since income flow is extremely time-dependent, we should look to monitor the changes in income related to each season (or month) to get a better understanding of the income data. This would lead to more consistent comparisons across years through potential time-series modelling methods like a seasonal ARIMA model or something similar.\nCounties needing alternative approaches: No counties were classified as low confidence in NJ, so no specific alternatives are needed. However, it may be worthwhile to monitor the percent change in some of the higher MOE counties if they were to exhibit drastic increases in MOE % in future years. As mentioned previously, tract data within each county are much less reliable and could definitely benefit from precautionary measures such as manual data validation or increased data collection through additional surveys to give analysts more confidence in deriving algorithms to address issues at this level."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#questions-for-further-investigation",
    "href": "assignments/assignment1/assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nHow does MOE % for each county trend over time? Do counties with lower MOE % or higher MOE % fluctuate over time? In the specific case of tourism-driven economies like Cape May or Atlantic, does data exist on their economic output across each month / season?\nHow does geographical location factor into MOE % within counties? We saw how there can be small clusters of counties that share higher MOE % relative to the rest of NJ? Is this a common occurrence that we see in most other states as well? What factors can cause these clusters other than income and demographic trends?\nFor tract-level data, why did we see a lot more missing data for people of color than the White demographic? What possible solutions are there to mitigating this issue for future surveys?"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#submission-checklist",
    "href": "assignments/assignment1/assignment1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\n\nName: Henry (Jack) Bader\nWhy I am taking this course: I want to deepen my understanding of geospatial data science techniques in the public sector that provides a different perspective than typical engineering data science courses.\n\n\n\n\n\nEmail: jbader14@seas.upenn.edu\nGitHub: jbader14"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Name: Henry (Jack) Bader\nWhy I am taking this course: I want to deepen my understanding of geospatial data science techniques in the public sector that provides a different perspective than typical engineering data science courses."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: jbader14@seas.upenn.edu\nGitHub: jbader14"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "",
    "text": "Clone Repo Workflow:\n\nNavigate to repository\nClick &lt;&gt; code: green button\nOpen with Github Desktop\nCopy over data, template scripts, etc. over from cloned class repo to personal repo\nOTHER METHOD: Use terminal\n\ncd ~/file location\ngit clone https://github.com/MUSA-5080-Fall-2025/MUSA-5080-Fall-2025.git\n\n\nALGORITHM: Set of rules/instructions for solving a problem or completing a task.\n\nIn Government: Systems used to assist / replace human decision-makers.\n\nBased on predictions from models that process historical data:\nInputs: features/predictors/independent variables/x\nOutputs: labels/outcome/dependent variable/y\nHelps eliminate human bias / subjectivity\nExamples: criminal justice (bail, sentencing), housing and finance (whether or not to rent), healthcare (prioritization, resource allocation)\n\nHealthcare Algorithmic Bias: Algorithm used to identify high-risk patients for additional care systematically discriminated against Black patients.\n\nAlgorithm used healthcare costs as a proxy for need / risk.\nHistorical inequity: Black patients typically incur lower costs due to systematic inequities in access.\nResult: Black patients under-prioritized despite equivalent levels of illness.\nScale: Used by hospitals and insurers for over 200 million people annually.\n\nCOMPAS Recidivism Prediction (Criminal Justice):\n\nAlgorithm twice as likely to flag Black defendants as high risk.\nHistorical arrest data reflects biased policing patterns.\n\nDutch Welfare Fraud Detection:\n\n“Black Box” secret system\nDisproportionally targeted vulnerable populations\n\n\n\n\nTerms:\n\nData Science: Computer science/engineering focus on algorithms and methods.\nData Analytics: Application of data science methods to other disciplines.\nMachine Learning: Algorithms for classification and prediction that learn from data.\nAI: Algorithms that adjust and improve across iterations (neural nets, etc.).\n\nPublic Sector context:\n\nLong history of govt data collection:\n\nCivic registration systems\nCensus data\nAdministrative records\nOperations research (post-WWII)\n\nWhat’s new?\n\nMore data (official and “accidental” such as social media data)\nFocus on prediction rather than explanation\nHarder to interpret and explain\n\nWhy govt use algorithms:\n\nGovts have limited budgets and need to serve everyone.\nAlgorithmic decision making is appealing because it promises:\nEfficiency: faster\nConsistency: established methods\nObjectivity: less human bias\nCost savings: less staff\n\n\nData Analytics is subjective:\n\nEvery step involves human choices (embedded values and biases):\n\nData cleaning\nData coding / classification\nData collection - use of imperfect proxies\nHow you interpret results\nWhat variables you use in the model\n\n\nScenario Example: Housing assistance allocation\n\nProxy: median household income\nBlind spots:\n\nLower income communities: median is a central measure.\nHigh rent/demand communities (NYC): median is a central measure.\n\nHarm + Fixes:\n\nUpper bound for hh income (well under city median income) to provide more housing assistance.\nNeighborhood housing price scale (divide income by housing price) to see how well off a hh is compared to the relative neighborhood.\nGuardrail:\n\nRent control\nHousing stipends / upper bounded interest rates\n\n\n\nCENSUS:\n\nFoundation for:\n\nUnderstanding community demographics\nAllocating government resources\nTracking neighborhood change\nDesigning fair algorithms (like those we just discussed)\n\nDecennial Census (2020)\n\nEveryone counted every 10 years (full population)\n9 basic questions: age, race, sex, housing\nConstitutional requirement\nDetermines political representation\n\n\nAmerican Community Survey (ACS):\n\nWhat it is:\n\n3% of households surveyed annually\nDetailed questions: income, education, employment, housing costs\nReplaced old “long form” in 2005\n\n1-Year estimates (areas &gt; 65K people)\n\nMost current data, smallest sample\nTake with grain of salt (only aggregate levels, not neighborhood)\n\n5-Year estimates (all areas including census tracts)\n\nMost reliable data, largest sample\nKey point: All ACS data comes with margins of error (since samples) - uncertainty\n\n\nCensus Geography Hierarchy:\n\nCounty level: state and regional planning\nCensus tract level: neighborhood analysis (1500 - 8000 people)\nBlock group level: very local analysis (huge MOEs) (600 - 3000)\n\nBlocks: decennial only, 85 people\n\n\n2020 Census Innovation: Differential Privacy\n\nChallenge: Modern computing can re-identify individuals from census data.\nSolution: Add mathematical noise to protect privacy while preserving overall patterns.\nControversy: Some places now show impossible results in populations (ex. living underwater).\nWhy this matters: Even objective data involves subjective choices about privacy vs. accuracy (can cause errors).\n\nMargin of Error (MOE)\n\nLarge MOE relative to estimate = less reliable\nIn analysis:\n\nAlways report MOE alongside estimates.\nBe cautious comparing estimates with overlapping error margins.\nConsider using 5-year estimates for greater reliability\n\nDate intervals can be affected by new advancements during that time.\n\n\n\nTwo Types of Census Data:\n\nSummary Tables (what we’ll use mostly)\n\nPre-calculated statistics by geography\nMedian income, percent college-educated, etc.\nGood for: Mapping, comparing places\n\nPUMS: Individual Records\n\nAnonymous individual/household responses\nGood for: Custom analysis, regression models\nMore complex but more flexible\n\n\nData Sources you’ll use:\n\nTIGER/Line Files\n\nGeographic boundaries (shapefiles)\nCensus tracts, counties, states\nNow released as shapefiles (easier to use!)\n\nHistorical Data Sources:\n\nNHGIS (nhgis.org): Historical census data\nNeighborhood Change Database\nLongitudinal Tract Database: Track changes over time’"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "",
    "text": "Clone Repo Workflow:\n\nNavigate to repository\nClick &lt;&gt; code: green button\nOpen with Github Desktop\nCopy over data, template scripts, etc. over from cloned class repo to personal repo\nOTHER METHOD: Use terminal\n\ncd ~/file location\ngit clone https://github.com/MUSA-5080-Fall-2025/MUSA-5080-Fall-2025.git\n\n\nALGORITHM: Set of rules/instructions for solving a problem or completing a task.\n\nIn Government: Systems used to assist / replace human decision-makers.\n\nBased on predictions from models that process historical data:\nInputs: features/predictors/independent variables/x\nOutputs: labels/outcome/dependent variable/y\nHelps eliminate human bias / subjectivity\nExamples: criminal justice (bail, sentencing), housing and finance (whether or not to rent), healthcare (prioritization, resource allocation)\n\nHealthcare Algorithmic Bias: Algorithm used to identify high-risk patients for additional care systematically discriminated against Black patients.\n\nAlgorithm used healthcare costs as a proxy for need / risk.\nHistorical inequity: Black patients typically incur lower costs due to systematic inequities in access.\nResult: Black patients under-prioritized despite equivalent levels of illness.\nScale: Used by hospitals and insurers for over 200 million people annually.\n\nCOMPAS Recidivism Prediction (Criminal Justice):\n\nAlgorithm twice as likely to flag Black defendants as high risk.\nHistorical arrest data reflects biased policing patterns.\n\nDutch Welfare Fraud Detection:\n\n“Black Box” secret system\nDisproportionally targeted vulnerable populations\n\n\n\n\nTerms:\n\nData Science: Computer science/engineering focus on algorithms and methods.\nData Analytics: Application of data science methods to other disciplines.\nMachine Learning: Algorithms for classification and prediction that learn from data.\nAI: Algorithms that adjust and improve across iterations (neural nets, etc.).\n\nPublic Sector context:\n\nLong history of govt data collection:\n\nCivic registration systems\nCensus data\nAdministrative records\nOperations research (post-WWII)\n\nWhat’s new?\n\nMore data (official and “accidental” such as social media data)\nFocus on prediction rather than explanation\nHarder to interpret and explain\n\nWhy govt use algorithms:\n\nGovts have limited budgets and need to serve everyone.\nAlgorithmic decision making is appealing because it promises:\nEfficiency: faster\nConsistency: established methods\nObjectivity: less human bias\nCost savings: less staff\n\n\nData Analytics is subjective:\n\nEvery step involves human choices (embedded values and biases):\n\nData cleaning\nData coding / classification\nData collection - use of imperfect proxies\nHow you interpret results\nWhat variables you use in the model\n\n\nScenario Example: Housing assistance allocation\n\nProxy: median household income\nBlind spots:\n\nLower income communities: median is a central measure.\nHigh rent/demand communities (NYC): median is a central measure.\n\nHarm + Fixes:\n\nUpper bound for hh income (well under city median income) to provide more housing assistance.\nNeighborhood housing price scale (divide income by housing price) to see how well off a hh is compared to the relative neighborhood.\nGuardrail:\n\nRent control\nHousing stipends / upper bounded interest rates\n\n\n\nCENSUS:\n\nFoundation for:\n\nUnderstanding community demographics\nAllocating government resources\nTracking neighborhood change\nDesigning fair algorithms (like those we just discussed)\n\nDecennial Census (2020)\n\nEveryone counted every 10 years (full population)\n9 basic questions: age, race, sex, housing\nConstitutional requirement\nDetermines political representation\n\n\nAmerican Community Survey (ACS):\n\nWhat it is:\n\n3% of households surveyed annually\nDetailed questions: income, education, employment, housing costs\nReplaced old “long form” in 2005\n\n1-Year estimates (areas &gt; 65K people)\n\nMost current data, smallest sample\nTake with grain of salt (only aggregate levels, not neighborhood)\n\n5-Year estimates (all areas including census tracts)\n\nMost reliable data, largest sample\nKey point: All ACS data comes with margins of error (since samples) - uncertainty\n\n\nCensus Geography Hierarchy:\n\nCounty level: state and regional planning\nCensus tract level: neighborhood analysis (1500 - 8000 people)\nBlock group level: very local analysis (huge MOEs) (600 - 3000)\n\nBlocks: decennial only, 85 people\n\n\n2020 Census Innovation: Differential Privacy\n\nChallenge: Modern computing can re-identify individuals from census data.\nSolution: Add mathematical noise to protect privacy while preserving overall patterns.\nControversy: Some places now show impossible results in populations (ex. living underwater).\nWhy this matters: Even objective data involves subjective choices about privacy vs. accuracy (can cause errors).\n\nMargin of Error (MOE)\n\nLarge MOE relative to estimate = less reliable\nIn analysis:\n\nAlways report MOE alongside estimates.\nBe cautious comparing estimates with overlapping error margins.\nConsider using 5-year estimates for greater reliability\n\nDate intervals can be affected by new advancements during that time.\n\n\n\nTwo Types of Census Data:\n\nSummary Tables (what we’ll use mostly)\n\nPre-calculated statistics by geography\nMedian income, percent college-educated, etc.\nGood for: Mapping, comparing places\n\nPUMS: Individual Records\n\nAnonymous individual/household responses\nGood for: Custom analysis, regression models\nMore complex but more flexible\n\n\nData Sources you’ll use:\n\nTIGER/Line Files\n\nGeographic boundaries (shapefiles)\nCensus tracts, counties, states\nNow released as shapefiles (easier to use!)\n\nHistorical Data Sources:\n\nNHGIS (nhgis.org): Historical census data\nNeighborhood Change Database\nLongitudinal Tract Database: Track changes over time’"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nnew dplyr functions:\n\nglimpse(): brief overview of df (num rows/cols, col name/type, some row examples)\ncolnames(): column names of df\nbetween(x, lower bound, upper bound): use w/ case_when + mutate\nfilter(): Joint string conditions\n\nfilter(Manufacturer %in% c(“Honda”, “Nissan”))\n\nPiping: |&gt; chain together dplyr commands on a single dataframe\n\nAccessing Census Data in R:\n\nModern approach: use R packages to access data directly.\n\nAlways get latest data\nReproducible workflows\nAutomatic geographic boundaries\nBuilt-in error handling\n\nUse tidycensus package\nTable organization:\n\nB19013: Median Household Income\nB25003: Housing Tenure (Own/Rent)\nB15003: Educational Attainment\nB08301: Commuting to Work\n\nVariable Examples: E (Estimate), M (Margin of Error)\n\nB19013_001E = Median household income (estimate)\nB19013_001M = Median household income (margin of error)\n\n\nSAMPLE CODE:\n\ncensus_api_key(): access data\nget_acs(geography, variables, year, state, survey, output): get the data into file\nstr_remove(var, str_to_remove): remove substrings of original string in column"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nThe examples provided context to how we use data science skills and techniques to apply in a real-world setting.\nThe conversations around algorithmic biases and biased data is important to consider when making policy considerations. It is extremely important to understand our data and recognize flaws in algorithms that can perpetuate bias, which usually harm marginalized groups of people.\nThe Census data provides valuable information (ACS) for us to analyze with data-driven methods in formulating policies that solve problems in bettering communities."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "Reflection",
    "text": "Reflection\n\nI had not had a formal introduction to Census data before. Sometimes, working with assigned datasets in class overlook a key aspect being data collection, which is a key decision in framing an analytics problem with human choice.\nI think that it is very valuable to go over biases in algorithms and data. One saying that I was taught is “Data is never neutral”, highlighting that there are many subjective human decisions that go into creating data-driven solutions. It is crucial that we recognize these risks in our problem-solving process to mitigate risk of biases in our policy decisions to protect at-risk groups of people."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 - Course Introduction",
    "section": "",
    "text": "Upcoming assignments: lab0: 9/15 create Quatro portfolio; lab1: 9/27 EDA\nGit: Version-control system - tracks changes in files\n\nCollaboration tool for people teams to work\n\nGithub: cloud host for Git repos (projects)\n\nBackup work in cloud\nShare / collaborate on projects with other\nGithub pages hosts websites\n\nGithub Concepts:\n\nRepo: folder containing proj files\nCommit: snapshot of work at point in time\nPush: Send changes to Github cloud\nPull: Get latest changes from Github cloud\n\nWorkflow each week:\n\nEdit files in RStudio (local)\nCommit changes w/ message\nPush to Github\nPortfolio propogates changes automatically\n\nGithub Classroom:\n\nCreates individual repos for each student\nDistributes assignments automatically\nTAs can provide feedback\n\nLists syntax\nLinks / Images syntax"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 - Course Introduction",
    "section": "",
    "text": "Upcoming assignments: lab0: 9/15 create Quatro portfolio; lab1: 9/27 EDA\nGit: Version-control system - tracks changes in files\n\nCollaboration tool for people teams to work\n\nGithub: cloud host for Git repos (projects)\n\nBackup work in cloud\nShare / collaborate on projects with other\nGithub pages hosts websites\n\nGithub Concepts:\n\nRepo: folder containing proj files\nCommit: snapshot of work at point in time\nPush: Send changes to Github cloud\nPull: Get latest changes from Github cloud\n\nWorkflow each week:\n\nEdit files in RStudio (local)\nCommit changes w/ message\nPush to Github\nPortfolio propogates changes automatically\n\nGithub Classroom:\n\nCreates individual repos for each student\nDistributes assignments automatically\nTAs can provide feedback\n\nLists syntax\nLinks / Images syntax"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nR (open software - people create packages free to use) and dplyr:\n\ntidyverse: Data Science packages\n\nConsistent syntax across functions\nReadable code tells story\nEfficient workflows for common tasks\n\nTibbles: enhanced data frames\n\nTraditional df: class(data)\nTibble: car_data &lt;- as_tibble(data)\nPretty print: Shows ONLY first 10 rows, displays cleaner than traditional df\nread_csv (read.csv for traditional df)\n\nEssential dplyr functions (data cleaning):\nselect(): choose columns - select(df, col1, col2)\nfilter(): choose rows - filter(df, condition)\n\nUse &, | symbol to join conditions (and/or)\n\nmutate(): create new variables - mutate(df, new_col = old_col / 1000)\n\nCan use case_when (if statement) argument: ex) car_df = mutate(car_df, case_when(age &gt; 20 ~ “old”, TRUE ~ “not old”))\n\nsummarize(): calculate stats\ngroup_by(): operate on groups (usually goes together w/ summarize())\n\ngroup_by(col1) |&gt; summarize (n = n()) |&gt; mutate(freq = n / sum(n)) -&gt; calculates proportions (frequencies) for groups\n\nMust assign back to original or new df name to save the operation (always start with dataframe operating on in syntax)\nOthers:\n\nnames(): column names\nglimpse(): rows, columnns, some entries etc.\nClick on df to view in table format\nRename(): renames column: rename(df, new = old)\nRemove a column: select(df, -col)\n\n%&gt;% or |&gt;: Pipes -&gt; combines lines of codes, pass outputs through each pipe\n\nQuarto (better version of R Markdown) - publishing system that combines:\n\nCode (R, Python, etc.), Text (explanations, analysis, etc.), Output (plots, tables results, etc.)\nBenefits:\n\nReproducible research: Ccde and explanation in one place, others can re-run analysis, professional presentation\n\nComponents:\n\nYAML header: title, author, date, format\nR code chunks: load library, read_csv, etc.\nText (formatting: bold, italic, etc.)"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nWe learned how to clean data, transforming it to drive actionable insights. Datasets can be messy, and by learning these skills, we build foundational knowledge on how we can improve the quality of data for future analyses.\nWe learned how to create a Quarto portfolio as a professional way to organize and present our work. This is highly valuable for making our findings professional, clean, and easily accessible to peers, future employers, etc."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nI found the Quarto portfolio the most fascinating part of this lecture. It displays our work in a much cleaner format than in R-Markdown.\nI will be using this portfolio to highlight the work in this course. Additionally, I can use skills in making a Quatro portfolio for personal projects as well!"
  }
]