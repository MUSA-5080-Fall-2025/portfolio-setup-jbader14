[
  {
    "objectID": "weekly-notes/week-09-notes.html",
    "href": "weekly-notes/week-09-notes.html",
    "title": "Week 9 Notes - Logistic Regression: The Case of Recidivism",
    "section": "",
    "text": "Logistic Regression: Predicting binary outcomes\n\nPredict the probability of Y, not Y directly\nNeed 0-1 bounds\n\nLogit Transformation: Behind the scenes, work with log-odds\n\nLog-Odds: Logit(p) = ln (p / 1-p)\nCreates linear relationship: Logit(p) = B0 + B1*x1 ‚Ä¶\nCoefs are log odds\nInterpret as odds ratio\n\n1: predictor increases odds of outcome\n\n\nClassify threshold to split on classification: Diff risks\nConfusion Matrix (TP, FP, FN, TN)\nMetrics to evaluate: Sensitivity (Recall, TPR), Specificity (TNR), Precision\nROC curve: Receiver Operating Characteristic\n\nEvery possible threshold\nTrade off between TPR and FPR\nOverall model discrimination ability"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-09-notes.html#key-concepts-learned",
    "title": "Week 9 Notes - Logistic Regression: The Case of Recidivism",
    "section": "",
    "text": "Logistic Regression: Predicting binary outcomes\n\nPredict the probability of Y, not Y directly\nNeed 0-1 bounds\n\nLogit Transformation: Behind the scenes, work with log-odds\n\nLog-Odds: Logit(p) = ln (p / 1-p)\nCreates linear relationship: Logit(p) = B0 + B1*x1 ‚Ä¶\nCoefs are log odds\nInterpret as odds ratio\n\n1: predictor increases odds of outcome\n\n\nClassify threshold to split on classification: Diff risks\nConfusion Matrix (TP, FP, FN, TN)\nMetrics to evaluate: Sensitivity (Recall, TPR), Specificity (TNR), Precision\nROC curve: Receiver Operating Characteristic\n\nEvery possible threshold\nTrade off between TPR and FPR\nOverall model discrimination ability"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#coding-techniques",
    "href": "weekly-notes/week-09-notes.html#coding-techniques",
    "title": "Week 9 Notes - Logistic Regression: The Case of Recidivism",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nLogit Model: Use glm() with family = ‚Äòbinomial‚Äô\n\nexp() convert to odds ratio\ncoef() interpret coefs\npredict(): make predictions\n\nPlotting:\n\nconfusionMatrix()\nggroc()"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#questions-challenges",
    "href": "weekly-notes/week-09-notes.html#questions-challenges",
    "title": "Week 9 Notes - Logistic Regression: The Case of Recidivism",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#connections-to-policy",
    "href": "weekly-notes/week-09-notes.html#connections-to-policy",
    "title": "Week 9 Notes - Logistic Regression: The Case of Recidivism",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nLogistic regression is very important to policy markers for supporting algorithmic decision making for binary outcomes."
  },
  {
    "objectID": "weekly-notes/week-09-notes.html#reflection",
    "href": "weekly-notes/week-09-notes.html#reflection",
    "title": "Week 9 Notes - Logistic Regression: The Case of Recidivism",
    "section": "Reflection",
    "text": "Reflection\n\nLearning these topics in a much more applied setting compared to my Data Science classes has really helped me further grasp the concepts."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html",
    "href": "weekly-notes/week-07-notes.html",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "",
    "text": "Hide progress bars when loading spatial data:\n\nprogress = FALSE inside get_acs\nOR: options(tigris_progress = FALSE)\n\nGood (random) errors:\n\nNo systematic pattern\nScattered across space\nPrediction equally good everywhere\nModel captures key relationships\n\nBad (clustered) errors:\n\nSpatial pattern visible\nUnder / over predict in areas\nModel misses something about location\nNeed more spatial features\n\nLook for spatial autocorrelation in the errors\nMoran‚Äôs I: [-1, 1]\n\n1: Perfect spatial correlation (clustering)\n0: Random spatial pattern\n(-1): Perfect negative correlation (dispersion)\nIf high: (diff buffer sizes, more amenities, neighborhood specific vars)\n\n\nAdd more spatial features\n\n\nTry spatial fixed effects (neighborhood or grid cell dummies)\n\n\nConsider spatial regression models (spatial lag or error models)\n\n\nNOT for this class.\n\n\n\nDefining ‚Äúneighbors‚Äù\n\nContiguity:\n\nPolygons that share a border\n\nDistance:\n\nAll within X meters\nFixed threshold\n\nk-Nearest:\n\nClosest k points\nAdaptive distance\n\n\nSpatial lag = avg value of neighbors (see slide for syntax)"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-07-notes.html#key-concepts-learned",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "",
    "text": "Hide progress bars when loading spatial data:\n\nprogress = FALSE inside get_acs\nOR: options(tigris_progress = FALSE)\n\nGood (random) errors:\n\nNo systematic pattern\nScattered across space\nPrediction equally good everywhere\nModel captures key relationships\n\nBad (clustered) errors:\n\nSpatial pattern visible\nUnder / over predict in areas\nModel misses something about location\nNeed more spatial features\n\nLook for spatial autocorrelation in the errors\nMoran‚Äôs I: [-1, 1]\n\n1: Perfect spatial correlation (clustering)\n0: Random spatial pattern\n(-1): Perfect negative correlation (dispersion)\nIf high: (diff buffer sizes, more amenities, neighborhood specific vars)\n\n\nAdd more spatial features\n\n\nTry spatial fixed effects (neighborhood or grid cell dummies)\n\n\nConsider spatial regression models (spatial lag or error models)\n\n\nNOT for this class.\n\n\n\nDefining ‚Äúneighbors‚Äù\n\nContiguity:\n\nPolygons that share a border\n\nDistance:\n\nAll within X meters\nFixed threshold\n\nk-Nearest:\n\nClosest k points\nAdaptive distance\n\n\nSpatial lag = avg value of neighbors (see slide for syntax)"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#coding-techniques",
    "href": "weekly-notes/week-07-notes.html#coding-techniques",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\neval: true -&gt; code chunk actually ran (#| ) before\necho: true -&gt; code chunk is actualy displayed\nspdep library to do spatial analysis like knn, weights, etc. (see slides)"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#questions-challenges",
    "href": "weekly-notes/week-07-notes.html#questions-challenges",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#connections-to-policy",
    "href": "weekly-notes/week-07-notes.html#connections-to-policy",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nIdentifying spatial autocorrelation is very important in understanding what errors are good / bad and how we can fix them to make better models for policy considerations."
  },
  {
    "objectID": "weekly-notes/week-07-notes.html#reflection",
    "href": "weekly-notes/week-07-notes.html#reflection",
    "title": "Week 7 Notes - Model Diagnostics & Spatial Autocorrelation",
    "section": "Reflection",
    "text": "Reflection\n\nI havent‚Äôt learned about Moran‚Äôs I before, but I think it is very useful to use to test for autocorrelaton and ultimately, model performance."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 5 - Intro to Linear Regression",
    "section": "",
    "text": "Statistical learning = a set of approaches for estimating relationship between variables\n\nObserve data like: counties, income, population, education, etc.\n\nResponse / outcome variable Y as a function of predictors X‚Äôs and noise‚Ä¶\n\nùëå=ùëì(ùëã)+ùúñ\nf = the systematic information X provides about Y\nŒµ = random error (irreducible)\n\nf represents the true relationship between predictors and outcome\n\nfixed but unknown\nwhat we want to estimate\nI think this is just the coefficient\n\nWhy estimate f?\n\n\nPrediction:\n\n\nEstimate Y for new observations\nDon‚Äôt necessarily care about the exact form of f\nPrediction intervals (confidence) matter\nDon‚Äôt need to understand why\nFocus: accuracy of predictions\n\n\nInference\n\n\nUnderstand how X affects Y (mechanisms)\nStatistical significance matters\nWhich predictors matter?\nWhat‚Äôs the nature of the relationship?\nFocus: interpreting the model / coefs\n\nintercept (usually not that useful), slope (coef), p-value (reject null / fail to reject)\n\n\n\nHow do we estimate f?\n\nParametric Methods\n\nMake an assumption about the functional form (e.g., linear)\nReduces problem to estimating a few parameters\nEasier to interpret\nThis is what we‚Äôll focus on\nMethod: OLS\n\nNon-Parametric Methods\n\nDon‚Äôt assume a specific form\nMore flexible\nRequire more data\nHarder to interpret\n\nKey Diff:\n\nParametric: We assume f is linear, then estimate Œ≤‚ÇÄ and Œ≤‚ÇÅ\nNon-Parametric: We let the data determine the shape of f.\n\n\nWhy linear regression?\n\nAdvantages:\n\nSimple and interpretable\nWell-understood properties\nWorks remarkably well for many problems\nFoundation for more complex methods\n\nLimitations:\n\nAssumes linearity\nSensitive to outliers\nMakes several assumptions\n\n\nStatistical Significance:\n\nNull hypothesis (H‚ÇÄ): Œ≤‚ÇÅ = 0 (no relationship)\nOur estimate: Œ≤‚ÇÅ = 0.02\nQuestion: Could we get 0.02 just by chance if H‚ÇÄ is true?\nt-statistic: How many standard errors away from 0?\n\nBigger |t| = more confidence the relationship is real\n\np-value: Probability of seeing our estimate if H‚ÇÄ is true\n\nSmall p ‚Üí reject H‚ÇÄ, conclude relationship exists\n\n\nModel Evaluation:\n\nHow well does it fit the data we used? (in sample fit)\n\nR-squared: _ % of variation in income is explained by population.\n\nFor prediction: Pop is moderately good predictor of income.\nFor inference: Pop matters, but other factors exist.\nR-squared alone does not tell us if the model is trustworthy (overfit)\n\n\nHow well would it predict new data? (out of sample performance)\n\nPROBLEM:\n\nUnderfitting: Model too simple (high bias)\nGood fit: Captures pattern without noise\nOverfitting: Memorizes training data (high variance)\n\nCross-Validation: Multiple train-test splits\n\nGives more stable estimate of true prediction performance\n\nLinear regression makes assumptions. If violated:\n\nCoefficients may be biased\nStandard errors wrong\nPredictions unreliable\n\nImproving the model:\n\nAdd more predictors\nLog transform predictors\nCategorical variables\n\nRegression workflow:\n\nUnderstand the framework: What‚Äôs f? What‚Äôs the goal?\nVisualize first: Does a linear model make sense?\nFit the model: Estimate coefficients\nEvaluate performance: Train/test split, cross-validation\nCheck assumptions: Residual plots, VIF, outliers\nImprove if needed: Transformations, more variables\nConsider ethics: Who could be harmed by this model?"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "title": "Week 5 - Intro to Linear Regression",
    "section": "",
    "text": "Statistical learning = a set of approaches for estimating relationship between variables\n\nObserve data like: counties, income, population, education, etc.\n\nResponse / outcome variable Y as a function of predictors X‚Äôs and noise‚Ä¶\n\nùëå=ùëì(ùëã)+ùúñ\nf = the systematic information X provides about Y\nŒµ = random error (irreducible)\n\nf represents the true relationship between predictors and outcome\n\nfixed but unknown\nwhat we want to estimate\nI think this is just the coefficient\n\nWhy estimate f?\n\n\nPrediction:\n\n\nEstimate Y for new observations\nDon‚Äôt necessarily care about the exact form of f\nPrediction intervals (confidence) matter\nDon‚Äôt need to understand why\nFocus: accuracy of predictions\n\n\nInference\n\n\nUnderstand how X affects Y (mechanisms)\nStatistical significance matters\nWhich predictors matter?\nWhat‚Äôs the nature of the relationship?\nFocus: interpreting the model / coefs\n\nintercept (usually not that useful), slope (coef), p-value (reject null / fail to reject)\n\n\n\nHow do we estimate f?\n\nParametric Methods\n\nMake an assumption about the functional form (e.g., linear)\nReduces problem to estimating a few parameters\nEasier to interpret\nThis is what we‚Äôll focus on\nMethod: OLS\n\nNon-Parametric Methods\n\nDon‚Äôt assume a specific form\nMore flexible\nRequire more data\nHarder to interpret\n\nKey Diff:\n\nParametric: We assume f is linear, then estimate Œ≤‚ÇÄ and Œ≤‚ÇÅ\nNon-Parametric: We let the data determine the shape of f.\n\n\nWhy linear regression?\n\nAdvantages:\n\nSimple and interpretable\nWell-understood properties\nWorks remarkably well for many problems\nFoundation for more complex methods\n\nLimitations:\n\nAssumes linearity\nSensitive to outliers\nMakes several assumptions\n\n\nStatistical Significance:\n\nNull hypothesis (H‚ÇÄ): Œ≤‚ÇÅ = 0 (no relationship)\nOur estimate: Œ≤‚ÇÅ = 0.02\nQuestion: Could we get 0.02 just by chance if H‚ÇÄ is true?\nt-statistic: How many standard errors away from 0?\n\nBigger |t| = more confidence the relationship is real\n\np-value: Probability of seeing our estimate if H‚ÇÄ is true\n\nSmall p ‚Üí reject H‚ÇÄ, conclude relationship exists\n\n\nModel Evaluation:\n\nHow well does it fit the data we used? (in sample fit)\n\nR-squared: _ % of variation in income is explained by population.\n\nFor prediction: Pop is moderately good predictor of income.\nFor inference: Pop matters, but other factors exist.\nR-squared alone does not tell us if the model is trustworthy (overfit)\n\n\nHow well would it predict new data? (out of sample performance)\n\nPROBLEM:\n\nUnderfitting: Model too simple (high bias)\nGood fit: Captures pattern without noise\nOverfitting: Memorizes training data (high variance)\n\nCross-Validation: Multiple train-test splits\n\nGives more stable estimate of true prediction performance\n\nLinear regression makes assumptions. If violated:\n\nCoefficients may be biased\nStandard errors wrong\nPredictions unreliable\n\nImproving the model:\n\nAdd more predictors\nLog transform predictors\nCategorical variables\n\nRegression workflow:\n\nUnderstand the framework: What‚Äôs f? What‚Äôs the goal?\nVisualize first: Does a linear model make sense?\nFit the model: Estimate coefficients\nEvaluate performance: Train/test split, cross-validation\nCheck assumptions: Residual plots, VIF, outliers\nImprove if needed: Transformations, more variables\nConsider ethics: Who could be harmed by this model?"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#coding-techniques",
    "href": "weekly-notes/week-05-notes.html#coding-techniques",
    "title": "Week 5 - Intro to Linear Regression",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nFit the model:\n\nlinear reg: model1 &lt;- lm(median_incomeE ~ total_popE, data = pa_data)\nsummary stats: summary(model1)\n\nTrain/Test Split:\n\nset.seed(123)\nn &lt;- nrow(pa_data)\n70% training, 30% testing\ntrain_indices &lt;- sample(1:n, size = 0.7 * n)\ntrain_data &lt;- pa_data[train_indices, ]\ntest_data &lt;- pa_data[-train_indices, ]\nFit on training data only\nmodel_train &lt;- lm(median_incomeE ~ total_popE, data = train_data)\nPredict on test data\ntest_predictions &lt;- predict(model_train, newdata = test_data)\n\nEvaluate Prediction:\n\nCalculate prediction error (RMSE)\nrmse_test &lt;- sqrt(mean((test_data$median_incomeE - test_predictions)^2))\nrmse_train &lt;- summary(model_train)$sigma\ncat(‚ÄúTraining RMSE:‚Äù, round(rmse_train, 0), ‚Äú‚Äù)\nTraining RMSE: 12893\ncat(‚ÄúTest RMSE:‚Äù, round(rmse_test, 0), ‚Äú‚Äù)\nTest RMSE: 9536\nInterpreting RMSE\nOn new data (test set), our predictions are off by ~$9,500 on average. Is this level of error acceptable for policy decisions?\n\nCross-Validation:\n\nlibrary(caret)\n10-fold cross-validation\ntrain_control &lt;- trainControl(method = ‚Äúcv‚Äù, number = 10)\ncv_model &lt;- train(median_incomeE ~ total_popE, data = pa_data, method = ‚Äúlm‚Äù, trControl = train_control)\ncv_model$results\nintercept RMSE Rsquared MAE RMSESD RsquaredSD MAESD\nTRUE  12577.76 0.5643826 8859.865 5609.002  0.2997098 2238.042\nKey Metrics (Averaged Across 10 Folds)\n\nRMSE: Typical prediction error (~$12,578)\nR¬≤: % of variation explained (0.564)\nMAE: Average absolute error (~$8,860) - easier to interpret\n\n\nAssumption 1: Linearity - Check: Residual Plot\n\npa_data$residuals &lt;- residuals(model1)\npa_data$fitted &lt;- fitted(model1)\nggplot(pa_data, aes(x = fitted, y = residuals)) +\ngeom_point() +\ngeom_hline(yintercept = 0, color = ‚Äúred‚Äù, linetype = ‚Äúdashed‚Äù) +\nlabs(title = ‚ÄúResidual Plot‚Äù, x = ‚ÄúFitted Values‚Äù, y = ‚ÄúResiduals‚Äù) +\ntheme_minimal()\n\nReading residual plots:\n\nGood: Random scatter, points around zero, constant spread\nBad: Curved pattern, model missing something, predictions biased\nLinearity violations hurt predictions, not just inference:\n\nIf the true relationship is curved and you fit a straight line, you‚Äôll systematically underpredict in some regions and overpredict in others\nBiased predictions in predictable ways (not random errors!)\nResidual plots should show random scatter - any pattern means your model is missing something systematic\n\n\nAssumption 2: Constant Variance - Check: Residual Plot\n\nHeteroskedasticity: Variance changes across X\nImpact; Standard errors are wrong -&gt; misleading p-values\nOften a symptom of model misspecification:\n\nModel fits well for some values (e.g., small counties) but poorly for others (large counties)\nMay indicate missing variables that matter more at certain X values\nAsk: ‚ÄúWhat‚Äôs different about observations with large residuals?‚Äù\n\nFormal test: Breusch-Pagan\n\nlibrary(lmtest)\nbptest(model1)\np &gt; 0.05: Constant variance assumption OK\np &lt; 0.05: Evidence of heteroscedasticity\n\nIf detected, solutions:\n\nTransform Y (try log(income))\nRobust standard errors\nAdd missing variables\nAccept it (point predictions still OK for prediction goals)\n\n\nAsssumption 3: Normality of Residuals - Check: Q-Q Plot\n\nLess critical for point predictions (unbiased regardless)\nImportant for confidence intervals and prediction intervals\nNeeded for valid hypothesis tests (t-tests, F-tests)\n\nAssumption 4: No Multicollinearity - VIF\n\nX‚Äôs shouldn‚Äôt be correlated w/ one another\nCoefs can become unstable, harder to interpret\nlibrary(car)\nvif(model1) # Variance Inflation Factor\nRule of thumb: VIF &gt; 10 suggests problems\nNot relevant with only 1 predictor!\n\nAssumption 5: No influential outliers:\n\nNot all outliers are problems - only those with high leverage AND large residuals\n\nHigh leverage + large residual = pulls regression line\n\nCheck w/ visual diagnostic / Identify influential points (z-score)\nWhat to do with influential outliers:\n\nInvestigate: Why is this observation unusual? (data error? truly unique?)\nReport: Always note influential observations in your analysis\nSensitivity check: Refit model without them - do conclusions change?\nDon‚Äôt automatically remove: They might represent real, important cases\nFor policy: An influential county might need special attention, not exclusion!"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#questions-challenges",
    "href": "weekly-notes/week-05-notes.html#questions-challenges",
    "title": "Week 5 - Intro to Linear Regression",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#connections-to-policy",
    "href": "weekly-notes/week-05-notes.html#connections-to-policy",
    "title": "Week 5 - Intro to Linear Regression",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nThere are a lot of human choices in modeling, which influences the conclusions we draw to make policy decisions. It is important to know how to verify assumptions, improve modeling, etc. to make for better policy considerations."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#reflection",
    "href": "weekly-notes/week-05-notes.html#reflection",
    "title": "Week 5 - Intro to Linear Regression",
    "section": "Reflection",
    "text": "Reflection\n\nI have covered regression many times, but this was a good refresher especially on all of the assumptions that we should verify and methods to do so."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3: EDA & ggplot2",
    "section": "",
    "text": "EDA:\n\nCreate summary statistics table\n\nBut can hide underlying data distribution\n\nOutliers, non-linear relationships\n\nSo data visualization is important\nCan have same mean, var, cov, regression but diff patterns in visualization\n\nBad / Misleading data visualizations\n\nMisleading scales or axes\nCherry-picked time periods\nHidden or ignored uncertainty\nMissing context about data reliability\nResult: Poor policy decisions\n\nGoal: Understand your data before making decisions or building models\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\nWhat‚Äôs unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?\n\nWorkflow w/ Data Quality Focus: Enhanced process for policy analysis\n\nLoad and inspect - dimensions, variable types, missing data\nAssess reliability - examine margins of error, calculate coefficients of variation\nVisualize distributions - histograms, boxplots for each variable\nExplore relationships - scatter plots, correlations\nIdentify patterns - grouping, clustering, geographical patterns\nQuestion anomalies - investigate outliers and unusual patterns\nDocument limitations - prepare honest communication about data quality\n\nJOINS - join predicate does not need to be same name, NEED SAME TYPE\n\nleft (most-common) / right: keeps everything from left/right\nfull: keeps everything from both columns\ninner: keeps only matches"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "title": "Week 3: EDA & ggplot2",
    "section": "",
    "text": "EDA:\n\nCreate summary statistics table\n\nBut can hide underlying data distribution\n\nOutliers, non-linear relationships\n\nSo data visualization is important\nCan have same mean, var, cov, regression but diff patterns in visualization\n\nBad / Misleading data visualizations\n\nMisleading scales or axes\nCherry-picked time periods\nHidden or ignored uncertainty\nMissing context about data reliability\nResult: Poor policy decisions\n\nGoal: Understand your data before making decisions or building models\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\nWhat‚Äôs unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?\n\nWorkflow w/ Data Quality Focus: Enhanced process for policy analysis\n\nLoad and inspect - dimensions, variable types, missing data\nAssess reliability - examine margins of error, calculate coefficients of variation\nVisualize distributions - histograms, boxplots for each variable\nExplore relationships - scatter plots, correlations\nIdentify patterns - grouping, clustering, geographical patterns\nQuestion anomalies - investigate outliers and unusual patterns\nDocument limitations - prepare honest communication about data quality\n\nJOINS - join predicate does not need to be same name, NEED SAME TYPE\n\nleft (most-common) / right: keeps everything from left/right\nfull: keeps everything from both columns\ninner: keeps only matches"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03-notes.html#coding-techniques",
    "title": "Week 3: EDA & ggplot2",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nggplot2\n\nData -&gt; Aesthetics -&gt; Geometry -&gt; Visual\n\nData: Your dataset (census data, survey responses, etc.)\nAesthetics: What variables map to visual properties (x, y, color, size)\nGeometries: How to display the data (points, bars, lines)\nAdditional layers: Scales, themes, facets, annotations\nGeneral syntax: ggplot(data = your_data) + aes(x = variable1, y = variable2) + geom_something() + additional_layers()\n\nAesthetic Mappings: maps data to visual properties\n\nx, y - position\ncolor - point/line color\nfill - area fill color\nsize - point/line size\nshape - point shape\nalpha - transparency\n\nstr_detect(), str_extract(), regex() to search within strings"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3: EDA & ggplot2",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3: EDA & ggplot2",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nLearning how to make good (and not misleading) plots is crucial in conveying information to less data-savvy people who may not understand advanced statistical terms.\nEDA is extremely important in learning more about your data before operating on it and then later drawing analysis from for policy considerations."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3: EDA & ggplot2",
    "section": "Reflection",
    "text": "Reflection\n\nI think that these topics are very important to learn to be a good analyst. Being prepared and having a good understanding of your data is a crucial skill to have and plotting allows you to easily convey your findings to your peers."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 - Course Introduction",
    "section": "",
    "text": "Upcoming assignments: lab0: 9/15 create Quatro portfolio; lab1: 9/27 EDA\nGit: Version-control system - tracks changes in files\n\nCollaboration tool for people teams to work\n\nGithub: cloud host for Git repos (projects)\n\nBackup work in cloud\nShare / collaborate on projects with other\nGithub pages hosts websites\n\nGithub Concepts:\n\nRepo: folder containing proj files\nCommit: snapshot of work at point in time\nPush: Send changes to Github cloud\nPull: Get latest changes from Github cloud\n\nWorkflow each week:\n\nEdit files in RStudio (local)\nCommit changes w/ message\nPush to Github\nPortfolio propogates changes automatically\n\nGithub Classroom:\n\nCreates individual repos for each student\nDistributes assignments automatically\nTAs can provide feedback\n\nLists syntax\nLinks / Images syntax"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 - Course Introduction",
    "section": "",
    "text": "Upcoming assignments: lab0: 9/15 create Quatro portfolio; lab1: 9/27 EDA\nGit: Version-control system - tracks changes in files\n\nCollaboration tool for people teams to work\n\nGithub: cloud host for Git repos (projects)\n\nBackup work in cloud\nShare / collaborate on projects with other\nGithub pages hosts websites\n\nGithub Concepts:\n\nRepo: folder containing proj files\nCommit: snapshot of work at point in time\nPush: Send changes to Github cloud\nPull: Get latest changes from Github cloud\n\nWorkflow each week:\n\nEdit files in RStudio (local)\nCommit changes w/ message\nPush to Github\nPortfolio propogates changes automatically\n\nGithub Classroom:\n\nCreates individual repos for each student\nDistributes assignments automatically\nTAs can provide feedback\n\nLists syntax\nLinks / Images syntax"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nR (open software - people create packages free to use) and dplyr:\n\ntidyverse: Data Science packages\n\nConsistent syntax across functions\nReadable code tells story\nEfficient workflows for common tasks\n\nTibbles: enhanced data frames\n\nTraditional df: class(data)\nTibble: car_data &lt;- as_tibble(data)\nPretty print: Shows ONLY first 10 rows, displays cleaner than traditional df\nread_csv (read.csv for traditional df)\n\nEssential dplyr functions (data cleaning):\nselect(): choose columns - select(df, col1, col2)\nfilter(): choose rows - filter(df, condition)\n\nUse &, | symbol to join conditions (and/or)\n\nmutate(): create new variables - mutate(df, new_col = old_col / 1000)\n\nCan use case_when (if statement) argument: ex) car_df = mutate(car_df, case_when(age &gt; 20 ~ ‚Äúold‚Äù, TRUE ~ ‚Äúnot old‚Äù))\n\nsummarize(): calculate stats\ngroup_by(): operate on groups (usually goes together w/ summarize())\n\ngroup_by(col1) |&gt; summarize (n = n()) |&gt; mutate(freq = n / sum(n)) -&gt; calculates proportions (frequencies) for groups\n\nMust assign back to original or new df name to save the operation (always start with dataframe operating on in syntax)\nOthers:\n\nnames(): column names\nglimpse(): rows, columnns, some entries etc.\nClick on df to view in table format\nRename(): renames column: rename(df, new = old)\nRemove a column: select(df, -col)\n\n%&gt;% or |&gt;: Pipes -&gt; combines lines of codes, pass outputs through each pipe\n\nQuarto (better version of R Markdown) - publishing system that combines:\n\nCode (R, Python, etc.), Text (explanations, analysis, etc.), Output (plots, tables results, etc.)\nBenefits:\n\nReproducible research: Ccde and explanation in one place, others can re-run analysis, professional presentation\n\nComponents:\n\nYAML header: title, author, date, format\nR code chunks: load library, read_csv, etc.\nText (formatting: bold, italic, etc.)"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nWe learned how to clean data, transforming it to drive actionable insights. Datasets can be messy, and by learning these skills, we build foundational knowledge on how we can improve the quality of data for future analyses.\nWe learned how to create a Quarto portfolio as a professional way to organize and present our work. This is highly valuable for making our findings professional, clean, and easily accessible to peers, future employers, etc."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nI found the Quarto portfolio the most fascinating part of this lecture. It displays our work in a much cleaner format than in R-Markdown.\nI will be using this portfolio to highlight the work in this course. Additionally, I can use skills in making a Quatro portfolio for personal projects as well!"
  },
  {
    "objectID": "labs/lab0/lab0.html",
    "href": "labs/lab0/lab0.html",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "",
    "text": "Welcome to your first lab! In this (not graded) assignment, you‚Äôll practice the fundamental dplyr operations I overviewed in class using car sales data. This lab will help you get comfortable with:\n\nBasic data exploration\nColumn selection and manipulation\n\nCreating new variables\nFiltering data\nGrouping and summarizing\n\nInstructions: Copy this template into your portfolio repository under a lab_0/ folder, then complete each section with your code and answers. You will write the code under the comment section in each chunk. Be sure to also copy the data folder into your lab_0 folder."
  },
  {
    "objectID": "labs/lab0/lab0.html#data-structure-exploration",
    "href": "labs/lab0/lab0.html#data-structure-exploration",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.1 Data Structure Exploration",
    "text": "1.1 Data Structure Exploration\nExplore the structure of your data and answer these questions:\n\n# Use glimpse() to see the data structure\nglimpse(car_data)\n\nRows: 50,000\nColumns: 7\n$ Manufacturer          &lt;chr&gt; \"Ford\", \"Porsche\", \"Ford\", \"Toyota\", \"VW\", \"Ford‚Ä¶\n$ Model                 &lt;chr&gt; \"Fiesta\", \"718 Cayman\", \"Mondeo\", \"RAV4\", \"Polo\"‚Ä¶\n$ `Engine size`         &lt;dbl&gt; 1.0, 4.0, 1.6, 1.8, 1.0, 1.4, 1.8, 1.4, 1.2, 2.0‚Ä¶\n$ `Fuel type`           &lt;chr&gt; \"Petrol\", \"Petrol\", \"Diesel\", \"Hybrid\", \"Petrol\"‚Ä¶\n$ `Year of manufacture` &lt;dbl&gt; 2002, 2016, 2014, 1988, 2006, 2018, 2010, 2015, ‚Ä¶\n$ Mileage               &lt;dbl&gt; 127300, 57850, 39190, 210814, 127869, 33603, 866‚Ä¶\n$ Price                 &lt;dbl&gt; 3074, 49704, 24072, 1705, 4101, 29204, 14350, 30‚Ä¶\n\n# Check the column names\ncolnames(car_data)\n\n[1] \"Manufacturer\"        \"Model\"               \"Engine size\"        \n[4] \"Fuel type\"           \"Year of manufacture\" \"Mileage\"            \n[7] \"Price\"              \n\n# Look at the first few rows\nhead(car_data)\n\n# A tibble: 6 √ó 7\n  Manufacturer Model     `Engine size` `Fuel type` `Year of manufacture` Mileage\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n1 Ford         Fiesta              1   Petrol                       2002  127300\n2 Porsche      718 Caym‚Ä¶           4   Petrol                       2016   57850\n3 Ford         Mondeo              1.6 Diesel                       2014   39190\n4 Toyota       RAV4                1.8 Hybrid                       1988  210814\n5 VW           Polo                1   Petrol                       2006  127869\n6 Ford         Focus               1.4 Petrol                       2018   33603\n# ‚Ñπ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestions to answer: - How many rows and columns does the dataset have? - What types of variables do you see (numeric, character, etc.)? - Are there any column names that might cause problems? Why?\nYour answers: - Rows: 50,000 - Columns: 7\n- Variable types: Character, double - Problematic names:‚ÄúEngine size‚Äù, ‚ÄúFuel type‚Äù, ‚ÄúYear of manufacture‚Äù are problematic since they have spaces in their names. When we call them, we must wrap these names in quotation marks as shown above."
  },
  {
    "objectID": "labs/lab0/lab0.html#tibble-vs-data-frame",
    "href": "labs/lab0/lab0.html#tibble-vs-data-frame",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.2 Tibble vs Data Frame",
    "text": "1.2 Tibble vs Data Frame\nCompare how tibbles and data frames display:\n\n# Look at the tibble version (what we have)\ncar_data\n\n# A tibble: 50,000 √ó 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay‚Ä¶           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ‚Ñπ 49,990 more rows\n# ‚Ñπ 1 more variable: Price &lt;dbl&gt;\n\n# Convert to regular data frame and display\ncar_df &lt;- as.data.frame(car_data)\nhead(car_df, 50) # Changed for better render\n\n   Manufacturer      Model Engine size Fuel type Year of manufacture Mileage\n1          Ford     Fiesta         1.0    Petrol                2002  127300\n2       Porsche 718 Cayman         4.0    Petrol                2016   57850\n3          Ford     Mondeo         1.6    Diesel                2014   39190\n4        Toyota       RAV4         1.8    Hybrid                1988  210814\n5            VW       Polo         1.0    Petrol                2006  127869\n6          Ford      Focus         1.4    Petrol                2018   33603\n7          Ford     Mondeo         1.8    Diesel                2010   86686\n8        Toyota      Prius         1.4    Hybrid                2015   30663\n9            VW       Polo         1.2    Petrol                2012   73470\n10         Ford      Focus         2.0    Diesel                1992  262514\n11           VW       Golf         2.0    Diesel                2014   83047\n12          BMW         Z4         2.0    Petrol                1990  293666\n13           VW       Golf         1.2    Diesel                2007   92697\n14       Toyota       RAV4         2.2    Petrol                2007   79393\n15       Toyota      Yaris         1.4    Petrol                1998   97286\n16           VW       Golf         1.6    Diesel                1989  222390\n17       Toyota       RAV4         2.4    Hybrid                2003  117425\n18       Toyota      Yaris         1.2    Petrol                1992  245990\n19       Toyota       RAV4         2.0    Hybrid                2018   28381\n20           VW       Polo         1.2    Petrol                1998  155038\n21           VW       Golf         1.2    Hybrid                1987  121744\n22         Ford     Mondeo         1.6    Diesel                1996   77584\n23       Toyota      Prius         1.0    Hybrid                2003  115291\n24       Toyota      Prius         1.0    Hybrid                1990  238571\n25      Porsche        911         2.6    Petrol                2009   66273\n26       Toyota      Prius         1.8    Hybrid                2017   31958\n27      Porsche        911         3.5    Petrol                2005  151556\n28       Toyota      Yaris         1.2    Petrol                2002  179097\n29           VW       Golf         2.0    Petrol                2020   18985\n30       Toyota       RAV4         1.8    Hybrid                2002   66990\n31         Ford      Focus         1.0    Hybrid                2010   85131\n32         Ford     Fiesta         1.0    Petrol                2001  144731\n33           VW       Polo         2.0    Diesel                2008   97001\n34           VW       Polo         1.6    Petrol                2016   52409\n35         Ford     Fiesta         1.4    Petrol                2010  112714\n36           VW     Passat         2.0    Diesel                1992  198540\n37           VW     Passat         1.8    Diesel                1989  213162\n38          BMW         Z4         2.2    Petrol                2005  133174\n39           VW       Polo         1.6    Petrol                2008  129588\n40          BMW         Z4         2.0    Petrol                1990  148586\n41         Ford      Focus         2.0    Petrol                1995   91173\n42          BMW         M5         4.0    Petrol                2017   22759\n43           VW       Polo         1.4    Petrol                1990  261526\n44           VW     Passat         1.4    Diesel                1995  235594\n45          BMW         Z4         2.0    Petrol                1995   42759\n46           VW     Passat         1.8    Diesel                2005  103352\n47           VW     Passat         1.6    Petrol                2003   63372\n48       Toyota      Yaris         1.0    Petrol                2009   51787\n49         Ford     Fiesta         1.4    Petrol                2014   41495\n50       Toyota      Yaris         1.4    Petrol                2010   43111\n   Price\n1   3074\n2  49704\n3  24072\n4   1705\n5   4101\n6  29204\n7  14350\n8  30297\n9   9977\n10  1049\n11 17173\n12   719\n13  7792\n14 16026\n15  4046\n16   933\n17 11667\n18   720\n19 52671\n20  2118\n21  1890\n22  5667\n23  6512\n24   961\n25 41963\n26 38961\n27 19747\n28  2548\n29 36387\n30 13553\n31 12472\n32  2503\n33  8784\n34 17257\n35  6936\n36  1964\n37  1340\n38  8511\n39  5848\n40  2637\n41  5181\n42 97758\n43   522\n44  1439\n45  7873\n46  9633\n47 10001\n48  9689\n49 14721\n50 12928\n\n\nQuestion: What differences do you notice in how they print?\nYour answer: Tibble will only display the first 10 rows with total number of rows and columns as well as column types as additional info. It also only shows a certain number of columns (in this case, only 6 of 7 columns displayed)."
  },
  {
    "objectID": "labs/lab0/lab0.html#selecting-columns",
    "href": "labs/lab0/lab0.html#selecting-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.1 Selecting Columns",
    "text": "2.1 Selecting Columns\nPractice selecting different combinations of columns:\n\n# Select just Model and Mileage columns\nmodel_mile &lt;- select(car_data, Model, Mileage)\nhead(model_mile, 5)\n\n# A tibble: 5 √ó 2\n  Model      Mileage\n  &lt;chr&gt;        &lt;dbl&gt;\n1 Fiesta      127300\n2 718 Cayman   57850\n3 Mondeo       39190\n4 RAV4        210814\n5 Polo        127869\n\n# Select Manufacturer, Price, and Fuel type\nmanuf_price_fuel &lt;- select(car_data, Manufacturer, Price, \"Fuel type\")\nhead(manuf_price_fuel, 5)\n\n# A tibble: 5 √ó 3\n  Manufacturer Price `Fuel type`\n  &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;      \n1 Ford          3074 Petrol     \n2 Porsche      49704 Petrol     \n3 Ford         24072 Diesel     \n4 Toyota        1705 Hybrid     \n5 VW            4101 Petrol     \n\n# Challenge: Select all columns EXCEPT Engine Size\nexcept_engine &lt;- select(car_data, -\"Engine size\")\nhead(except_engine, 5)\n\n# A tibble: 5 √ó 6\n  Manufacturer Model      `Fuel type` `Year of manufacture` Mileage Price\n  &lt;chr&gt;        &lt;chr&gt;      &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;\n1 Ford         Fiesta     Petrol                       2002  127300  3074\n2 Porsche      718 Cayman Petrol                       2016   57850 49704\n3 Ford         Mondeo     Diesel                       2014   39190 24072\n4 Toyota       RAV4       Hybrid                       1988  210814  1705\n5 VW           Polo       Petrol                       2006  127869  4101"
  },
  {
    "objectID": "labs/lab0/lab0.html#renaming-columns",
    "href": "labs/lab0/lab0.html#renaming-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.2 Renaming Columns",
    "text": "2.2 Renaming Columns\nLet‚Äôs fix a problematic column name:\n\n# Rename 'Year of manufacture' to year\ncar_data &lt;- car_data |&gt;\n    rename(\n    year = `Year of manufacture`,\n    engine_size = `Engine size`,\n    fuel = `Fuel type`\n  )\n\n# Check that it worked\nnames(car_data)\n\n[1] \"Manufacturer\" \"Model\"        \"engine_size\"  \"fuel\"         \"year\"        \n[6] \"Mileage\"      \"Price\"       \n\n\nQuestion: Why did we need backticks around Year of manufacture but not around year?\nYour answer: Because ‚Äòyear‚Äô has no spaces in its variable name."
  },
  {
    "objectID": "labs/lab0/lab0.html#calculate-car-age",
    "href": "labs/lab0/lab0.html#calculate-car-age",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.1 Calculate Car Age",
    "text": "3.1 Calculate Car Age\n\n# Create an 'age' column (2025 minus year of manufacture)\n# Create a mileage_per_year column \ncar_data &lt;- car_data |&gt;\n  mutate(age = 2025 - year,\n         mileage_per_year = Mileage / age)\n\n# Look at your new columns\nselect(car_data, Model, year, age, Mileage, mileage_per_year)\n\n# A tibble: 50,000 √ó 5\n   Model       year   age Mileage mileage_per_year\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1 Fiesta      2002    23  127300            5535.\n 2 718 Cayman  2016     9   57850            6428.\n 3 Mondeo      2014    11   39190            3563.\n 4 RAV4        1988    37  210814            5698.\n 5 Polo        2006    19  127869            6730.\n 6 Focus       2018     7   33603            4800.\n 7 Mondeo      2010    15   86686            5779.\n 8 Prius       2015    10   30663            3066.\n 9 Polo        2012    13   73470            5652.\n10 Focus       1992    33  262514            7955.\n# ‚Ñπ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/lab0.html#categorize-cars",
    "href": "labs/lab0/lab0.html#categorize-cars",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.2 Categorize Cars",
    "text": "3.2 Categorize Cars\n\n# Create a price_category column where if price is &lt; 15000, its is coded as budget, between 15000 and 30000 is midrange and greater than 30000 is mid-range (use case_when)\ncar_data &lt;- car_data |&gt;\n  mutate(price_category = case_when(\n    Price &lt; 15000 ~ \"budget\", \n    between(Price, 15000, 30000) ~ \"midrange\",\n    TRUE ~ \"luxury\"))\n\n# Check your categories select the new column and show it\nselect(car_data, price_category)\n\n# A tibble: 50,000 √ó 1\n   price_category\n   &lt;chr&gt;         \n 1 budget        \n 2 luxury        \n 3 midrange      \n 4 budget        \n 5 budget        \n 6 midrange      \n 7 budget        \n 8 luxury        \n 9 budget        \n10 budget        \n# ‚Ñπ 49,990 more rows"
  },
  {
    "objectID": "labs/lab0/lab0.html#basic-filtering",
    "href": "labs/lab0/lab0.html#basic-filtering",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.1 Basic Filtering",
    "text": "4.1 Basic Filtering\n\n# Find all Toyota cars\ntoyota &lt;- car_data |&gt;\n  filter(Manufacturer == \"Toyota\")\nhead(toyota, 5)\n\n# A tibble: 5 √ó 10\n  Manufacturer Model engine_size fuel    year Mileage Price   age\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Toyota       RAV4          1.8 Hybrid  1988  210814  1705    37\n2 Toyota       Prius         1.4 Hybrid  2015   30663 30297    10\n3 Toyota       RAV4          2.2 Petrol  2007   79393 16026    18\n4 Toyota       Yaris         1.4 Petrol  1998   97286  4046    27\n5 Toyota       RAV4          2.4 Hybrid  2003  117425 11667    22\n# ‚Ñπ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find cars with mileage less than 30,000\nmile_less_30000 &lt;- car_data |&gt;\n  filter(Mileage &lt; 30000)\nhead(mile_less_30000, 5)\n\n# A tibble: 5 √ó 10\n  Manufacturer Model engine_size fuel    year Mileage Price   age\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Toyota       RAV4          2   Hybrid  2018   28381 52671     7\n2 VW           Golf          2   Petrol  2020   18985 36387     5\n3 BMW          M5            4   Petrol  2017   22759 97758     8\n4 Toyota       RAV4          2.4 Petrol  2018   24588 49125     7\n5 VW           Golf          2   Hybrid  2018   25017 36957     7\n# ‚Ñπ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find luxury cars (from price category) with low mileage\n# ASSUMING low mileage is less than 30000 from part 2\nluxury_low_mile &lt;- mile_less_30000 |&gt;\n  filter(price_category == \"luxury\")\nhead(luxury_low_mile, 5)\n\n# A tibble: 5 √ó 10\n  Manufacturer Model engine_size fuel    year Mileage Price   age\n  &lt;chr&gt;        &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Toyota       RAV4          2   Hybrid  2018   28381 52671     7\n2 VW           Golf          2   Petrol  2020   18985 36387     5\n3 BMW          M5            4   Petrol  2017   22759 97758     8\n4 Toyota       RAV4          2.4 Petrol  2018   24588 49125     7\n5 VW           Golf          2   Hybrid  2018   25017 36957     7\n# ‚Ñπ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "labs/lab0/lab0.html#multiple-conditions",
    "href": "labs/lab0/lab0.html#multiple-conditions",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.2 Multiple Conditions",
    "text": "4.2 Multiple Conditions\n\n# Find cars that are EITHER Honda OR Nissan\nhonda_nissan &lt;- car_data |&gt;\n    filter(Manufacturer %in% c(\"Honda\", \"Nissan\"))\nhead(honda_nissan, 5)\n\n# A tibble: 0 √ó 10\n# ‚Ñπ 10 variables: Manufacturer &lt;chr&gt;, Model &lt;chr&gt;, engine_size &lt;dbl&gt;,\n#   fuel &lt;chr&gt;, year &lt;dbl&gt;, Mileage &lt;dbl&gt;, Price &lt;dbl&gt;, age &lt;dbl&gt;,\n#   mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# NO NISSAN OR HONDA IN DATASET\n\n# Find cars with price between $20,000 and $35,000\nprice_20k_35k &lt;- car_data |&gt;\n  filter(Price &gt; 20000 & Price &lt; 35000)\nhead(price_20k_35k, 5)\n\n# A tibble: 5 √ó 10\n  Manufacturer Model  engine_size fuel    year Mileage Price   age\n  &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Ford         Mondeo         1.6 Diesel  2014   39190 24072    11\n2 Ford         Focus          1.4 Petrol  2018   33603 29204     7\n3 Toyota       Prius          1.4 Hybrid  2015   30663 30297    10\n4 Toyota       Prius          1.4 Hybrid  2016   43893 29946     9\n5 Toyota       Prius          1.4 Hybrid  2016   43130 30085     9\n# ‚Ñπ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n# Find diesel cars less than 10 years old\ndiesel_less_10_year &lt;- car_data |&gt;\n  filter(fuel == \"Diesel\" & age &lt; 10)\nhead(diesel_less_10_year, 5)\n\n# A tibble: 5 √ó 10\n  Manufacturer Model  engine_size fuel    year Mileage Price   age\n  &lt;chr&gt;        &lt;chr&gt;        &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Ford         Fiesta         1   Diesel  2017   38370 16257     8\n2 VW           Passat         1.6 Diesel  2018   22122 36634     7\n3 VW           Passat         1.4 Diesel  2020   21413 39310     5\n4 BMW          X3             2   Diesel  2018   27389 44018     7\n5 Ford         Mondeo         2   Diesel  2016   51724 28482     9\n# ‚Ñπ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\nglimpse(diesel_less_10_year)\n\nRows: 2,040\nColumns: 10\n$ Manufacturer     &lt;chr&gt; \"Ford\", \"VW\", \"VW\", \"BMW\", \"Ford\", \"Porsche\", \"VW\", \"‚Ä¶\n$ Model            &lt;chr&gt; \"Fiesta\", \"Passat\", \"Passat\", \"X3\", \"Mondeo\", \"Cayenn‚Ä¶\n$ engine_size      &lt;dbl&gt; 1.0, 1.6, 1.4, 2.0, 2.0, 2.6, 1.2, 1.8, 1.4, 1.4, 1.4‚Ä¶\n$ fuel             &lt;chr&gt; \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Diesel\", \"Di‚Ä¶\n$ year             &lt;dbl&gt; 2017, 2018, 2020, 2018, 2016, 2019, 2018, 2016, 2020,‚Ä¶\n$ Mileage          &lt;dbl&gt; 38370, 22122, 21413, 27389, 51724, 20147, 37411, 2943‚Ä¶\n$ Price            &lt;dbl&gt; 16257, 36634, 39310, 44018, 28482, 76182, 19649, 3088‚Ä¶\n$ age              &lt;dbl&gt; 8, 7, 5, 7, 9, 6, 7, 9, 5, 7, 7, 9, 8, 3, 7, 3, 9, 7,‚Ä¶\n$ mileage_per_year &lt;dbl&gt; 4796.2500, 3160.2857, 4282.6000, 3912.7143, 5747.1111‚Ä¶\n$ price_category   &lt;chr&gt; \"midrange\", \"luxury\", \"luxury\", \"luxury\", \"midrange\",‚Ä¶\n\n\nQuestion: How many diesel cars are less than 10 years old?\nYour answer: 2040"
  },
  {
    "objectID": "labs/lab0/lab0.html#basic-summaries",
    "href": "labs/lab0/lab0.html#basic-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.1 Basic Summaries",
    "text": "5.1 Basic Summaries\n\n# Calculate average price by manufacturer\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 5 √ó 2\n  Manufacturer avg_price\n  &lt;chr&gt;            &lt;dbl&gt;\n1 BMW             24429.\n2 Ford            10672.\n3 Porsche         29104.\n4 Toyota          14340.\n5 VW              10363.\n\n# Calculate average mileage by fuel type\navg_mile_by_fuel &lt;- car_data |&gt;\n  group_by(fuel) |&gt;\n  summarize(avg_mile = mean(Mileage, na.rm = T))\n\navg_mile_by_fuel\n\n# A tibble: 3 √ó 2\n  fuel   avg_mile\n  &lt;chr&gt;     &lt;dbl&gt;\n1 Diesel  112667.\n2 Hybrid  111622.\n3 Petrol  112795.\n\n# Count cars by manufacturer\ncount_cars_by_brand &lt;- car_data |&gt;\n  group_by(Manufacturer) |&gt;\n  summarize(count_car = n())\n\ncount_cars_by_brand\n\n# A tibble: 5 √ó 2\n  Manufacturer count_car\n  &lt;chr&gt;            &lt;int&gt;\n1 BMW               4965\n2 Ford             14959\n3 Porsche           2609\n4 Toyota           12554\n5 VW               14913"
  },
  {
    "objectID": "labs/lab0/lab0.html#categorical-summaries",
    "href": "labs/lab0/lab0.html#categorical-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.2 Categorical Summaries",
    "text": "5.2 Categorical Summaries\n\n# Frequency table for price categories\ncar_data |&gt;\n  count(price_category) |&gt;\n  mutate(percent = 100 * n / sum(n)) |&gt;\n  select(-n)\n\n# A tibble: 3 √ó 2\n  price_category percent\n  &lt;chr&gt;            &lt;dbl&gt;\n1 budget            68.1\n2 luxury            12.4\n3 midrange          19.6\n\ncar_data\n\n# A tibble: 50,000 √ó 10\n   Manufacturer Model      engine_size fuel    year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol  2002  127300  3074    23\n 2 Porsche      718 Cayman         4   Petrol  2016   57850 49704     9\n 3 Ford         Mondeo             1.6 Diesel  2014   39190 24072    11\n 4 Toyota       RAV4               1.8 Hybrid  1988  210814  1705    37\n 5 VW           Polo               1   Petrol  2006  127869  4101    19\n 6 Ford         Focus              1.4 Petrol  2018   33603 29204     7\n 7 Ford         Mondeo             1.8 Diesel  2010   86686 14350    15\n 8 Toyota       Prius              1.4 Hybrid  2015   30663 30297    10\n 9 VW           Polo               1.2 Petrol  2012   73470  9977    13\n10 Ford         Focus              2   Diesel  1992  262514  1049    33\n# ‚Ñπ 49,990 more rows\n# ‚Ñπ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\n\nName: Henry (Jack) Bader\nWhy I am taking this course: I want to deepen my understanding of geospatial data science techniques in the public sector that provides a different perspective than typical engineering data science courses.\n\n\n\n\n\nEmail: jbader14@seas.upenn.edu\nGitHub: jbader14"
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Name: Henry (Jack) Bader\nWhy I am taking this course: I want to deepen my understanding of geospatial data science techniques in the public sector that provides a different perspective than typical engineering data science courses."
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: jbader14@seas.upenn.edu\nGitHub: jbader14"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html",
    "href": "assignments/assignment4/assignment4.html",
    "title": "assignment4",
    "section": "",
    "text": "Code\n# Load required packages\nlibrary(readr)\nlibrary(tidyverse)      # Data manipulation\nlibrary(sf)             # Spatial operations\nlibrary(here)           # Relative file paths\nlibrary(viridis)        # Color scales\nlibrary(terra)          # Raster operations (replaces 'raster')\nlibrary(spdep)          # Spatial dependence\nlibrary(FNN)            # Fast nearest neighbors\nlibrary(MASS)           # Negative binomial regression\nlibrary(patchwork)      # Plot composition (replaces grid/gridExtra)\nlibrary(knitr)          # Tables\nlibrary(kableExtra)     # Table formatting\nlibrary(classInt)       # Classification intervals\nlibrary(here)\n\n# Spatstat split into sub-packages\nlibrary(spatstat.geom)    # Spatial geometries\nlibrary(spatstat.explore) # Spatial exploration/KDE\n\n# Set options\noptions(scipen = 999)  # No scientific notation\nset.seed(5080)         # Reproducibility\n\n\n\n\nCode\n# Create consistent theme for visualizations\ntheme_graffiti &lt;- function(base_size = 11) {\n  theme_minimal(base_size = base_size) +\n    theme(\n      plot.title = element_text(face = \"bold\", size = base_size + 1),\n      plot.subtitle = element_text(color = \"gray30\", size = base_size - 1),\n      legend.position = \"right\",\n      panel.grid.minor = element_blank(),\n      axis.text = element_blank(),\n      axis.title = element_blank()\n    )\n}"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#data-loading",
    "href": "assignments/assignment4/assignment4.html#data-loading",
    "title": "assignment4",
    "section": "1.1 Data Loading",
    "text": "1.1 Data Loading\n\n\nCode\n# Load Chicago boundary\nchicagoBoundary &lt;- \n  st_read(\"https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson\") |&gt;\n  st_transform('ESRI:102271')\n\n\nReading layer `chicagoBoundary' from data source \n  `https://raw.githubusercontent.com/urbanSpatial/Public-Policy-Analytics-Landing/master/DATA/Chapter5/chicagoBoundary.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 1 feature and 1 field\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -87.8367 ymin: 41.64454 xmax: -87.52414 ymax: 42.02304\nGeodetic CRS:  WGS 84\n\n\nCode\n# Selected 311 data: Graffiti Removal\ngraffiti &lt;- read_csv(\"/Users/jack/Documents/GitHub/portfolio-setup-jbader14/assignments/assignment4/data/graffiti.csv\")\n\n\nRows: 1052679 Columns: 18\n‚îÄ‚îÄ Column specification ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\nDelimiter: \",\"\nchr (9): Creation Date, Status, Completion Date, Service Request Number, Typ...\ndbl (9): ZIP Code, X Coordinate, Y Coordinate, Ward, Police District, Commun...\n\n‚Ñπ Use `spec()` to retrieve the full column specification for this data.\n‚Ñπ Specify the column types or set `show_col_types = FALSE` to quiet this message."
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#data-cleaning",
    "href": "assignments/assignment4/assignment4.html#data-cleaning",
    "title": "assignment4",
    "section": "1.2 Data Cleaning",
    "text": "1.2 Data Cleaning\n\n\nCode\n# Invalid locations\ngraffiti &lt;- graffiti |&gt;\n  filter(\n    !is.na(Longitude), \n    !is.na(Latitude),\n    Latitude &gt; 0\n    )\n\n# Convert to sf\ngraffiti_sf &lt;- st_as_sf(graffiti, coords = c(\"Longitude\", \"Latitude\"), crs = 4326) |&gt;\n  st_transform('ESRI:102271')"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#spatial-visualizations",
    "href": "assignments/assignment4/assignment4.html#spatial-visualizations",
    "title": "assignment4",
    "section": "1.3 Spatial Visualizations",
    "text": "1.3 Spatial Visualizations\n\n\nCode\n# Simple point map\np1 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_sf(data = graffiti_sf, color = \"#d62828\", size = 0.1, alpha = 0.4) +\n  labs(\n    title = \"Graffiti Locations\",\n    subtitle = paste0(\"Chicago, n = \", nrow(graffiti_sf))\n  )\n\n# Density surface using modern syntax\np2 &lt;- ggplot() + \n  geom_sf(data = chicagoBoundary, fill = \"gray95\", color = \"gray60\") +\n  geom_density_2d_filled(\n    data = data.frame(st_coordinates(graffiti_sf)),\n    aes(X, Y),\n    alpha = 0.7,\n    bins = 8\n  ) +\n  scale_fill_viridis_d(\n    option = \"plasma\",\n    direction = -1,\n    guide = \"none\"\n  ) +\n  labs(\n    title = \"Density Surface\",\n    subtitle = \"Kernel Density Estimation (KDE)\"\n  )\n\n# Combine plots using patchwork (modern approach)\np1 + p2 + \n  plot_annotation(\n    title = \"Spatial Distribution of Graffiti Removal Requests in Chicago\",\n    tag_levels = 'A'\n  )"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#description-of-patterns",
    "href": "assignments/assignment4/assignment4.html#description-of-patterns",
    "title": "assignment4",
    "section": "Description of Patterns",
    "text": "Description of Patterns"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#explanation-of-step-1",
    "href": "assignments/assignment4/assignment4.html#explanation-of-step-1",
    "title": "assignment4",
    "section": "Explanation of Step 1",
    "text": "Explanation of Step 1"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#create-a-500m-vs.-500m-fishnet-grid",
    "href": "assignments/assignment4/assignment4.html#create-a-500m-vs.-500m-fishnet-grid",
    "title": "assignment4",
    "section": "2.1 Create a 500m vs.¬†500m Fishnet Grid",
    "text": "2.1 Create a 500m vs.¬†500m Fishnet Grid\n\n\nCode\n# Create 500m x 500m grid\nfishnet &lt;- st_make_grid(\n  chicagoBoundary,\n  cellsize = 500,  # 500 m\n  square = TRUE\n) |&gt;\n  st_sf() |&gt;\n  mutate(uniqueID = row_number())\n\n# Keep only cells that intersect Chicago\nfishnet &lt;- fishnet[chicagoBoundary, ]\n\n# View basic info\ncat(\"‚úì Created fishnet grid\\n\")\n\n\n‚úì Created fishnet grid\n\n\nCode\ncat(\"  - Number of cells:\", nrow(fishnet), \"\\n\")\n\n\n  - Number of cells: 2458 \n\n\nCode\ncat(\"  - Cell size:\", 500, \"x\", 500, \"meters\\n\")\n\n\n  - Cell size: 500 x 500 meters\n\n\nCode\ncat(\"  - Cell area:\", round(st_area(fishnet[1,])), \"square meters\\n\")\n\n\n  - Cell area: 250000 square meters"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#aggregate-your-violations",
    "href": "assignments/assignment4/assignment4.html#aggregate-your-violations",
    "title": "assignment4",
    "section": "2.2 Aggregate Your Violations",
    "text": "2.2 Aggregate Your Violations\n\n\nCode\n# Spatial join to determine how many graffiti calls in each cell\ngraffiti_fish &lt;- st_join(graffiti_sf, fishnet, join = st_within) |&gt;\n  st_drop_geometry() |&gt;\n  group_by(uniqueID) |&gt;\n  summarize(num_graffiti = n())\n\n# Join back to fishnet\nfishnet &lt;- fishnet |&gt;\n  left_join(graffiti_fish, by = \"uniqueID\") |&gt;\n  mutate(num_graffiti = replace_na(num_graffiti, 0))\n\n# Print Summary Stats\ncat(\"\\nGraffiti count distribution:\\n\")\n\n\n\nGraffiti count distribution:\n\n\nCode\nsummary(fishnet$num_graffiti)\n\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    0.0    16.0    80.0   427.7   487.8  7590.0 \n\n\nCode\ncat(\"\\nCells with zero graffiti incidents:\", \n    sum(fishnet$num_graffiti == 0), \n    \"/\", nrow(fishnet),\n    \"(\", round(100 * sum(fishnet$num_graffiti == 0) / nrow(fishnet), 1), \"%)\\n\")\n\n\n\nCells with zero graffiti incidents: 251 / 2458 ( 10.2 %)"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#visualize-the-count-distribution",
    "href": "assignments/assignment4/assignment4.html#visualize-the-count-distribution",
    "title": "assignment4",
    "section": "2.3 Visualize the Count Distribution",
    "text": "2.3 Visualize the Count Distribution\n\n\nCode\n# Trim top 5% of graffiti counts\ncutoff &lt;- quantile(fishnet$num_graffiti, 0.95, na.rm = TRUE)\nfishnet_trim &lt;- subset(fishnet, num_graffiti &lt;= cutoff)\n\ngraffiti_median &lt;- median(fishnet_trim$num_graffiti, na.rm = TRUE)\n\nggplot(fishnet_trim, aes(num_graffiti)) +\n  geom_histogram(bins = 40, fill = \"maroon\", color = \"black\") +\n  geom_vline(xintercept = graffiti_median, linetype = 5) +\n  annotate(\"text\",\n           x = graffiti_median,\n           y = Inf,\n           label = \"Median\",\n           hjust = -0.25,\n           vjust = 3,\n           color = \"black\",\n           size = 3) +\n  labs(title = \"Distribution of Graffiti Counts in Chicago (2017)\",\n       subtitle = \"Data obtained from removal requests directed to the 311 call center.\",\n       x = \"Count of Graffiti Incidents\",\n       y = \"Count\",\n       caption = \"Top 5% of Counts Trimmed for Easier Visualization\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nCode\nggplot() +\n  geom_sf(data = fishnet, aes(fill = num_graffiti), color = NA) +\n  geom_sf(data = chicagoBoundary, fill = NA, color = \"white\", linewidth = 1) +\n  scale_fill_viridis_c(\n    name = \"Graffiti\",\n    option = \"plasma\",\n    trans = \"sqrt\",\n    breaks = c(0, 1, 5, 10, 20, 40)\n  ) +\n  labs(\n    title = \"Graffiti Counts by Grid Cell\",\n    subtitle = \"500m x 500m cells, Chicago 2017\"\n  ) +\n  theme_void()"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#calculate-k-nearest-neighbor-features",
    "href": "assignments/assignment4/assignment4.html#calculate-k-nearest-neighbor-features",
    "title": "assignment4",
    "section": "3.1 Calculate k-nearest neighbor features",
    "text": "3.1 Calculate k-nearest neighbor features\n\n\nCode\n# Calculate mean distance to 3 nearest graffiti sites\n\n# Get coordinates\ngraffiti_coords &lt;- st_coordinates(graffiti_sf)\nfishnet_coords &lt;- st_coordinates(st_centroid(fishnet))\n\n\nWarning in st_centroid.sf(fishnet): st_centroid assumes attributes are constant\nover geometries of x\n\n\nCode\n# Calculate k nearest neighbors and distances\nknn_results &lt;- get.knnx(graffiti_coords, fishnet_coords, k = 3)\n\n# Add to fishnet\nfishnet &lt;- fishnet %&gt;%\n  mutate(\n    graffiti_knn = rowMeans(knn_results$nn.dist)\n  )\n\ncat(\"‚úì Calculated k nearest neighbor distances for graffiti in Chicago\\n\")\n\n\n‚úì Calculated k nearest neighbor distances for graffiti in Chicago\n\n\nCode\nsummary(fishnet$graffiti_knn)\n\n\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n   0.5533   28.6074   60.2756  135.1374  137.9480 1476.2625"
  },
  {
    "objectID": "assignments/assignment4/assignment4.html#perform-local-morans-i-analysis",
    "href": "assignments/assignment4/assignment4.html#perform-local-morans-i-analysis",
    "title": "assignment4",
    "section": "3.2 Perform Local Moran‚Äôs I Analysis",
    "text": "3.2 Perform Local Moran‚Äôs I Analysis\n\n\nCode\n# Function to calculate Local Moran's I\ncalculate_local_morans &lt;- function(data, variable, k = 5) {\n  \n  # Create spatial weights\n  coords &lt;- st_coordinates(st_centroid(data))\n  neighbors &lt;- knn2nb(knearneigh(coords, k = k))\n  weights &lt;- nb2listw(neighbors, style = \"W\", zero.policy = TRUE)\n  \n  # Calculate Local Moran's I\n  local_moran &lt;- localmoran(data[[variable]], weights)\n  \n  # Classify clusters\n  mean_val &lt;- mean(data[[variable]], na.rm = TRUE)\n  \n  data %&gt;%\n    mutate(\n      local_i = local_moran[, 1],\n      p_value = local_moran[, 5],\n      is_significant = p_value &lt; 0.05,\n      \n      moran_class = case_when(\n        !is_significant ~ \"Not Significant\",\n        local_i &gt; 0 & .data[[variable]] &gt; mean_val ~ \"High-High\",\n        local_i &gt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-Low\",\n        local_i &lt; 0 & .data[[variable]] &gt; mean_val ~ \"High-Low\",\n        local_i &lt; 0 & .data[[variable]] &lt;= mean_val ~ \"Low-High\",\n        TRUE ~ \"Not Significant\"\n      )\n    )\n}\n\n\n\n\nCode\n# Apply to graffiti\nfishnet &lt;- calculate_local_morans(fishnet, \"num_graffiti\", k = 5)\n\n\nWarning in st_centroid.sf(data): st_centroid assumes attributes are constant\nover geometries of x"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html",
    "href": "assignments/assignment1/assignment1.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the NJ Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders\n\n\n\n\nSubmit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an ‚ÄúAssignments‚Äù menu."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#scenario",
    "href": "assignments/assignment1/assignment1.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the NJ Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#learning-objectives",
    "href": "assignments/assignment1/assignment1.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#submission-instructions",
    "href": "assignments/assignment1/assignment1.html#submission-instructions",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Submit by posting your updated portfolio link on Canvas. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/\nMake sure to update your _quarto.yml navigation to include this assignment under an ‚ÄúAssignments‚Äù menu."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#data-retrieval",
    "href": "assignments/assignment1/assignment1.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\nnj_inc_pop &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n        median_income = \"B19013_001\",\n        total_pop = \"B01003_001\"\n  ),\n  state = my_state,\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n)\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\nnj_inc_pop &lt;- nj_inc_pop |&gt;\n  # Remove state name and \"County\"\n  mutate(\n    county_name = str_remove(NAME, \", New Jersey\"),\n    county_name = str_remove(county_name, \" County\")\n  ) |&gt;\n  # Drop NAME column\n  select(-NAME)\n\n# Display the first few rows\nhead(nj_inc_pop, 5)\n\n# A tibble: 5 √ó 6\n  GEOID median_incomeE median_incomeM total_popE total_popM county_name\n  &lt;chr&gt;          &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt; &lt;chr&gt;      \n1 34001          73113           1917     274339         NA Atlantic   \n2 34003         118714           1607     953243         NA Bergen     \n3 34005         102615           1436     461853         NA Burlington \n4 34007          82005           1414     522581         NA Camden     \n5 34009          83870           3707      95456         NA Cape May"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#data-quality-assessment",
    "href": "assignments/assignment1/assignment1.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\nnj_reli &lt;- nj_inc_pop |&gt;\n  mutate(\n    # Compute MOE percentage\n    moe_percent = round((median_incomeM / median_incomeE) * 100, 2),\n    # Create reliability categories\n    reliability = case_when(\n      moe_percent &lt; 5 ~ \"High Confidence\",\n      moe_percent &gt;= 5 & moe_percent &lt;= 10 ~ \"Moderate\",\n      moe_percent &gt; 10 ~ \"Low Confidence\"\n    ),\n    # Flag when MOE is greater than 10%\n    moe_flag = moe_percent &gt; 10\n  )\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\nnj_reli_summary &lt;- nj_reli |&gt;\n  # Count the number of counties per reliability category\n  count(reliability, name = \"n\") |&gt;\n  # Add percentages - count(county) / count(all)\n  mutate(\n  reliability_percent = round(100 * n / sum(n), 1),\n  reliability_percent = paste0(reliability_percent, \"%\")\n  ) \n\n# Display summary table\nkable(nj_reli_summary,\n    caption = \"County Reliability Summary\",\n    col.names = c(\"Reliability\", \"Count\", \"Percent\"))\n\n\nCounty Reliability Summary\n\n\nReliability\nCount\nPercent\n\n\n\n\nHigh Confidence\n21\n100%"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#high-uncertainty-counties",
    "href": "assignments/assignment1/assignment1.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\nnj_top_moe &lt;- nj_reli |&gt;\n  # Sort by DESC MOE percent\n  arrange(desc(moe_percent)) |&gt;\n  # Select top 5 counties\n  slice_head(n = 5) |&gt;\n  # Select columns\n  select(county_name, median_incomeE, median_incomeM, moe_percent, reliability)\n\n# Format as table with kable() - include appropriate column names and caption\nkable(nj_top_moe,\n      col.names = c(\"County\", \"Median Income\", \"MOE\", \"MOE %\", \"Reliabilty\"),\n      caption = \"NJ Counties with the Highest Income Data Uncertainty\",\n      format.args = list(big.mark = \",\"))\n\n\nNJ Counties with the Highest Income Data Uncertainty\n\n\nCounty\nMedian Income\nMOE\nMOE %\nReliabilty\n\n\n\n\nCape May\n83,870\n3,707\n4.42\nHigh Confidence\n\n\nSalem\n73,378\n3,047\n4.15\nHigh Confidence\n\n\nCumberland\n62,310\n2,205\n3.54\nHigh Confidence\n\n\nAtlantic\n73,113\n1,917\n2.62\nHigh Confidence\n\n\nGloucester\n99,668\n2,605\n2.61\nHigh Confidence\n\n\n\n\n\nData Quality Commentary:\nIn New Jersey, all counties have a median income MOE % below our threshold of 5%, indicating that the reporting of income date is highly reliable based on our standards. Even though these fall within high confidence, we should still proceed with caution when using this data for algorithmic decision-making. A pattern amongst the top highest MOE % revealed that all five counties make up the most southern counties in New Jersey.\nFor Cape May and Atlantic counties, a potential cause for higher MOE % stems from their economies being highly reliant on tourism for many residents. Especially, when you consider their location at the Jersey Shore, the flow of tourists is highly dependent on the season (notably, Summer), which can lead to further uncertainty in income reporting.\nCumberland, Salem, and Atlantic counties represent the three lowest counties in median income from this data. It may be the case that these counties may exhibit larger margins of error due to more residents working lower paying, hourly jobs that skew the reporting of median income, leading to higher MOE %."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#focus-area-selection",
    "href": "assignments/assignment1/assignment1.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\nNote from Jack: All counties in NJ have high confidence levels. I checked with Prof Delmelle, and she said that it was fine to continue with my analysis for NJ. I chose Cape May (MOE = 4.42%, Beach), Essex (2.00%. North Jersey), and Burlington (MOE = 1.40%, South Jersey / Philly) counties for this section to get a diverse selection of MOE percentages and geographical locations.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\nselected_countries &lt;- nj_reli |&gt;\n  filter(county_name %in% c(\"Cape May\", \"Essex\", \"Burlington\")) |&gt;\n  # Select county name, median income, MOE percentage, reliability category\n  select(county_name, median_incomeE, moe_percent, reliability)\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nkable(selected_countries,\n      caption = \"Selected Counties\",\n      col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability\"))\n\n\nSelected Counties\n\n\nCounty\nMedian Income\nMOE %\nReliability\n\n\n\n\nBurlington\n102615\n1.40\nHigh Confidence\n\n\nCape May\n83870\n4.42\nHigh Confidence\n\n\nEssex\n73785\n2.00\nHigh Confidence\n\n\n\n\n\nComment on the output: Burlington county has the highest median income with the lowest MOE %, indicating that it is the safest out of the three for algorithmic decision-making. As stated previously, Cape May has a high MOE %, relative to the rest of NJ, due to its tourist driven economy and volatility in income during a calendar year. Essex county has a moderately low MOE % and median income, revealing that lower income communities may not always have higher MOE %. Although, Essex could be an outlier."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#tract-level-demographics",
    "href": "assignments/assignment1/assignment1.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You‚Äôll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\nwhite &lt;- \"B03002_003\"\nblack &lt;- \"B03002_004\"\nhispanic &lt;- \"B03002_012\"\ntotal_pop &lt;- \"B03002_001\"\n\n# Set county codes (GEOID): Burlington, Cape May, Essex\n# Remove \"34\" from start of GEOID code\ngeoid_codes &lt;- c(\"005\", \"009\", \"013\")\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\nnj_tract &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n        white = white,\n        black = black,\n        hispanic = hispanic,\n        total_pop = total_pop\n  ),\n  state = my_state, \n  county = geoid_codes,\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n) \n\n# Calculate percentage of each group using mutate()\nnj_tract &lt;- nj_tract |&gt;\n  # Create percentages for white, Black, and Hispanic populations\n  mutate(\n    white_percent = round(100 * whiteE / total_popE, 2),\n    black_percent = round(100 * blackE / total_popE, 2),\n    hispanic_percent = round(100 * hispanicE / total_popE, 2)) |&gt;\n    # Add readable tract and county name columns using str_extract() or similar                \n    mutate(\n      # Tract (RegEx)\n      tract = str_remove(str_extract(NAME, \"Census Tract\\\\s*[0-9]+(?:\\\\.[0-9]+)?\"),\"^Census Tract\\\\s*\"),\n\n      # County (RegEx)\n      county = str_extract(NAME, \";\\\\s*[^;]+\\\\s*;\") |&gt;\n               str_remove(\"^;\\\\s*\") |&gt;\n               str_remove(\"\\\\s*;$\") |&gt;\n               str_trim(),\n      \n      county = str_remove(county, \"\\\\s+County\\\\b\") |&gt;\n               str_trim()\n  ) \n\n# head(nj_tract)"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#demographic-analysis",
    "href": "assignments/assignment1/assignment1.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\nhigh_hispanic &lt;- nj_tract |&gt;\n  arrange(desc(hispanic_percent)) |&gt;\n  slice_head(n = 1) |&gt;\n  select(county, tract, hispanic_percent)\n\nkable(high_hispanic,\n    caption = \"Highest Percentage of Hispanic Residents\",\n    col.names = c(\"County\", \"Tract\", \"Hispanic %\"))\n\n\nHighest Percentage of Hispanic Residents\n\n\nCounty\nTract\nHispanic %\n\n\n\n\nEssex\n97\n89.51\n\n\n\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\navg_demo &lt;- nj_tract |&gt;\n  group_by(county) |&gt;\n  summarize(\n    num_tracts = n(),\n    avg_white = round(mean(white_percent, na.rm = TRUE), 2),\n    avg_black = round(mean(black_percent, na.rm = TRUE), 2),\n    avg_hispanic = round(mean(hispanic_percent, na.rm = TRUE), 2)\n  )\n\n# Create a nicely formatted table of your results using kable()\nkable(avg_demo,\n      col.names = c(\"County\", \"Number of Tracts\", \"Average White %\", \"Average Black %\", \"Average Hispanic %\"),\n      caption = \"Average Demographics for Burlington, Cape May, and Essex Counties (NJ)\",\n      format.args = list(big.mark = \",\"))\n\n\nAverage Demographics for Burlington, Cape May, and Essex Counties (NJ)\n\n\n\n\n\n\n\n\n\nCounty\nNumber of Tracts\nAverage White %\nAverage Black %\nAverage Hispanic %\n\n\n\n\nBurlington\n117\n62.77\n17.41\n9.79\n\n\nCape May\n33\n86.01\n2.83\n7.66\n\n\nEssex\n211\n24.98\n41.78\n23.42"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#moe-analysis-for-demographic-variables",
    "href": "assignments/assignment1/assignment1.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\nnj_tract &lt;- nj_tract |&gt;\n  mutate(\n    # Calculate MOE percentages for white, Black, and Hispanic variables\n    # Hint: use the same formula as before (margin/estimate * 100)\n    # Need if/else some tracts show 0 population for a demographic\n    white_moe_percent = if_else(whiteE &gt; 0, round(100 * whiteM / whiteE, 2), NA_real_),\n    black_moe_percent = if_else(blackE &gt; 0, round((blackM / blackE) * 100, 2), NA_real_),\n    hispanic_moe_percent = if_else(hispanicE &gt; 0, round((whiteM / whiteE) * 100, 2), NA_real_)\n  )\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\nflag_tracts &lt;- nj_tract |&gt;\n   mutate(\n     flag_moe = ifelse(\n      coalesce(white_moe_percent &gt; 10, FALSE) |\n      coalesce(black_moe_percent &gt; 10, FALSE) |\n      coalesce(hispanic_moe_percent &gt; 10, FALSE),\n      \"High MOE\", \"Low MOE\"\n     )\n   )\n\n# Create summary statistics showing how many tracts have data quality issues\nflag_tracts_summary &lt;- flag_tracts |&gt;\n  summarize(\n    total_tracts = n(),\n    flagged_tracts = sum(flag_moe == \"High MOE\", na.rm = TRUE),\n    flagged_percents = round(100 * flagged_tracts / total_tracts, 2)\n  )\n\nkable(flag_tracts_summary, \n      col.names = c(\"Total Tracts\", \"Flagged Tracts\", \"Flagged %\"),\n      caption = \"Summary of Tracts in Burlington, Cape May, Essex counties with High MOE (Any Demo &gt; 15%)\",\n      format.args = list(big.mark = \",\"))\n\n\nSummary of Tracts in Burlington, Cape May, Essex counties with High MOE (Any Demo &gt; 15%)\n\n\nTotal Tracts\nFlagged Tracts\nFlagged %\n\n\n\n\n361\n355\n98.34"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#pattern-analysis",
    "href": "assignments/assignment1/assignment1.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Calculate population percentages for each tract\nflag_tracts &lt;- flag_tracts %&gt;%\n  mutate(\n    white_pop_percent = if_else(total_popE &gt; 0, round(100 * whiteE / total_popE, 2), NA_real_),\n    black_pop_percent = if_else(total_popE &gt; 0, round(100 * blackE / total_popE, 2), NA_real_),\n    hispanic_pop_percent = if_else(total_popE &gt; 0, round(100 * hispanicE / total_popE, 2), NA_real_)\n  )\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\nflag_tracts_group &lt;- flag_tracts |&gt;\n  # Group by MOE threshold\n  group_by(flag_moe) |&gt;\n  summarize(\n    # Total number of tracts\n    num_tracts = n(),\n    \n    # Avg population sizes for each demographic group\n    avg_white_pop = round(mean(whiteE, na.rm = TRUE), 2),\n    avg_black_pop = round(mean(blackE, na.rm = TRUE), 2),\n    avg_hispanic_pop = round(mean(hispanicE, na.rm = TRUE), 2),\n    \n    # Avg Population percentage for each demographic group\n    avg_white_pop_percent = round(mean(white_pop_percent, na.rm = TRUE), 2),\n    avg_black_pop_percent = round(mean(black_pop_percent, na.rm = TRUE), 2),\n    avg_hispanic_pop_percent = round(mean(hispanic_pop_percent, na.rm = TRUE), 2),\n  )\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\nkable(\n  flag_tracts_group,\n  col.names = c(\n    \"MOE Flag Indicator\",\n    \"Number of Tracts\",\n    \"Avg White Population\", \"Avg Black Population\", \"Avg Hispanic Population\",\n    \"Avg White Pop %\", \"Avg Black Pop %\", \"Avg Hispanic Pop %\"\n  ),\n  caption = \"Tract Demographic Statistics Based on MOE Flag Indicator\",\n  format.args = list(big.mark = \",\")\n)\n\n\nTract Demographic Statistics Based on MOE Flag Indicator\n\n\n\n\n\n\n\n\n\n\n\n\nMOE Flag Indicator\nNumber of Tracts\nAvg White Population\nAvg Black Population\nAvg Hispanic Population\nAvg White Pop %\nAvg Black Pop %\nAvg Hispanic Pop %\n\n\n\n\nHigh MOE\n355\n1,713.62\n1,096.94\n713.41\n42.21\n30.71\n17.72\n\n\nLow MOE\n6\n2,383.67\n0.00\n130.17\n88.89\n0.00\n4.86\n\n\n\n\n\nPattern Analysis:\nOne pattern is that an overwhelming majority of tracts are flagged with high MOE (for at least one demographic) of around 98.34%. There are a few reasons why we observe this. For one, tracts are much smaller than counties and when we have a smaller sample size, this can lead to more variation, hence less reliability (higher margins of error throughout the data). Secondly, since we set our MOE indicator to be classified as ‚ÄúHigh MOE‚Äù if one or more demographic had a higher MOE than 10%. As a result, this design decision leads to more tracts being categorized as ‚ÄúHigh MOE‚Äù than if we made a stricter boundary of all three demographics must have a higher MOE than 10%. Lastly, the tract data was much sparser than the county level data with many demographic estimates and margins being zero for certain tracks, which could possibly be a reporting error.\nFrom the clear imbalance of our high and low MOE classes for the tracts in our selected counties, we should approach drawing conclusions about demographics and reliability with caution. Nevertheless, it seems that our six ‚ÄúLow MOE‚Äù tracts are much less diverse and predominantly White (88.89% compared to 42.21%). It is possible that these communities may have better access to reporting their statistics or are potentially more well informed about the Census and obligation to report. But, it is important to question the oddity that there average Black population is 0% across these tracts. This leads me to believe that this is due to the sparsity issue in the data itself. If the data showed that there are no Black residents (and very little Hispanic residents), then these tracts theoretically would only have to worry about passing the MOE threshold for the White demographic. It is important to consider all possibilities when trying to analyze and uncover patterns within data."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#analysis-integration-and-professional-summary",
    "href": "assignments/assignment1/assignment1.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements:\n\nOverall Pattern Identification: What are the systematic patterns across all your analyses?\nEquity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings?\nRoot Cause Analysis: What underlying factors drive both data quality issues and bias risk?\nStrategic Recommendations: What should the Department implement to address these systematic issues?\n\nExecutive Summary:\nOverall Pattern Identification: Three significant systematic patterns arose across our analyses. The first pattern was that there was a drastic decline in data reliability when moving from county to tract-level data. In our county analysis, all counties were classified as ‚Äúhigh confidence‚Äù in MOE % for median income data. However, in our tract data for selected counties, less than 2% of tracts were classified as ‚Äúhigh confidence‚Äù for demographic data. The second pattern came from our tract-level analysis where we saw that more diverse tracts were much less reliable in terms of collective demographic MOE %. The small percentage of ‚Äúhigh confidence‚Äù tracts were predominantly white (88.89% compared to 42.21%). The third trend was revealed in geographic location in county analysis. Specifically, the highest MOE % of income data mainly resided in South Jersey, revealing clusters of uncertainty among these counties.\nEquity Assessment: Diverse communities with higher population percentages of people of color (Black, Hispanic) are most at risk of algorithmic biases. In our tract-level analysis, we saw more confident aggregate results among communities that were predominantly White rather than diverse. In addition, a majority of the missing data in these tracts came from the Black and Hispanic population estimates and margins. These discrepancies could cause further inequity in these diverse communities when using this tract-level data without caution in our algorithmic decision-making.\nRoot Cause Analysis:\n\nCounties with more variable income flow such as Cape May or Atlantic are likely to have less reliable survey responses, possibly skewing our data depending on when they are surveyed. For example, imagine an ice cream salesman at the Beach is asked about his income in July vs.¬†December.\nThe effect of a smaller sample size in our tract data compared to county data is the main cause of less reliability in our MOE estimates. Smaller sample sizes leads to fewer survey responses, which leads to greater variability causing more uncertainty.\nThe effect of sparse or missing data at the tract-level is another potential cause for biased estimates in the reliability of our tract analysis. Since most of the ‚Äúlow MOE‚Äù tracts had missing demographic data for either the Black or Hispanic demographics, this made it easier for these tracts to pass our threshold test.\n\nStrategic Recommendations:\n\nReliability Thresholds: We should establish more realistic reliability thresholds at both the county and tract level. It is evident that our current decision boundary is not insightful with all counties having high reliability and 98% of tracts having low reliability. We should tighten the threshold for county data in New Jersey. For tract data, it would be valuable to split up reliability by race rather than testing if at least one race fails the threshold. It is also important to highlight which counties have missing data and exclude them from this analysis or state their omission.\nImproved Efforts in Surveying Diverse Tracts & Country Clusters: We saw that diverse tracts had less reliability than predominantly White tracts. Policymakers should make a stronger effort at reaching these communities to ensure that we are obtaining accurate data to prevent drawing biased conclusions about these demographic groups without proper representation. Additionally, we saw that there were clusters of counties that showed higher uncertainty than the rest of the state. It would be beneficial to address these clusters by promoting survey participation to aid policymakers.\nHigher Frequency Monitoring: For immediate policy considerations, it would be helpful to have more frequent data collection such as every three months or even every month if possible. This would be very beneficial to counties like Cape May or Atlantic but others as well. It would identify problems in communities much faster and allow for more actionable, targeted intervention in policy making."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#specific-recommendations",
    "href": "assignments/assignment1/assignment1.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\nfinal_df &lt;- nj_reli |&gt;\n  # Add a new column with algorithm recommendations using case_when():\n  # - High Confidence: \"Safe for algorithmic decisions\"\n  # - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n  # - Low Confidence: \"Requires manual review or additional data\"\n  mutate(\n    reli_cat = case_when(\n      reliability == \"High Confidence\" ~ \"Safe for algorithmic decisions\",\n      reliability == \"Moderate Confidence\" ~ \"Use with caution - monitor outcomes\",\n      reliability == \"Low Confidence\" ~ \"Requires manual review or additional data\"\n    )\n  ) |&gt;\n  # Sort by MOE percent (DESC)\n  arrange(desc(moe_percent)) |&gt;\n  # Select relevant columns to include\n  select(county_name, median_incomeE, moe_percent, reliability, reli_cat) |&gt;\n  # Add $ and % symbols to median_incomeE and moe_percent\n  mutate(\n    median_incomeE = dollar(median_incomeE),\n    moe_percent = paste0(number(moe_percent, accuracy = 0.01), \"%\")\n  )\n\n# Format as a professional table with kable()\nkable(\n  final_df,\n  col.names = c(\"County\", \"Median Income\", \"MOE Percent\", \"Reliability\", \"Recommendation\"),\n  caption = \"Median Income Decision Framework for NJ Counties\",\n  format.args = list(big.mark = \",\")\n)\n\n\nMedian Income Decision Framework for NJ Counties\n\n\n\n\n\n\n\n\n\nCounty\nMedian Income\nMOE Percent\nReliability\nRecommendation\n\n\n\n\nCape May\n$83,870\n4.42%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSalem\n$73,378\n4.15%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCumberland\n$62,310\n3.54%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nAtlantic\n$73,113\n2.62%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nGloucester\n$99,668\n2.61%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nWarren\n$92,620\n2.60%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSomerset\n$131,948\n2.49%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSussex\n$111,094\n2.46%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHunterdon\n$133,534\n2.42%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMercer\n$92,697\n2.33%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nUnion\n$95,000\n2.33%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMorris\n$130,808\n2.08%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHudson\n$86,854\n2.05%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nEssex\n$73,785\n2.00%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPassaic\n$84,465\n1.85%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOcean\n$82,379\n1.77%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nCamden\n$82,005\n1.72%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMonmouth\n$118,527\n1.61%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMiddlesex\n$105,206\n1.46%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBurlington\n$102,615\n1.40%\nHigh Confidence\nSafe for algorithmic decisions\n\n\nBergen\n$118,714\n1.35%\nHigh Confidence\nSafe for algorithmic decisions\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: All counties in NJ are considered suitable for algorithmic implementation. They all have MOE less than our 5% threshold, making them high confidence for algorithmic decision making. However, if we wanted to examine tracts in individual counties, based on our tract-level analysis of three NJ counties, our data is much less reliable to use. Therefore, we should proceed with more caution for any kind of tract-level algorithmic implementation.\nCounties requiring additional oversight: No counties were classified as moderate confidence in NJ. However, I would say that counties near the bound of the 5% threshold like Cape May and Salem could be more closely monitored given their relatively high MOE above 4%. Additionally, as expressed earlier, counties such as Cape May and Atlantic that are highly reliant on tourism in generating income for its residents could use some supplementary data collection for analysis. Since income flow is extremely time-dependent, we should look to monitor the changes in income related to each season (or month) to get a better understanding of the income data. This would lead to more consistent comparisons across years through potential time-series modelling methods like a seasonal ARIMA model or something similar.\nCounties needing alternative approaches: No counties were classified as low confidence in NJ, so no specific alternatives are needed. However, it may be worthwhile to monitor the percent change in some of the higher MOE counties if they were to exhibit drastic increases in MOE % in future years. As mentioned previously, tract data within each county are much less reliable and could definitely benefit from precautionary measures such as manual data validation or increased data collection through additional surveys to give analysts more confidence in deriving algorithms to address issues at this level."
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#questions-for-further-investigation",
    "href": "assignments/assignment1/assignment1.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nHow does MOE % for each county trend over time? Do counties with lower MOE % or higher MOE % fluctuate over time? In the specific case of tourism-driven economies like Cape May or Atlantic, does data exist on their economic output across each month / season?\nHow does geographical location factor into MOE % within counties? We saw how there can be small clusters of counties that share higher MOE % relative to the rest of NJ? Is this a common occurrence that we see in most other states as well? What factors can cause these clusters other than income and demographic trends?\nFor tract-level data, why did we see a lot more missing data for people of color than the White demographic? What possible solutions are there to mitigating this issue for future surveys?"
  },
  {
    "objectID": "assignments/assignment1/assignment1.html#submission-checklist",
    "href": "assignments/assignment1/assignment1.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll ‚Äú[Fill this in]‚Äù prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "assignments/assignment2/assignment2.html",
    "href": "assignments/assignment2/assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives:\n\nApply spatial operations to answer policy-relevant research questions\nIntegrate census demographic data with spatial analysis\nCreate publication-quality visualizations and maps\nWork with spatial data from multiple sources\nCommunicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment2/assignment2.html#assignment-overview",
    "href": "assignments/assignment2/assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives:\n\nApply spatial operations to answer policy-relevant research questions\nIntegrate census demographic data with spatial analysis\nCreate publication-quality visualizations and maps\nWork with spatial data from multiple sources\nCommunicate findings effectively for policy audiences"
  },
  {
    "objectID": "assignments/assignment2/assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "assignments/assignment2/assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour analysis should identify counties that should be priorities for healthcare investment and policy intervention.\n\n\nRequired Analysis Steps\nComplete the following analysis, documenting each step with code and brief explanations:\n\nStep 1: Data Collection (5 points)\nLoad the required spatial data:\n\nPennsylvania county boundaries\nPennsylvania hospitals (from lecture data)\nPennsylvania census tracts\n\n\n# Load required packages\nlibrary(tidycensus)\nlibrary(tidyverse)\nlibrary(knitr)\nlibrary(scales)\nlibrary(RColorBrewer)\nlibrary(sf)\nlibrary(ggplot2)\nlibrary(tigris)\nlibrary(patchwork)\nlibrary(here)\n\n# Hides progress bars from output\noptions(tigris_use_cache = TRUE, tigris_progress = FALSE) \n\n\n# Load spatial data\npa_county_bounds &lt;- st_read(\"data/Pennsylvania_County_Boundaries.shp\")\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `/Users/jack/Documents/GitHub/portfolio-setup-jbader14/assignments/assignment2/data/Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\nhospitals &lt;- st_read(\"data/hospitals.geojson\")\n\nReading layer `hospitals' from data source \n  `/Users/jack/Documents/GitHub/portfolio-setup-jbader14/assignments/assignment2/data/hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\n# Check that all data loaded correctly - hidden from submission\n#head(pa_county_bounds)\n# head(hospitals)\n\n\n# How many hospitals in your dataset?\ncount_hospitals &lt;- hospitals |&gt;\n  distinct(FACILITY_N) |&gt;\n  nrow()\n\n# How many census tracts?\npa_tract_bounds &lt;- tracts(state = \"PA\", cb = TRUE)\n\ncount_tract &lt;- pa_tract_bounds |&gt;\n  distinct(NAMELSAD) |&gt;\n  nrow()\n\n# What coordinate reference system is each dataset in?\n# HIDDEN from output\n# st_crs(pa_county_bounds)\n# st_crs(hospitals)\n\nAnalysis:\n\nThere are 222 hospitals and 2547 census tracts in PA from our datasets. Both datasets are using the WGS 84 coordinate reference systems.\n\n\n\n\nStep 2: Get Demographic Data\nUse tidycensus to download tract-level demographic data for Pennsylvania.\nRequired variables:\n\nTotal population\nMedian household income\nPopulation 65 years and over (you may need to sum multiple age categories)\n\n\n# Get demographic data from ACS\n\n# Set census api key\nkey &lt;- Sys.getenv(\"CENSUS_API_KEY\")\ncensus_api_key(key)\n\n# Define age groups 65+\nage_groups_65_plus &lt;- c(\n  male_65_66 = \"B01001_020\",\n  male_67_69 = \"B01001_021\",\n  male_70_74 = \"B01001_022\",\n  male_75_79 = \"B01001_023\",\n  male_80_84 = \"B01001_024\",\n  male_85_plus = \"B01001_025\",\n  fem_65_66 = \"B01001_044\",\n  fem_67_69 = \"B01001_045\",\n  fem_70_74 = \"B01001_046\",\n  fem_75_79 = \"B01001_047\",\n  fem_80_84 = \"B01001_048\",\n  fem_85_plus = \"B01001_049\"\n)\n\n# 65+ categories to sum over\ncols_65_plus &lt;- c(\n  \"male_65_66E\",\"male_67_69E\",\"male_70_74E\",\"male_75_79E\",\"male_80_84E\",\"male_85_plusE\",\n  \"fem_65_66E\",\"fem_67_69E\",\"fem_70_74E\",\"fem_75_79E\",\"fem_80_84E\",\"fem_85_plusE\"\n)\n\n# Build variable set\nvariables &lt;- c(\n  total_pop     = \"B01003_001\",\n  age_groups_65_plus,\n  median_income = \"B19013_001\"\n)\n\npa_tract_demo &lt;- get_acs(\n  geography = \"tract\",\n  variables = variables,\n  state = \"PA\", \n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\"\n  ) |&gt;\n  # Sum columns for age groups &gt; 65\n  mutate(\n    pop_65_plusE = rowSums(across(all_of(cols_65_plus)), na.rm = TRUE)\n  ) |&gt;\n  # Select relevant columns\n  select(GEOID, total_popE, pop_65_plusE, median_incomeE)\n\n# Join to tract boundaries\npa_tract_full &lt;- pa_tract_bounds |&gt;\n  left_join(pa_tract_demo, by = \"GEOID\")\n\n\n# How many tracts have missing income data? \nmissing_income &lt;- pa_tract_demo |&gt;\n  filter(is.na(median_incomeE)) |&gt;\n  nrow()\n\n# What is the median income across all PA census tracts?\nmedian_income &lt;- pa_tract_demo |&gt;\n  summarize(median_income = median(median_incomeE, na.rm = TRUE))\n\nAnalysis:\n\nThe ACS data in this analysis is from 2022 for PA where 63 tracts have missing income data and the median income across all tracts is $70,188.\n\n\n\n\nStep 3: Define Vulnerable Populations\nIdentify census tracts with vulnerable populations based on TWO criteria:\n\nLow median household income (choose an appropriate threshold)\nSignificant elderly population (choose an appropriate threshold)\n\n\n# Filter for vulnerable tracts based on your criteria\n\n# Create elderly population percentage\npa_tract_full &lt;- pa_tract_full |&gt;\n  mutate(\n    percent_65_plus = round(100 * pop_65_plusE / total_popE, 2)\n  )\n\n# Define thresholds for each criteria\n# Low median household income: &lt;= 25th percentile\n# High elderly population percentage: &gt;= 75th percentile\nincome_thr   &lt;- quantile(pa_tract_full$median_incomeE, probs = 0.10, na.rm = TRUE)\nelderly_thr  &lt;- quantile(pa_tract_full$percent_65_plus, probs = 0.90, na.rm = TRUE)\n\n# Create new df with flagged thresholds\npa_tract_flagged &lt;- pa_tract_full |&gt;\n  mutate(\n    # Construct boolean cols for income and elderly vulnerable tracts\n    # Handle NA entries\n    income_vuln = (!is.na(median_incomeE)) & (median_incomeE &lt;= income_thr),\n    elderly_vuln = (!is.na(percent_65_plus)) & (percent_65_plus &gt;= elderly_thr),\n    # Tract is vulnerable if below income thresh OR above elderly thresh\n    vulnerable = income_vuln | elderly_vuln\n  )\n\n\n# How many tracts meet your vulnerability criteria?\ncount_vuln &lt;- sum(pa_tract_flagged$vulnerable, na.rm = TRUE)\n\n# What percentage of PA census tracts are considered vulnerable by your definition?\npercent_vuln &lt;- round(100 * count_vuln / 2547, 2)\n\nAnalysis:\n\nThe income threshold is set to identify vulnerable tracts for all tracts that fall at or below the 10th percentile of median household income. Instead of setting an arbitrary threshold value like $50,000, this method will flag the lowest 10% of median incomes as vulnerable.\nSimilarly, the elderly population threshold is set to identify vulnerable tracts for all tracts that fall at or above the 90th percentile of elderly populations. The tracts the highest 10% of elderly population percentages are flagged as vulnerable. We chose to use elderly population percentages over raw estimates to normalize for total tract populations.\n659 tracts are classified as vulnerable, meaning they are either vulnerable by a low median income or high elderly population, which accounts for 25.87% of tracts. After testing different thresholds such as 25th income / 75th elderly population percentiles and using ‚Äúand‚Äù, meaning both conditions must be true. I opted to make both thresholds stricter but use ‚Äúor‚Äù, only one condition must be true. Ultimately, this decision led to more insightful results when analyzing underserved tracts with respect to vulnerable ones, which was conducted later in my analysis.\n\n\n\n\nStep 4: Calculate Distance to Hospitals\nFor each vulnerable tract, calculate the distance to the nearest hospital.\n\n\n\n# Calculate distance from each tract centroid to nearest hospital\n\n# Get centroids of each tract (POINT)\ncentroids &lt;- st_centroid(pa_tract_flagged) \n\n# Transform to US Albers Equal Area CRS\ncentroids &lt;- st_transform(centroids, 5070)\nhospitals &lt;- st_transform(hospitals, 5070)\n\n# Create distance matrix between centroids & hospitals\ndistance_matrix &lt;- st_distance(centroids, hospitals)\n\n# Find closest hospital to each centroid\nmin_dist &lt;- apply(distance_matrix, 1, min)\n\n# Create new df with feature for distance to nearest hospital\npa_tract_flagged &lt;- pa_tract_flagged |&gt;\n  mutate(\n    # Convert nearest diff to miles\n    dist_to_near_hosp = round(as.numeric(min_dist) * 0.000621371192, 2)\n  )\n\npa_tract_vuln &lt;- pa_tract_flagged |&gt;\n  # Filter for only vulnerable tracts\n  filter(vulnerable)\n\nRequirements: - Use an appropriate projected coordinate system for Pennsylvania - Calculate distances in miles\n\n# What is the average distance to the nearest hospital for vulnerable tracts?\navg_dist_to_hosp &lt;- round(mean(pa_tract_vuln$dist_to_near_hosp, na.rm = TRUE), 2)\n\n# What is the maximum distance?\nmax_dist_to_hosp &lt;- max(pa_tract_vuln$dist_to_near_hosp, na.rm = TRUE)\n\n# How many vulnerable tracts are more than 15 miles from the nearest hospital?\ncount_tracts_far_from_hosp &lt;- sum(pa_tract_vuln$dist_to_near_hosp &gt; 15, na.rm = TRUE)\n\nAnalysis:\n\nWe are using the US Albers Equal Area projection because it preserves areas and distances without much distortion like the Web Mercator projection. It also has been proven to be effective to use with demographic and statistical analysis in the United States, making it a strong choice for this work.\nAnong vulnerable tracts, the average distance to the nearest hospital for vulnerable tracts is 3.6 miles, and the maximum distance is 28.98 miles.\nThere are 19 vulnerable tracts whose nearest hospital is greater than 15 miles away. These tracts are categorized as underserved in step 5.\n\n\n\n\nStep 5: Identify Underserved Areas\nDefine ‚Äúunderserved‚Äù as vulnerable tracts that are more than 15 miles from the nearest hospital.\n\n# Create underserved variable\npa_tract_flagged &lt;- pa_tract_flagged |&gt;\n  mutate(\n    underserved = case_when((dist_to_near_hosp &gt; 15 & vulnerable == TRUE) ~ TRUE)\n  ) \n\n\n# Count number of underserved tracts\ncount_underserved &lt;- round(sum(pa_tract_flagged$underserved == TRUE, na.rm = TRUE), 2)\n\n# What percentage of vulnerable tracts are underserved?\npercent_underserved &lt;- round(100 * count_tracts_far_from_hosp / count_vuln, 2)\n\nAnalysis:\n\nAs discussed previously, there are 19 underserved tracts. 2.88% of vulnerable tracts are classified as underserved. This metric seems somewhat surprisingly low to me. It would make sense that vulnerable tracts with lower median incomes would have worse access to hospitals than this. However, we must remember that some vulnerable tracts are classified only by high elderly population percentages. Intuitively, these tracts should have more hospitals nearby since elderly populations would most likely need access to more hospital resources and services than younger populations. Since these two effects inversely relate to one another, it makes some sense why only a small percentage of vulnerable tracts are classified as underserved; however, the metric still seems low to me. In a future analysis, it could be worthwhile to experiment with a lower threshold for distance to nearest hospital to get more concrete results of underserved tracts in PA.\n\n\n\n\nStep 6: Aggregate to County Level\nUse spatial joins and aggregation to calculate county-level statistics about vulnerable populations and hospital access.\n\n# Transform CRS\npa_tract_flagged &lt;- st_transform(pa_tract_flagged, 5070)\npa_county_bounds &lt;- st_transform(pa_county_bounds, 5070)\n\ncounty_level_stats &lt;- pa_tract_flagged |&gt;\n  # Spatial join tracts to counties\n  st_join(pa_county_bounds) |&gt;\n  # Drop geom\n  st_drop_geometry() |&gt;\n  # Group by county\n  group_by(COUNTY_NAM) |&gt;\n  # Aggregate statistics by county\n  summarize(\n    # Number of vulnerable tracts\n    n_vuln_tracts = sum(vulnerable, na.rm = TRUE),\n    # Underserved total pop\n    total_under_pop = sum(total_popE[underserved], na.rm = TRUE),\n    # Percentage of vulnerable tracts that are underserved\n    n_under_tracts = sum(underserved, na.rm = TRUE),\n    pct_under_of_vuln = round(dplyr::if_else(\n      n_vuln_tracts &gt; 0,\n      100 * n_under_tracts / n_vuln_tracts,\n      0), \n      2),\n    # Avg hospital distance for vulnerable tracts\n    avg_dist_vuln_hosp = round(mean(dist_to_near_hosp[vulnerable], na.rm = TRUE), 2),\n    # Total vulnerable population\n    total_vuln_pop = sum(total_popE[vulnerable], na.rm = TRUE)\n  ) |&gt;\n  ungroup()\n\n# county_level_stats - hidden from output\n\n\n# Which 5 counties have the highest percentage of underserved vulnerable tracts?\ntop5_counties_under &lt;- county_level_stats |&gt;\n  arrange(desc(pct_under_of_vuln)) |&gt;\n  slice_head(n = 5) |&gt;\n  select(COUNTY_NAM, n_vuln_tracts, n_under_tracts, pct_under_of_vuln)\n\n# top5_counties_under - hidden from output\n\n# Which counties have the most vulnerable people living far from hospitals?\n# Reminder: Underserved populations = vulnerable populations living far from hospitals\ntop5_under_pops &lt;- county_level_stats |&gt;\n  arrange(desc(total_under_pop)) |&gt;\n  slice_head(n = 10) |&gt;\n  select(COUNTY_NAM, total_under_pop)\n\n# top5_under_pops - hidden from output\n\nRequired county-level statistics:\n\nNumber of vulnerable tracts\nNumber of underserved tracts\nPercentage of vulnerable tracts that are underserved\nAverage distance to nearest hospital for vulnerable tracts\nTotal vulnerable population\n\nAnalysis:\n\nThe five counties with the highest percentage of underserved tracts among vulnerable tracts are Bradford, Cameron, Potter, Sullivan, and Tioga.\nThe counties with the most vulnerable people living far from hospitals are Pike, Bradford, Clearfield, Elk, and Chester.\nThere is a common theme that most underserved tracts with worse access to nearby hospitals come from more rural counties in North Central PA. This intuitively makes sense for a few reasons. Firstly, there are less hospitals in those counties. Secondly, the counties in this region of PA are geographically larger as well as the census tracts within them. The combination of less hospitals and larger census tract areas means there is less access to nearby hospitals (within the 15 mile threshold), leading to higher underserved populations. It is hard to distinguish patterns from tables alone; therefore, we will supplement this analysis with plots in future sections.\n\n\n\n\nStep 7: Create Summary Table\nCreate a professional table showing the top 10 priority counties for healthcare investment.\n\n# Create and format priority counties table\npriority_counties &lt;- county_level_stats |&gt;\n  arrange(desc(total_under_pop)) |&gt;\n  slice_head(n = 10) |&gt;\n  transmute(\n    County = COUNTY_NAM,\n    `Underserved Population` = comma(total_under_pop),\n    `Vulnerable tracts` = n_vuln_tracts,\n    `Underserved tracts` = n_under_tracts,\n    `% of Underserved Tracts` = percent(pct_under_of_vuln / 100, accuracy = 0.1),\n    `Avg distance to Hospital for Vulnerable Tracts` = number(avg_dist_vuln_hosp, accuracy = 0.01)\n  )\n  \n  \nkable(priority_counties, \n      caption = \"Top 10 Priority Counties for Healthcare Investment in PA\",\n      format.args = list(big.mark = \",\"))\n\n\nTop 10 Priority Counties for Healthcare Investment in PA\n\n\n\n\n\n\n\n\n\n\nCounty\nUnderserved Population\nVulnerable tracts\nUnderserved tracts\n% of Underserved Tracts\nAvg distance to Hospital for Vulnerable Tracts\n\n\n\n\nPIKE\n7,666\n12\n5\n41.7%\n15.90\n\n\nBRADFORD\n7,166\n2\n2\n100.0%\n18.35\n\n\nCLEARFIELD\n5,891\n4\n2\n50.0%\n12.75\n\n\nELK\n5,891\n3\n2\n66.7%\n12.89\n\n\nCHESTER\n5,626\n13\n1\n7.7%\n5.79\n\n\nCLINTON\n5,578\n4\n3\n75.0%\n15.51\n\n\nPOTTER\n5,578\n3\n3\n100.0%\n17.40\n\n\nLUZERNE\n4,905\n31\n3\n9.7%\n5.15\n\n\nCAMERON\n4,536\n2\n2\n100.0%\n18.93\n\n\nSULLIVAN\n3,860\n3\n3\n100.0%\n20.97\n\n\n\n\n\nRequirements:\n\nUse knitr::kable() or similar for formatting\nInclude descriptive column names\nFormat numbers appropriately (commas for population, percentages, etc.)\nAdd an informative caption\nSort by priority (you decide the metric)"
  },
  {
    "objectID": "assignments/assignment2/assignment2.html#part-2-comprehensive-visualization",
    "href": "assignments/assignment2/assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\nUsing the skills from Week 3 (Data Visualization), create publication-quality maps and charts.\n\nMap 1: County-Level Choropleth\nCreate a choropleth map showing healthcare access challenges at the county level.\n\n# Create county-level access map\n\ncounty_level_stats_sf &lt;- pa_county_bounds |&gt;\n  select(COUNTY_NAM, geometry) |&gt;\n  left_join(county_level_stats, by = \"COUNTY_NAM\") |&gt;\n  st_transform(5070)\n\nggplot() +\n  geom_sf(data = county_level_stats_sf, aes(fill = pct_under_of_vuln), color = \"slategray\", size = 0.75) +\n  geom_sf(data = hospitals, color = \"black\", size = 1.5) +\n  scale_fill_distiller(\n    palette = \"Reds\",\n    direction = 1,\n    na.value = \"gray\",\n    name = \"% of underserved tracts\",\n    labels = scales::label_percent(accuracy = 1, scale = 1)\n  ) +\n  labs(\n    title = \"Healthcare Access Challenges in PA Counties\",\n    subtitle = \"Underserved Tracts: Vulnerable tracts that are &gt;15 miles from the nearest hospital.\",\n    caption = \"Vulnerable Tracts: Tracts below 10th percentile in income or above 90th percentile in elderly population.\"\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10),\n    legend.text = element_text(size = 8),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text()\n  )\n\n\n\n\n\n\n\n\nRequirements:\n\nFill counties by percentage of vulnerable tracts that are underserved\nInclude hospital locations as points\nUse an appropriate color scheme\nInclude clear title, subtitle, and caption\nUse theme_void() or similar clean theme\nAdd a legend with formatted labels\n\n\n\n\nMap 2: Detailed Vulnerability Map\nCreate a map highlighting underserved vulnerable tracts.\n\n# Create detailed tract-level map\n\ncrs &lt;- 5070\npa_tract_flagged  &lt;- st_transform(pa_tract_flagged, crs)\npa_county_bounds &lt;- st_transform(pa_county_bounds, crs)\nhospitals &lt;- st_transform(hospitals, crs)\n\nunderserved_tracts &lt;- pa_tract_flagged |&gt;\n  filter(underserved)\n\nggplot() +\n  # Tracts\n  geom_sf(data = pa_tract_flagged, fill = \"gray\", color = \"white\", size = 0.07) +\n  \n  # Underserved tracts\n  geom_sf(data = underserved_tracts, aes(fill = total_popE),size = 0.2) +\n\n  # County boundaries \n  geom_sf(data = pa_county_bounds, fill = NA, color = \"slategrey\", size = 0.7) +\n\n  # Hospitals \n  geom_sf(data = hospitals, color = \"black\", size = 2) +\n\n  # Color scale by underserved population sizes\n  scale_fill_distiller(\n    palette = \"Reds\", \n    direction = 1,\n    na.value = \"transparent\",\n    name = \"Underserved population\",\n    labels = scales::label_comma()\n  ) +\n  # Title, subtitle, caption\n  labs(\n    title = \"Underserved Vulnerable Census Tracts in Pennsylvania\",\n    subtitle = \"Underserved Tracts: Vulnerable tracts that are &gt;15 miles from the nearest hospital.\",\n    caption = \"Vulnerable Tracts: Tracts below 10th percentile in income or above 90th percentile in elderly population.\"\n  ) +\n  # Theme, positioning, sizing\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    legend.title = element_text(size = 10),\n    legend.text = element_text(size = 8),\n    plot.title = element_text(face = \"bold\"),\n    plot.subtitle = element_text()\n  )\n\n\n\n\n\n\n\n\nRequirements:\n\nShow underserved vulnerable tracts in a contrasting color\nInclude county boundaries for context\nShow hospital locations\nUse appropriate visual hierarchy (what should stand out?)\nInclude informative title and subtitle\n\n\n\n\nChart: Distribution Analysis\nCreate a visualization showing the distribution of distances to hospitals for vulnerable populations.\n\n# Get median distance to hospital among vuln tracts\nmedian_dist  &lt;- median(pa_tract_vuln$dist_to_near_hosp, na.rm = TRUE)\n\n# Create distribution visualization\nggplot(pa_tract_vuln, aes(x = dist_to_near_hosp)) +\n  # Histogram\n  geom_histogram(binwidth = 1, boundary = 0, closed = \"left\", fill = \"skyblue\", color = \"black\", linewidth = 0.3) +\n  # Underserved threshold (15 miles)\n  geom_vline(xintercept = 15, color = \"firebrick\", linewidth = 0.7) +\n  annotate(\"text\",\n           x = 15, \n           y = 100, \n           label = \"Underserved Threshold\", \n           hjust = - 0.1, \n           color = \"firebrick\", \n           size = 3) +\n  # Median distance\n  geom_vline(xintercept = median_dist, color = \"black\", linewidth = 0.7) +\n  annotate(\"text\",\n           x = median_dist, \n           y = 100, \n           label = \"Median Distance\", \n           hjust = - 0.1, \n           color = \"black\", \n           size = 3) +\n  # Title, axes labels, caption\n  labs(\n    title = \"Distance to Nearest Hospital for Vulnerable Census Tracts in PA\",\n    x = \"Distance to Nearest Hospital (miles)\",\n    y = \"Number of Tracts\",\n    caption = \"The data is right-skewed with a majority of vulnerable tracts below our underserved threshold. 50% of tracts are within 2 miles of the nearest hospital.\") +\n  # Theme\n  theme_minimal() +\n  # Resize the caption text to fit\n  theme(plot.caption = element_text(size = 7.5))\n\n\n\n\n\n\n\n\nSuggested chart types:\n\nHistogram or density plot of distances\nBox plot comparing distances across regions\nBar chart of underserved tracts by county\nScatter plot of distance vs.¬†vulnerable population size\n\nRequirements:\n\nClear axes labels with units\nAppropriate title\nProfessional formatting\nBrief interpretation (1-2 sentences as a caption or in text)"
  },
  {
    "objectID": "assignments/assignment2/assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "assignments/assignment2/assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\nChoose your own additional spatial dataset and conduct a supplementary analysis.\n\nChallenge Options\nChoose ONE of the following challenge exercises, or propose your own research question using OpenDataPhilly data (https://opendataphilly.org/datasets/).\nNote these are just loose suggestions to spark ideas - follow or make your own as the data permits and as your ideas evolve. This analysis should include bringing in your own dataset, ensuring the projection/CRS of your layers align and are appropriate for the analysis (not lat/long or geodetic coordinate systems). The analysis portion should include some combination of spatial and attribute operations to answer a relatively straightforward question\n\n\nEducation & Youth Services\nOption A: Educational Desert Analysis - Data: Schools, Libraries, Recreation Centers, Census tracts (child population) - Question: ‚ÄúWhich neighborhoods lack adequate educational infrastructure for children?‚Äù - Operations: Buffer schools/libraries (0.5 mile walking distance), identify coverage gaps, overlay with child population density - Policy relevance: School district planning, library placement, after-school program siting\nOption B: School Safety Zones - Data: Schools, Crime Incidents, Bike Network - Question: ‚ÄúAre school zones safe for walking/biking, or are they crime hotspots?‚Äù - Operations: Buffer schools (1000ft safety zone), spatial join with crime incidents, assess bike infrastructure coverage - Policy relevance: Safe Routes to School programs, crossing guard placement\n\n\n\nEnvironmental Justice\nOption C: Green Space Equity - Data: Parks, Street Trees, Census tracts (race/income demographics) - Question: ‚ÄúDo low-income and minority neighborhoods have equitable access to green space?‚Äù - Operations: Buffer parks (10-minute walk = 0.5 mile), calculate tree canopy or park acreage per capita, compare by demographics - Policy relevance: Climate resilience, environmental justice, urban forestry investment ‚Äî\n\n\nPublic Safety & Justice\nOption D: Crime & Community Resources - Data: Crime Incidents, Recreation Centers, Libraries, Street Lights - Question: ‚ÄúAre high-crime areas underserved by community resources?‚Äù - Operations: Aggregate crime counts to census tracts or neighborhoods, count community resources per area, spatial correlation analysis - Policy relevance: Community investment, violence prevention strategies ‚Äî\n\n\nInfrastructure & Services\nOption E: Polling Place Accessibility - Data: Polling Places, SEPTA stops, Census tracts (elderly population, disability rates) - Question: ‚ÄúAre polling places accessible for elderly and disabled voters?‚Äù - Operations: Buffer polling places and transit stops, identify vulnerable populations, find areas lacking access - Policy relevance: Voting rights, election infrastructure, ADA compliance\n\n\n\nHealth & Wellness\nOption F: Recreation & Population Health - Data: Recreation Centers, Playgrounds, Parks, Census tracts (demographics) - Question: ‚ÄúIs lack of recreation access associated with vulnerable populations?‚Äù - Operations: Calculate recreation facilities per capita by neighborhood, buffer facilities for walking access, overlay with demographic indicators - Policy relevance: Public health investment, recreation programming, obesity prevention\n\n\n\nEmergency Services\nOption G: EMS Response Coverage - Data: Fire Stations, EMS stations, Population density, High-rise buildings - Question: ‚ÄúAre population-dense areas adequately covered by emergency services?‚Äù - Operations: Create service area buffers (5-minute drive = ~2 miles), assess population coverage, identify gaps in high-density areas - Policy relevance: Emergency preparedness, station siting decisions\n\n\n\nArts & Culture\nOption H: Cultural Asset Distribution - Data: Public Art, Museums, Historic sites/markers, Neighborhoods - Question: ‚ÄúDo all neighborhoods have equitable access to cultural amenities?‚Äù - Operations: Count cultural assets per neighborhood, normalize by population, compare distribution across demographic groups - Policy relevance: Cultural equity, tourism, quality of life, neighborhood identity\n\n\n\n\nData Sources\nOpenDataPhilly: https://opendataphilly.org/datasets/ - Most datasets available as GeoJSON, Shapefile, or CSV with coordinates - Always check the Metadata for a data dictionary of the fields.\nAdditional Sources: - Pennsylvania Open Data: https://data.pa.gov/ - Census Bureau (via tidycensus): Demographics, economic indicators, commute patterns - TIGER/Line (via tigris): Geographic boundaries\n\n\nRecommended Starting Points\nIf you‚Äôre feeling confident: Choose an advanced challenge with multiple data layers. If you are a beginner, choose something more manageable that helps you understand the basics\nIf you have a different idea: Propose your own question! Just make sure: - You can access the spatial data - You can perform at least 2 spatial operations\n\n\nYour Analysis\n\nFind and load additional data\n\nDocument your data source\nCheck and standardize the CRS\nProvide basic summary statistics\n\n\n\n# Load crime, ppr sites, and street light datasets\ncrime &lt;- st_read(\"data/crime_incidents.shp\")\n\nReading layer `crime_incidents' from data source \n  `/Users/jack/Documents/GitHub/portfolio-setup-jbader14/assignments/assignment2/data/crime_incidents.shp' \n  using driver `ESRI Shapefile'\nreplacing null geometries with empty geometries\nSimple feature collection with 117984 features and 13 fields (with 4770 geometries empty)\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.27421 ymin: 5.684342e-14 xmax: 5.684342e-14 ymax: 40.13683\nGeodetic CRS:  WGS 84\n\nppr_sites &lt;- st_read(\"data/PPR_Program_Sites.geojson\")\n\nReading layer `PPR_Program_Sites' from data source \n  `/Users/jack/Documents/GitHub/portfolio-setup-jbader14/assignments/assignment2/data/PPR_Program_Sites.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 171 features and 10 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.2563 ymin: 39.90444 xmax: -74.96944 ymax: 40.12284\nGeodetic CRS:  WGS 84\n\nlights &lt;- st_read(\"data/Street_Poles.geojson\")\n\nReading layer `Street_Poles' from data source \n  `/Users/jack/Documents/GitHub/portfolio-setup-jbader14/assignments/assignment2/data/Street_Poles.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 202942 features and 16 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -75.28108 ymin: 39.87517 xmax: -74.95891 ymax: 40.13786\nGeodetic CRS:  WGS 84\n\n# Transform CRS to EPSG: 2272 (South PA)\ncrime &lt;- st_transform(crime, 2272)\nppr_sites &lt;- st_transform(ppr_sites, 2272)\nlights &lt;- st_transform(lights, 2272)\n\n# Get Philadelphia tract data\nphl_tracts &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(total_pop = \"B01003_001\"),\n  state = \"PA\",\n  county = \"Philadelphia\",\n  year = 2023,\n  survey = \"acs5\",\n  geometry = TRUE,\n  output = \"wide\",\n  cache_table = TRUE\n)\n\n# Transform CRS to EPSG: 2272\nphl_tracts &lt;- st_transform(phl_tracts, 2272)\n\n\n# Crime Summary Stats\n\ncrime &lt;- crime |&gt;\n  # Create a column for time of day\n  mutate(\n    time_of_day = case_when(\n      hour &gt;= 0  & hour &lt;= 5 ~ \"night\",\n      hour &gt;= 6  & hour &lt;=11 ~ \"morning\",\n      hour &gt;=12  & hour &lt;=17 ~ \"afternoon\",\n      TRUE                   ~ \"evening\"\n    )\n  )\n\n# Summary Stats: Type of Crime\ncrime_type &lt;- crime |&gt;\n  # Drop geometry\n  st_drop_geometry() |&gt;\n  # Group by type\n  group_by(text_gener) |&gt;\n  # Count each type of crime\n  summarize(count = n()) |&gt;\n  # Arrange by desc count\n  arrange(desc(count))\n\n# Summary Stats: Time of Day\ncrime_tod &lt;- crime |&gt;\n  # Drop geometry\n  st_drop_geometry() |&gt;\n  # Group by time of day\n  group_by(time_of_day) |&gt;\n  # Count crimes for each time of day\n  summarize(count = n()) |&gt;\n  # Arrange by desc count\n  arrange(desc(count))\n\n# Create tables for each summary stat\n\ncrime_type_table &lt;- crime_type |&gt;\n  # Create percentage variable\n  mutate(pct = count / sum(count)) |&gt;\n  mutate(\n    # Add commas for count, % symbols for percentage (rounded)\n    count = comma(count),\n    pct   = percent(pct, accuracy = 0.01)\n  ) |&gt;\n  # Select top 10 most common crimes by type\n  slice_head(n = 10)\n\ncrime_tod_table &lt;- crime_tod |&gt;\n  mutate(pct = count / sum(count)) |&gt;\n  mutate(\n    count = comma(count),\n    pct   = percent(pct, accuracy = 0.01)\n  )\n\n# Display summary statistics using kable()\nkable(\n  crime_type_table,\n  col.names = c(\"Crime type\", \"Count\", \"Percent\"),\n  caption = \"Crime incidents by Type\"\n)\n\n\nCrime incidents by Type\n\n\nCrime type\nCount\nPercent\n\n\n\n\nThefts\n28,206\n23.91%\n\n\nOther Assaults\n20,323\n17.23%\n\n\nAll Other Offenses\n12,796\n10.85%\n\n\nMotor Vehicle Theft\n11,542\n9.78%\n\n\nVandalism/Criminal Mischief\n9,841\n8.34%\n\n\nTheft from Vehicle\n7,521\n6.37%\n\n\nFraud\n7,161\n6.07%\n\n\nAggravated Assault No Firearm\n4,170\n3.53%\n\n\nBurglary Residential\n2,429\n2.06%\n\n\nNarcotic / Drug Law Violations\n1,875\n1.59%\n\n\n\n\nkable(\n  crime_tod_table,\n  col.names = c(\"Time of day\", \"Count\", \"Percent\"),\n  caption = \"Crime incidents by Time of Day\"\n)\n\n\nCrime incidents by Time of Day\n\n\nTime of day\nCount\nPercent\n\n\n\n\nafternoon\n42,762\n36.24%\n\n\nmorning\n33,324\n28.24%\n\n\nevening\n29,275\n24.81%\n\n\nnight\n12,623\n10.70%\n\n\n\n\n\nAnalysis:\n\nThe datasets used were crime incidents, Park and Rec Program (PPR) sites, and street lights.These datasets were selected to conduct crime analysis at the tract level in Philadelphia. Our goal was to see if tracts with greater street light densities and access to nearby PPR sites experienced lower crime levels.\nThe data source for all of our datasets came from from openphilly.org. Crime incidents are from 2025, and PPR sites are from 2022. There is no date associated with the street lights dataset.\nDataset Features:\n\nCrime incidents: 14 features\nPPR programs: 11 features\nStreet lights: 17 features\n\nThe CRS for the data was in WGS 84. I changed it to a projected CRS such as StatePlane PA South that is more locally accurate for area and distance calculations with low distortion levels.\nWe calculated count and percentage statistics from the crimes dataset, grouping by type of crime and time of day.\n\n\n\nPose a research question\n\nIn Philadelphia, do census tracts with worse access to recreation centers and fewer street lights experience higher evening / night time crime rates?\n\n\nConduct spatial analysis\n\nUse at least TWO spatial operations to answer your research question.\nRequired operations (choose 2+):\n\nBuffers\nSpatial joins\nSpatial filtering with predicates\nDistance calculations\nIntersections or unions\nPoint-in-polygon aggregation\n\n\n# Step 1a: Filter crime for only evening + night crimes for street lights analysis\ncrime_en &lt;- crime |&gt;\n  filter(time_of_day %in% c(\"evening\", \"night\"))\n\n# Step 1b: Filter crime for only morning + afternoon crimes for PPR sites analysis\ncrime_am &lt;- crime |&gt;\n  filter(time_of_day %in% c(\"morning\", \"afternoon\"))\n\n# Step 2: Spatial Join - crime, lights with census tracts\ncrime_tracts_en &lt;- st_join(crime_en, phl_tracts, join = st_within)\ncrime_tracts_am &lt;- st_join(crime_am, phl_tracts, join = st_within)\nlight_tracts &lt;- st_join(lights, phl_tracts, join = st_within)\n\n# Step 3: Calculate total number of crimes and street lights per tract\ncrime_count_en &lt;- crime_tracts_en |&gt;\n  st_drop_geometry() |&gt;\n  filter(!is.na(GEOID)) |&gt;\n  group_by(GEOID) |&gt;\n  summarise(crime_count_en = n(), .groups = \"drop\")\n\ncrime_count_am &lt;- crime_tracts_am |&gt;\n  st_drop_geometry() |&gt;\n  filter(!is.na(GEOID)) |&gt;\n  group_by(GEOID) |&gt;\n  summarise(crime_count_am = n(), .groups = \"drop\")\n\nlights_count &lt;- light_tracts |&gt;\n  st_drop_geometry() |&gt;\n  filter(!is.na(GEOID)) |&gt;\n  group_by(GEOID) |&gt;\n  summarise(lights_count = n(), .groups = \"drop\")\n\n# Step 4: Calculate distance to nearest ppr site for each tract centroid\n\n# Calculate tract centroids\ntr_centroids &lt;- st_centroid(phl_tracts)\n\n# Create distance matrix between centroids & ppr sites\ndist_to_ppr &lt;- st_distance(tr_centroids, ppr_sites)\n\n# Find closest ppr site to each centroid\nmin_dist_to_ppr &lt;- apply(dist_to_ppr, 1, min)\n\nphl_tracts_dist_ppr &lt;- phl_tracts |&gt;\n  st_drop_geometry() |&gt;\n  # Create new min dist feature\n  mutate(dist_to_ppr = round(as.numeric(min_dist_to_ppr) / 5280, 2))\n\n# Step 5: Join dataframes together to get tract-level statistics\ncrime_ppr_light_stats &lt;- phl_tracts_dist_ppr |&gt;\n  # Drop geometry\n  st_drop_geometry() |&gt;\n  # Join in crime counts and light counts\n  left_join(crime_count_am, by = \"GEOID\") |&gt;\n  left_join(crime_count_en, by = \"GEOID\") |&gt;\n  left_join(lights_count,  by = \"GEOID\")\n\n# Step 6: Add geometry back to stats df for mapping\ncrime_ppr_light_stats_sf &lt;- phl_tracts |&gt;\n  select(GEOID, geometry) |&gt;\n  left_join(crime_ppr_light_stats, by = \"GEOID\")\n\n\n# Step 7: Plot a choropleth across PA census tracts w/ point geom for PPR sites\nggplot(crime_ppr_light_stats_sf) +\n  geom_sf(aes(fill = crime_count_am), color = \"slategray\") +\n  geom_sf(data = ppr_sites, color = \"black\", size = 1.5) +\n  scale_fill_distiller(\n    palette = \"Reds\", direction = 1, na.value = \"gray\",\n    name = \"Morning/Afternoon crimes\"\n  ) +\n  labs(\n    title = \"Morning/Afternoon Crimes in Philadelphia Census Tracts\",\n    subtitle = \"Points represent park and recreation sites.\"\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n# Step 7b: Create similar choropleth but find crime heavy tracts\n# Where crime count is at or above 75th percentile\n\n# Crime-heavy threshold at the 75th percentile\ncrime_thr &lt;- quantile(crime_ppr_light_stats_sf$crime_count_am, probs = 0.75, na.rm = TRUE)\n\n# Create var that labels tracts as crime-heavy vs other\ncrime_flagged &lt;- crime_ppr_light_stats_sf |&gt;\n  mutate(crime_bucket = if_else(!is.na(crime_count_am) & crime_count_am &gt;= crime_thr,\"Crime-heavy\", \"Other tracts\"))\n\n# Plot Choropleth\nggplot() +\n  geom_sf(data = crime_flagged, aes(fill = crime_bucket), color = \"white\", size = 0.1) +\n  geom_sf(data = ppr_sites, color = \"black\", size = 1.5) +\n  scale_fill_manual(\n    values = c(\"Other tracts\" = \"gray\", \"Crime-heavy\" = \"firebrick\"),\n    guide = guide_legend(title = NULL, override.aes = list(color = NA))\n  ) +\n  labs(\n    title = \"Crime-Heavy Census Tracts in Philadelphia\",\n    subtitle = paste0(\"Crime-heavy tracts are at or above the 75th percentile of crime counts.\"),\n    caption = \"This data is for morning to afternoon crimes.\"\n  ) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    plot.title = element_text(face = \"bold\")\n  )\n\n\n\n\n\n\n\n\n\n# Step 8: Output table for crime counts, light counts, and distance to nearest ppr site\ntable_am &lt;- crime_ppr_light_stats |&gt;\n  # Select relevant cols\n  select(GEOID, crime_count_am, dist_to_ppr) |&gt;\n  # Order by descending AM crime count\n  arrange(desc(crime_count_am)) |&gt;\n  # Format for kable()\n  mutate(\n    crime_count_am = comma(crime_count_am),\n    dist_to_ppr    = number(dist_to_ppr, accuracy = 0.01)\n  ) |&gt;\n  slice_head(n = 10)\n\nkable(\n  table_am,\n  col.names = c(\"GEOID\", \"Crimes (Morning/Afternoon)\", \"Nearest PPR (miles)\"),\n  caption = \"Morning/Afternoon Crimes in Philadelphia Tracts with Distance to Nearest PPR Site\"\n)\n\n\nMorning/Afternoon Crimes in Philadelphia Tracts with Distance to Nearest PPR Site\n\n\nGEOID\nCrimes (Morning/Afternoon)\nNearest PPR (miles)\n\n\n\n\n42101017800\n1,058\n0.27\n\n\n42101000500\n995\n0.58\n\n\n42101017701\n729\n0.16\n\n\n42101017702\n703\n0.21\n\n\n42101011100\n637\n0.35\n\n\n42101029800\n561\n0.26\n\n\n42101036902\n538\n0.44\n\n\n42101000701\n523\n0.61\n\n\n42101037900\n520\n0.24\n\n\n42101014700\n505\n0.24\n\n\n\n\n\n\n# Step 8b: Evening/Night crimes with streetlight counts\ntable_pm &lt;- crime_ppr_light_stats |&gt;\n  select(GEOID, crime_count_en, lights_count) |&gt;\n  arrange(desc(crime_count_en)) |&gt;\n  mutate(\n    crime_count_en = comma(crime_count_en),\n    lights_count   = comma(lights_count)\n  ) |&gt;\n  slice_head(n = 10)\n\nkable(\n  table_pm,\n  col.names = c(\"Tract GEOID\", \"Crimes (Evening/Night)\", \"Streetlights\"),\n  caption = \"Evening/Night Crimes in Philadelphia Tracts with Streetlight Counts\"\n)\n\n\nEvening/Night Crimes in Philadelphia Tracts with Streetlight Counts\n\n\nTract GEOID\nCrimes (Evening/Night)\nStreetlights\n\n\n\n\n42101017800\n642\n860\n\n\n42101017701\n434\n443\n\n\n42101037900\n418\n1,107\n\n\n42101000500\n415\n838\n\n\n42101017702\n369\n614\n\n\n42101014700\n325\n326\n\n\n42101034900\n284\n672\n\n\n42101011100\n284\n800\n\n\n42101006600\n277\n546\n\n\n42101015200\n277\n700\n\n\n\n\n\n```\nAnalysis requirements:\n\nClear code comments explaining each step\nAppropriate CRS transformations\nSummary statistics or counts\nAt least one map showing your findings\nBrief interpretation of results (3-5 sentences)\n\nInterpretation: Looking at the choropleths and tables, it does not seem there is a strong relationship between number of crimes and nearby PPR sites / number of streetlights. Even by seperating the data to analyze morning/afternoon crimes with PPR sites and evening/night crimes with PPR sites, there are not significant discoveries or trends to be made from plots and tables alone. From the choropleth of crime-heavy tracts, it does look like these tracts do cluster near eachother, signifying they belong to unsafe neighborhoods. Additionally, from the tables, tracts that experience high crime levels in the morning/afternoon tend to have higher crime levels in the evening/night at a glance. For further research, I think it would be beneficial to do zip code level analysis to get a better sense of patterns in crime throughout Philadelphia in larger geographic regions than at the tract level. I also think it could be valuable to normalize crime counts into rates using population data and street light counts into densities with area data. However, it may be the case that these variables are not strong indicators or predictors of crime level throughout the city. For that, we could do correlation, regression analyses among others to delve deeper into the factors that cause high crime, which can benefit policymakers in their decision-making."
  },
  {
    "objectID": "assignments/assignment2/assignment2.html#incorporation-of-feedback",
    "href": "assignments/assignment2/assignment2.html#incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Incorporation of Feedback:",
    "text": "Incorporation of Feedback:\nI made sure to delete brackets and questions. I hid outputs not required by the assignment‚Äôs instructions. I also hid the census API key in a .env file."
  },
  {
    "objectID": "assignments/assignment2/assignment2.html#submission-requirements",
    "href": "assignments/assignment2/assignment2.html#submission-requirements",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nWhat to submit:\n\nRendered HTML document posted to your course portfolio with all code, outputs, maps, and text\n\nUse embed-resources: true in YAML so it‚Äôs a single file\nAll code should run without errors\nAll maps and charts should display correctly\n\n\nFile naming: LastName_FirstName_Assignment2.html and LastName_FirstName_Assignment2.qmd"
  },
  {
    "objectID": "assignments/index.html",
    "href": "assignments/index.html",
    "title": "Assignments",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\nAssignment 1: Census Data Quality for Policy Decisions\n\n\n\n\n\n\nAssignment 2: Spatial Analysis and Visualization\n\n\n\n\n\n\nassignment4\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "labs/index.html",
    "href": "labs/index.html",
    "title": "Labs",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\n\n\n\n\n\nLab 0: Getting Started with dplyr\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "weekly-notes/index.html",
    "href": "weekly-notes/index.html",
    "title": "Weekly Notes",
    "section": "",
    "text": "Order By\n      Default\n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Title\n      \n    \n  \n    \n      \n      \n    \n\n\n\n\n\n\nTitle\n\n\n\nDate\n\n\n\n\n\n\n\n\nWeek 9 Notes - Logistic Regression: The Case of Recidivism\n\n\nNov 10, 2025\n\n\n\n\n\n\nWeek 8 Notes - Predictive Policying, Dirty Data, and Count Regression\n\n\nNov 3, 2025\n\n\n\n\n\n\nWeek 7 Notes - Model Diagnostics & Spatial Autocorrelation\n\n\nOct 27, 2025\n\n\n\n\n\n\nWeek 6 - Spatial ML & Advanced LR\n\n\nOct 13, 2025\n\n\n\n\n\n\nWeek 5 - Intro to Linear Regression\n\n\nOct 6, 2025\n\n\n\n\n\n\nWeek 4: GIS & Spatial Analysis in R\n\n\nSep 29, 2025\n\n\n\n\n\n\nWeek 3: EDA & ggplot2\n\n\nSep 22, 2025\n\n\n\n\n\n\nWeek 2 - Algorithmic Decision Making & Census Data\n\n\nSep 15, 2025\n\n\n\n\n\n\nWeek 1 - Course Introduction\n\n\nSep 8, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "",
    "text": "Clone Repo Workflow:\n\nNavigate to repository\nClick &lt;&gt; code: green button\nOpen with Github Desktop\nCopy over data, template scripts, etc. over from cloned class repo to personal repo\nOTHER METHOD: Use terminal\n\ncd ~/file location\ngit clone https://github.com/MUSA-5080-Fall-2025/MUSA-5080-Fall-2025.git\n\n\nALGORITHM: Set of rules/instructions for solving a problem or completing a task.\n\nIn Government: Systems used to assist / replace human decision-makers.\n\nBased on predictions from models that process historical data:\nInputs: features/predictors/independent variables/x\nOutputs: labels/outcome/dependent variable/y\nHelps eliminate human bias / subjectivity\nExamples: criminal justice (bail, sentencing), housing and finance (whether or not to rent), healthcare (prioritization, resource allocation)\n\nHealthcare Algorithmic Bias: Algorithm used to identify high-risk patients for additional care systematically discriminated against Black patients.\n\nAlgorithm used healthcare costs as a proxy for need / risk.\nHistorical inequity: Black patients typically incur lower costs due to systematic inequities in access.\nResult: Black patients under-prioritized despite equivalent levels of illness.\nScale: Used by hospitals and insurers for over 200 million people annually.\n\nCOMPAS Recidivism Prediction (Criminal Justice):\n\nAlgorithm twice as likely to flag Black defendants as high risk.\nHistorical arrest data reflects biased policing patterns.\n\nDutch Welfare Fraud Detection:\n\n‚ÄúBlack Box‚Äù secret system\nDisproportionally targeted vulnerable populations\n\n\n\n\nTerms:\n\nData Science: Computer science/engineering focus on algorithms and methods.\nData Analytics: Application of data science methods to other disciplines.\nMachine Learning: Algorithms for classification and prediction that learn from data.\nAI: Algorithms that adjust and improve across iterations (neural nets, etc.).\n\nPublic Sector context:\n\nLong history of govt data collection:\n\nCivic registration systems\nCensus data\nAdministrative records\nOperations research (post-WWII)\n\nWhat‚Äôs new?\n\nMore data (official and ‚Äúaccidental‚Äù such as social media data)\nFocus on prediction rather than explanation\nHarder to interpret and explain\n\nWhy govt use algorithms:\n\nGovts have limited budgets and need to serve everyone.\nAlgorithmic decision making is appealing because it promises:\nEfficiency: faster\nConsistency: established methods\nObjectivity: less human bias\nCost savings: less staff\n\n\nData Analytics is subjective:\n\nEvery step involves human choices (embedded values and biases):\n\nData cleaning\nData coding / classification\nData collection - use of imperfect proxies\nHow you interpret results\nWhat variables you use in the model\n\n\nScenario Example: Housing assistance allocation\n\nProxy: median household income\nBlind spots:\n\nLower income communities: median is a central measure.\nHigh rent/demand communities (NYC): median is a central measure.\n\nHarm + Fixes:\n\nUpper bound for hh income (well under city median income) to provide more housing assistance.\nNeighborhood housing price scale (divide income by housing price) to see how well off a hh is compared to the relative neighborhood.\nGuardrail:\n\nRent control\nHousing stipends / upper bounded interest rates\n\n\n\nCENSUS:\n\nFoundation for:\n\nUnderstanding community demographics\nAllocating government resources\nTracking neighborhood change\nDesigning fair algorithms (like those we just discussed)\n\nDecennial Census (2020)\n\nEveryone counted every 10 years (full population)\n9 basic questions: age, race, sex, housing\nConstitutional requirement\nDetermines political representation\n\n\nAmerican Community Survey (ACS):\n\nWhat it is:\n\n3% of households surveyed annually\nDetailed questions: income, education, employment, housing costs\nReplaced old ‚Äúlong form‚Äù in 2005\n\n1-Year estimates (areas &gt; 65K people)\n\nMost current data, smallest sample\nTake with grain of salt (only aggregate levels, not neighborhood)\n\n5-Year estimates (all areas including census tracts)\n\nMost reliable data, largest sample\nKey point: All ACS data comes with margins of error (since samples) - uncertainty\n\n\nCensus Geography Hierarchy:\n\nCounty level: state and regional planning\nCensus tract level: neighborhood analysis (1500 - 8000 people)\nBlock group level: very local analysis (huge MOEs) (600 - 3000)\n\nBlocks: decennial only, 85 people\n\n\n2020 Census Innovation: Differential Privacy\n\nChallenge: Modern computing can re-identify individuals from census data.\nSolution: Add mathematical noise to protect privacy while preserving overall patterns.\nControversy: Some places now show impossible results in populations (ex. living underwater).\nWhy this matters: Even objective data involves subjective choices about privacy vs.¬†accuracy (can cause errors).\n\nMargin of Error (MOE)\n\nLarge MOE relative to estimate = less reliable\nIn analysis:\n\nAlways report MOE alongside estimates.\nBe cautious comparing estimates with overlapping error margins.\nConsider using 5-year estimates for greater reliability\n\nDate intervals can be affected by new advancements during that time.\n\n\n\nTwo Types of Census Data:\n\nSummary Tables (what we‚Äôll use mostly)\n\nPre-calculated statistics by geography\nMedian income, percent college-educated, etc.\nGood for: Mapping, comparing places\n\nPUMS: Individual Records\n\nAnonymous individual/household responses\nGood for: Custom analysis, regression models\nMore complex but more flexible\n\n\nData Sources you‚Äôll use:\n\nTIGER/Line Files\n\nGeographic boundaries (shapefiles)\nCensus tracts, counties, states\nNow released as shapefiles (easier to use!)\n\nHistorical Data Sources:\n\nNHGIS (nhgis.org): Historical census data\nNeighborhood Change Database\nLongitudinal Tract Database: Track changes over time‚Äô"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "",
    "text": "Clone Repo Workflow:\n\nNavigate to repository\nClick &lt;&gt; code: green button\nOpen with Github Desktop\nCopy over data, template scripts, etc. over from cloned class repo to personal repo\nOTHER METHOD: Use terminal\n\ncd ~/file location\ngit clone https://github.com/MUSA-5080-Fall-2025/MUSA-5080-Fall-2025.git\n\n\nALGORITHM: Set of rules/instructions for solving a problem or completing a task.\n\nIn Government: Systems used to assist / replace human decision-makers.\n\nBased on predictions from models that process historical data:\nInputs: features/predictors/independent variables/x\nOutputs: labels/outcome/dependent variable/y\nHelps eliminate human bias / subjectivity\nExamples: criminal justice (bail, sentencing), housing and finance (whether or not to rent), healthcare (prioritization, resource allocation)\n\nHealthcare Algorithmic Bias: Algorithm used to identify high-risk patients for additional care systematically discriminated against Black patients.\n\nAlgorithm used healthcare costs as a proxy for need / risk.\nHistorical inequity: Black patients typically incur lower costs due to systematic inequities in access.\nResult: Black patients under-prioritized despite equivalent levels of illness.\nScale: Used by hospitals and insurers for over 200 million people annually.\n\nCOMPAS Recidivism Prediction (Criminal Justice):\n\nAlgorithm twice as likely to flag Black defendants as high risk.\nHistorical arrest data reflects biased policing patterns.\n\nDutch Welfare Fraud Detection:\n\n‚ÄúBlack Box‚Äù secret system\nDisproportionally targeted vulnerable populations\n\n\n\n\nTerms:\n\nData Science: Computer science/engineering focus on algorithms and methods.\nData Analytics: Application of data science methods to other disciplines.\nMachine Learning: Algorithms for classification and prediction that learn from data.\nAI: Algorithms that adjust and improve across iterations (neural nets, etc.).\n\nPublic Sector context:\n\nLong history of govt data collection:\n\nCivic registration systems\nCensus data\nAdministrative records\nOperations research (post-WWII)\n\nWhat‚Äôs new?\n\nMore data (official and ‚Äúaccidental‚Äù such as social media data)\nFocus on prediction rather than explanation\nHarder to interpret and explain\n\nWhy govt use algorithms:\n\nGovts have limited budgets and need to serve everyone.\nAlgorithmic decision making is appealing because it promises:\nEfficiency: faster\nConsistency: established methods\nObjectivity: less human bias\nCost savings: less staff\n\n\nData Analytics is subjective:\n\nEvery step involves human choices (embedded values and biases):\n\nData cleaning\nData coding / classification\nData collection - use of imperfect proxies\nHow you interpret results\nWhat variables you use in the model\n\n\nScenario Example: Housing assistance allocation\n\nProxy: median household income\nBlind spots:\n\nLower income communities: median is a central measure.\nHigh rent/demand communities (NYC): median is a central measure.\n\nHarm + Fixes:\n\nUpper bound for hh income (well under city median income) to provide more housing assistance.\nNeighborhood housing price scale (divide income by housing price) to see how well off a hh is compared to the relative neighborhood.\nGuardrail:\n\nRent control\nHousing stipends / upper bounded interest rates\n\n\n\nCENSUS:\n\nFoundation for:\n\nUnderstanding community demographics\nAllocating government resources\nTracking neighborhood change\nDesigning fair algorithms (like those we just discussed)\n\nDecennial Census (2020)\n\nEveryone counted every 10 years (full population)\n9 basic questions: age, race, sex, housing\nConstitutional requirement\nDetermines political representation\n\n\nAmerican Community Survey (ACS):\n\nWhat it is:\n\n3% of households surveyed annually\nDetailed questions: income, education, employment, housing costs\nReplaced old ‚Äúlong form‚Äù in 2005\n\n1-Year estimates (areas &gt; 65K people)\n\nMost current data, smallest sample\nTake with grain of salt (only aggregate levels, not neighborhood)\n\n5-Year estimates (all areas including census tracts)\n\nMost reliable data, largest sample\nKey point: All ACS data comes with margins of error (since samples) - uncertainty\n\n\nCensus Geography Hierarchy:\n\nCounty level: state and regional planning\nCensus tract level: neighborhood analysis (1500 - 8000 people)\nBlock group level: very local analysis (huge MOEs) (600 - 3000)\n\nBlocks: decennial only, 85 people\n\n\n2020 Census Innovation: Differential Privacy\n\nChallenge: Modern computing can re-identify individuals from census data.\nSolution: Add mathematical noise to protect privacy while preserving overall patterns.\nControversy: Some places now show impossible results in populations (ex. living underwater).\nWhy this matters: Even objective data involves subjective choices about privacy vs.¬†accuracy (can cause errors).\n\nMargin of Error (MOE)\n\nLarge MOE relative to estimate = less reliable\nIn analysis:\n\nAlways report MOE alongside estimates.\nBe cautious comparing estimates with overlapping error margins.\nConsider using 5-year estimates for greater reliability\n\nDate intervals can be affected by new advancements during that time.\n\n\n\nTwo Types of Census Data:\n\nSummary Tables (what we‚Äôll use mostly)\n\nPre-calculated statistics by geography\nMedian income, percent college-educated, etc.\nGood for: Mapping, comparing places\n\nPUMS: Individual Records\n\nAnonymous individual/household responses\nGood for: Custom analysis, regression models\nMore complex but more flexible\n\n\nData Sources you‚Äôll use:\n\nTIGER/Line Files\n\nGeographic boundaries (shapefiles)\nCensus tracts, counties, states\nNow released as shapefiles (easier to use!)\n\nHistorical Data Sources:\n\nNHGIS (nhgis.org): Historical census data\nNeighborhood Change Database\nLongitudinal Tract Database: Track changes over time‚Äô"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nnew dplyr functions:\n\nglimpse(): brief overview of df (num rows/cols, col name/type, some row examples)\ncolnames(): column names of df\nbetween(x, lower bound, upper bound): use w/ case_when + mutate\nfilter(): Joint string conditions\n\nfilter(Manufacturer %in% c(‚ÄúHonda‚Äù, ‚ÄúNissan‚Äù))\n\nPiping: |&gt; chain together dplyr commands on a single dataframe\n\nAccessing Census Data in R:\n\nModern approach: use R packages to access data directly.\n\nAlways get latest data\nReproducible workflows\nAutomatic geographic boundaries\nBuilt-in error handling\n\nUse tidycensus package\nTable organization:\n\nB19013: Median Household Income\nB25003: Housing Tenure (Own/Rent)\nB15003: Educational Attainment\nB08301: Commuting to Work\n\nVariable Examples: E (Estimate), M (Margin of Error)\n\nB19013_001E = Median household income (estimate)\nB19013_001M = Median household income (margin of error)\n\n\nSAMPLE CODE:\n\ncensus_api_key(): access data\nget_acs(geography, variables, year, state, survey, output): get the data into file\nstr_remove(var, str_to_remove): remove substrings of original string in column"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nThe examples provided context to how we use data science skills and techniques to apply in a real-world setting.\nThe conversations around algorithmic biases and biased data is important to consider when making policy considerations. It is extremely important to understand our data and recognize flaws in algorithms that can perpetuate bias, which usually harm marginalized groups of people.\nThe Census data provides valuable information (ACS) for us to analyze with data-driven methods in formulating policies that solve problems in bettering communities."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 - Algorithmic Decision Making & Census Data",
    "section": "Reflection",
    "text": "Reflection\n\nI had not had a formal introduction to Census data before. Sometimes, working with assigned datasets in class overlook a key aspect being data collection, which is a key decision in framing an analytics problem with human choice.\nI think that it is very valuable to go over biases in algorithms and data. One saying that I was taught is ‚ÄúData is never neutral‚Äù, highlighting that there are many subjective human decisions that go into creating data-driven solutions. It is crucial that we recognize these risks in our problem-solving process to mitigate risk of biases in our policy decisions to protect at-risk groups of people."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 4: GIS & Spatial Analysis in R",
    "section": "",
    "text": "Add the spatial dimension to understand:\n\nGeographic clustering of problems\nSpatial relationships between communities\nAccess to services and resources\n\nGeographic bias in algorithms:\n\nTraining data may under-represent certain areas\nSpatial autocorrelation violates independence assumptions\nService delivery algorithms may reinforce geographic inequities\nExamples:\n\nRideshare algorithms avoiding certain neighborhoods\nCrime prediction concentrating enforcement in specific areas\nSocial service algorithms missing rural communities\n\n\nVector Data Model (DISCRETE)\n\nTypes of geometric representation:\n\nPoints: Locations (schools, hospitals)\nLines: Linear features (roads, rivers, train routes)\nPolygons: Areas (census tracts, neighborhoods, service areas)\n\nEach feature has:\n\nGeometry: shape and location\nAttributes: data about that feature (population, income, etc.)\n\n\nCommon coordinate ref systems:\n\nWGS84 (EPSG:4326)\n\nGPS standard, global coverage\nGeographic system (lat/lon)\nGood for web mapping, data sharing\n\nWeb Mercator (EPSG:3857)\n\nWeb mapping standard\nProjected system\nHeavily distorts areas near poles\n\nState Plane / UTM zones\n\nLocal accuracy\nDifferent zones for different regions\nOptimized for specific geographic areas\n\nAlbers Equal Area\n\nPreserves area\nGood for demographic/statistical analysis\n\n\nPolicy Analysis Workflow:\n\nLoad data: Get spatial boundaries and attribute data\nCheck projections: Transform to appropriate CRS\nJoin datasets: Combine spatial and non-spatial data\nSpatial operations: Buffers, intersections, distance calculations\nAggregation: Summarize across spatial units\nVisualization: Maps and charts\nInterpretation: Policy recommendations"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "title": "Week 4: GIS & Spatial Analysis in R",
    "section": "",
    "text": "Add the spatial dimension to understand:\n\nGeographic clustering of problems\nSpatial relationships between communities\nAccess to services and resources\n\nGeographic bias in algorithms:\n\nTraining data may under-represent certain areas\nSpatial autocorrelation violates independence assumptions\nService delivery algorithms may reinforce geographic inequities\nExamples:\n\nRideshare algorithms avoiding certain neighborhoods\nCrime prediction concentrating enforcement in specific areas\nSocial service algorithms missing rural communities\n\n\nVector Data Model (DISCRETE)\n\nTypes of geometric representation:\n\nPoints: Locations (schools, hospitals)\nLines: Linear features (roads, rivers, train routes)\nPolygons: Areas (census tracts, neighborhoods, service areas)\n\nEach feature has:\n\nGeometry: shape and location\nAttributes: data about that feature (population, income, etc.)\n\n\nCommon coordinate ref systems:\n\nWGS84 (EPSG:4326)\n\nGPS standard, global coverage\nGeographic system (lat/lon)\nGood for web mapping, data sharing\n\nWeb Mercator (EPSG:3857)\n\nWeb mapping standard\nProjected system\nHeavily distorts areas near poles\n\nState Plane / UTM zones\n\nLocal accuracy\nDifferent zones for different regions\nOptimized for specific geographic areas\n\nAlbers Equal Area\n\nPreserves area\nGood for demographic/statistical analysis\n\n\nPolicy Analysis Workflow:\n\nLoad data: Get spatial boundaries and attribute data\nCheck projections: Transform to appropriate CRS\nJoin datasets: Combine spatial and non-spatial data\nSpatial operations: Buffers, intersections, distance calculations\nAggregation: Summarize across spatial units\nVisualization: Maps and charts\nInterpretation: Policy recommendations"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coding-techniques",
    "href": "weekly-notes/week-04-notes.html#coding-techniques",
    "title": "Week 4: GIS & Spatial Analysis in R",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nsf package in R (Shape File):\n\nWhy sf: Spatial data is just data.frame + geometry column\n\nModern replacement for older spatial packages\nIntegrates with tidyverse workflows\nFollows international standards\nFast and reliable\nRead in shape file and turn into VECTOR data model\n\nExtensions: .shp, .shx, .dbf\n\nCommon spatial data formats:\n\nShapefiles (.shp + supporting files)\nGeoJSON (.geojson)\nKML/KMZ (Google Earth)\nDatabase connections (PostGIS)\n\nSpatial subsetting: Extract key features based on spatial relationships\n\nst_filter(), st_intersects(), st_touches(), st_within()\nst_filter(): Spatial selection based on spatial relationship.\nst_touches(): Share boundary, no interior overlap\n\n‚ÄúWhich counties border Allegheny?‚Äù\n\nst_within(): Completely inside\n\n‚ÄúWhich tracts are IN Allegheny?‚Äù\n\nst_intersects(): Any overlap at all\n\n‚ÄúWhich tracts overlap a metro area?‚Äù\n\nDefault: If no .predicate specified, uses st_intersects\n.predicate tells st_filter() what spatial relationship to look for:\n\nr predicate-structure # Basic structure: st_filter(data_to_filter, reference_geometry, .predicate = relationship)\n\nst_contains(): Completely contains\n\n‚ÄúDistricts containing hospitals\n\nst_overlaps(): Partial overlap\n\n‚ÄúOverlapping service areas‚Äù\n\nst_disjoint(): No spatial relationship\n\n‚ÄúCounties separate from urban areas‚Äù\n\nMost common: st_intersects() (any overlap) and st_touches() (neighbors)\n\nSpatial join: Combine datasets based on spatial relationships.\nDistance calculations:\n\nst_centroid(): converts polygons to points\nst_distance():\n\nArea calculations: Examples in code - Units depend on coordinate reference system.\nDot (.): The dot (.) is a placeholder that represents the data being passed through the pipe (%&gt;%).\n\npa_counties &lt;- pa_counties %&gt;% mutate( area_sqkm = as.numeric(st_area(.)) / 1000000 )\nThe . refers to pa_counties - the data frame being passed through the pipe. So this is equivalent to:\npa_counties &lt;- pa_counties %&gt;% mutate( area_sqkm = as.numeric(st_area(pa_counties)) / 1000000 )\n\nBuffer operations: Create zones around features\n\n`{r buffers} # 10km buffer around all hospitals hospital_buffers &lt;- hospitals %&gt;% st_buffer(dist = 10000) # 10,000 meters\n\nCoordinate Reference Systems:\n\nProblems:\n\nCan‚Äôt preserve area, distance, and angles simultaneously\nDifferent projections optimize different properties\nWrong projection ‚Üí wrong analysis results!\n\nIn-Class:\n\nStep 1: Approximate Earth‚Äôs shape w/ ellipsoid\nStep 2: Tie ellipsoid to real Earth (Datum)\nStep 3: Put down lat/long grid\n\nGeographic (geodetic) coord systems:\n\nNorth American Datum 1927 (NAD27)\nNAD 83, GRS 80, WGS 84\nAll lat/long on 3D ellipse.\nUnits: decimal degrees\nGood for: Global datasets, web mapping\nBad for: Area/distance calculations\n\nProjected Coordinate Systems (PCS):\n\nOn computer screen\nX/Y coordinates on a flat plane\nUnits: meters, feet, etc.\nGood for: Local analysis, accurate measurements\nBad for: Large areas, global datasets\n\nTransform when:\n\nCalculating areas or distances\nCreating buffers\nDoing geometric operations\nWorking with local/regional data\n\nCODE:\n\nTo simply check current CRS st_crs(pa_counties)\nTo set CRS (ONLY if missing) pa_counties &lt;- st_set_crs(pa_counties, 4326)\nTransform to different CRS\nPennsylvania South State Plane (good for PA analysis)\npa_counties_projected &lt;- pa_counties %&gt;% st_transform(crs = 3365)\nTransform to Albers Equal Area (good for area calculations) pa_counties_albers &lt;- pa_counties %&gt;% st_transform(crs = 5070) `"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#questions-challenges",
    "href": "weekly-notes/week-04-notes.html#questions-challenges",
    "title": "Week 4: GIS & Spatial Analysis in R",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#connections-to-policy",
    "href": "weekly-notes/week-04-notes.html#connections-to-policy",
    "title": "Week 4: GIS & Spatial Analysis in R",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nLearning techniques such as the sf package is extremely valuable in spatial visualizations in R. These help analysts draw conclusions to make policy considerations and also help display trends / patterns to non-data colleagues."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#reflection",
    "href": "weekly-notes/week-04-notes.html#reflection",
    "title": "Week 4: GIS & Spatial Analysis in R",
    "section": "Reflection",
    "text": "Reflection\n\nI have worked with the sf package before but not in this much depth. It was very useful going over the useful commands, parameters, relationships, etc. to deepen my understanding of this topic."
  },
  {
    "objectID": "weekly-notes/week-06-notes.html",
    "href": "weekly-notes/week-06-notes.html",
    "title": "Week 6 - Spatial ML & Advanced LR",
    "section": "",
    "text": "Build a baseline model, interpret coefs / performance / limitations\nBuilding dummy variables: binary indicators\n\ntype of categorical variable\nNeightborhoods:\n\nRittenhouse is reference (omit)\nPos coef: More expensive than Rittenhouse, Neg: Less expensive (all else equal)\n\n\nInteraction Effects: When effect of one variable depends on the level of another variable.\n\nex) Housing: Sq Foot matter more in wealthy neighborhood\n\nPolynomial Terms: nonlinear relationships\n\nU Shaped Age Effect: Really new / old houses high val - Middle aged houses are low in val.\n\nFirst Law of Geography: ‚ÄúEverything is related to everything else, but near things are more related than distant things‚Äù\nFixed Effects: Categorical variables that capture all unmeasured characteristics of a group\n\nIn Hedonic Models:\n\nEach neighborhood gets its own dummy variable\nCaptures everything unique about that neighborhood we didn‚Äôt explicitly measure\n\nBenefits:\n\nBetter predictions\nChange coefs\nBut Black Box (Don‚Äôt know the why)\n\n\nLOOCV - Leave one observation out at a time (special case of k-fold)"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-06-notes.html#key-concepts-learned",
    "title": "Week 6 - Spatial ML & Advanced LR",
    "section": "",
    "text": "Build a baseline model, interpret coefs / performance / limitations\nBuilding dummy variables: binary indicators\n\ntype of categorical variable\nNeightborhoods:\n\nRittenhouse is reference (omit)\nPos coef: More expensive than Rittenhouse, Neg: Less expensive (all else equal)\n\n\nInteraction Effects: When effect of one variable depends on the level of another variable.\n\nex) Housing: Sq Foot matter more in wealthy neighborhood\n\nPolynomial Terms: nonlinear relationships\n\nU Shaped Age Effect: Really new / old houses high val - Middle aged houses are low in val.\n\nFirst Law of Geography: ‚ÄúEverything is related to everything else, but near things are more related than distant things‚Äù\nFixed Effects: Categorical variables that capture all unmeasured characteristics of a group\n\nIn Hedonic Models:\n\nEach neighborhood gets its own dummy variable\nCaptures everything unique about that neighborhood we didn‚Äôt explicitly measure\n\nBenefits:\n\nBetter predictions\nChange coefs\nBut Black Box (Don‚Äôt know the why)\n\n\nLOOCV - Leave one observation out at a time (special case of k-fold)"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#coding-techniques",
    "href": "weekly-notes/week-06-notes.html#coding-techniques",
    "title": "Week 6 - Spatial ML & Advanced LR",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nst_as_sf(coords =, crs = ): Make data spatial\nDummy Variable:\n\nEnsure name is a factor\nboston.sf &lt;- boston.sf %&gt;%\nmutate(name = as.factor(name))\nCheck which is reference (first alphabetically)\nlevels(boston.sf$name)[1]\nFit model with neighborhood fixed effects\nmodel_neighborhoods &lt;- lm(SalePrice ~ LivingArea + name, data = boston.sf)\n\nInteraction Effect:\n\nmodel_interact &lt;- lm(SalePrice ~ LivingArea * wealthy_neighborhood, data = boston.sf)\n\nApproaches to Spatial Aggregation (See Lecture Slides):\n\nBuffer Aggregation: Count or sum events within a defined distance\nk-Nearest Neighbors (kNN): Average distance to k closest events\nDistance to Specific Points: Straight-line distance to important locations"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#questions-challenges",
    "href": "weekly-notes/week-06-notes.html#questions-challenges",
    "title": "Week 6 - Spatial ML & Advanced LR",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#connections-to-policy",
    "href": "weekly-notes/week-06-notes.html#connections-to-policy",
    "title": "Week 6 - Spatial ML & Advanced LR",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nI think these more advanced techniques for modeling regressions is very important to build better models to use for decision making in policy considerations"
  },
  {
    "objectID": "weekly-notes/week-06-notes.html#reflection",
    "href": "weekly-notes/week-06-notes.html#reflection",
    "title": "Week 6 - Spatial ML & Advanced LR",
    "section": "Reflection",
    "text": "Reflection\n\nI am excited to employ some of these advanced strategies on the midterm with my table! I think doing this lecture in the context of housing prices makes it easier to wrap my head around the concepts for the midterm."
  },
  {
    "objectID": "weekly-notes/week-08-notes.html",
    "href": "weekly-notes/week-08-notes.html",
    "title": "Week 8 Notes - Predictive Policying, Dirty Data, and Count Regression",
    "section": "",
    "text": "Broken Window‚Äôs Theory: Signs of disorder (abandoned cars) predict property crime (burglaries)\nGlobal v. Local Spatial Autocorrelation: Moran‚Äôs I (diff formulas for each)\n\nUse Moran‚Äôs I scatterplot to visualize hotspots/coldspots and outliers\n\nStatistical Significance Testing:\n\nCalculate observed I_i for location ùëñ\nRandomly shuffle values across locations (999 times)\nRecalculate I_i for each permutation\nCompare observed vs.¬†distribution of permuted values\nIf observed is extreme ‚Üí statistically significant (p &lt; 0.05)\n\nWhy linear regression does not work for ‚Äúcount‚Äù target data:\n\nCan predict negative values (impossible for counts)\nAssumes constant variance (counts often have variance ‚â† mean)\nAssumes continuous outcome (counts are discrete)\nAssumes normal errors (count data is skewed)\n\nModel 1: Poisson\n\nRaise e to the power of sum of regressors\nLog-scaled\n\nOver-Dispersion Problem: Variance &gt;&gt;&gt; Mean\n\nUnobserved heterogeneity: Some areas have unmeasured crime attractors\nContagion effects: One crime leads to others (not independent)\nMeasurement error: Counting issues, data quality\nModel misspecification: Missing important variables\nDispersion = Residual Deviance / Degrees of Freedom\n\nModel 2: Negative Bibomial (for over-dispersion)\n\nRelaxes variance = mean assumption\n\nFishnet Grid:\n\nStandard approach in predictive policing\nEasier spatial operations\nConsistent unit of analysis\n\nLOGO-CV: Leave-one-out CV for spatial data\nKernel Density Estimation (KDE): Simpleset spatial prediction method\n\nPlace smooth ‚Äúbump‚Äù over each crime location\nSum all bumps to create risk surface\nNo predictors, no model, just past locations"
  },
  {
    "objectID": "weekly-notes/week-08-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-08-notes.html#key-concepts-learned",
    "title": "Week 8 Notes - Predictive Policying, Dirty Data, and Count Regression",
    "section": "",
    "text": "Broken Window‚Äôs Theory: Signs of disorder (abandoned cars) predict property crime (burglaries)\nGlobal v. Local Spatial Autocorrelation: Moran‚Äôs I (diff formulas for each)\n\nUse Moran‚Äôs I scatterplot to visualize hotspots/coldspots and outliers\n\nStatistical Significance Testing:\n\nCalculate observed I_i for location ùëñ\nRandomly shuffle values across locations (999 times)\nRecalculate I_i for each permutation\nCompare observed vs.¬†distribution of permuted values\nIf observed is extreme ‚Üí statistically significant (p &lt; 0.05)\n\nWhy linear regression does not work for ‚Äúcount‚Äù target data:\n\nCan predict negative values (impossible for counts)\nAssumes constant variance (counts often have variance ‚â† mean)\nAssumes continuous outcome (counts are discrete)\nAssumes normal errors (count data is skewed)\n\nModel 1: Poisson\n\nRaise e to the power of sum of regressors\nLog-scaled\n\nOver-Dispersion Problem: Variance &gt;&gt;&gt; Mean\n\nUnobserved heterogeneity: Some areas have unmeasured crime attractors\nContagion effects: One crime leads to others (not independent)\nMeasurement error: Counting issues, data quality\nModel misspecification: Missing important variables\nDispersion = Residual Deviance / Degrees of Freedom\n\nModel 2: Negative Bibomial (for over-dispersion)\n\nRelaxes variance = mean assumption\n\nFishnet Grid:\n\nStandard approach in predictive policing\nEasier spatial operations\nConsistent unit of analysis\n\nLOGO-CV: Leave-one-out CV for spatial data\nKernel Density Estimation (KDE): Simpleset spatial prediction method\n\nPlace smooth ‚Äúbump‚Äù over each crime location\nSum all bumps to create risk surface\nNo predictors, no model, just past locations"
  },
  {
    "objectID": "weekly-notes/week-08-notes.html#coding-techniques",
    "href": "weekly-notes/week-08-notes.html#coding-techniques",
    "title": "Week 8 Notes - Predictive Policying, Dirty Data, and Count Regression",
    "section": "Coding Techniques",
    "text": "Coding Techniques"
  },
  {
    "objectID": "weekly-notes/week-08-notes.html#questions-challenges",
    "href": "weekly-notes/week-08-notes.html#questions-challenges",
    "title": "Week 8 Notes - Predictive Policying, Dirty Data, and Count Regression",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nNothing!"
  },
  {
    "objectID": "weekly-notes/week-08-notes.html#connections-to-policy",
    "href": "weekly-notes/week-08-notes.html#connections-to-policy",
    "title": "Week 8 Notes - Predictive Policying, Dirty Data, and Count Regression",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nUnderstanding dirty data is very important for preventing algorithmic biases in our modeling process."
  },
  {
    "objectID": "weekly-notes/week-08-notes.html#reflection",
    "href": "weekly-notes/week-08-notes.html#reflection",
    "title": "Week 8 Notes - Predictive Policying, Dirty Data, and Count Regression",
    "section": "Reflection",
    "text": "Reflection\n\nRe-learning the Poisson and Negative Binomial regression methods is important especially for cases like Crime count data."
  }
]